\documentclass[twoside,11pt]{article}
\usepackage{jair, theapa, rawfonts}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf


% Definitions of handy macros can go here

\usepackage{latexsym}
\usepackage{amssymb, amsmath,  trees, tree-dvips,parsetree,multirow}
\usepackage{url}
%\usepackage{subfigure}
\usepackage{graphicx}
%\usepackage{natbib}

\usepackage[]{subfig}
\usepackage{float}   
\usepackage{xcolor}
\usepackage[normalem]{ulem}

\usepackage{pgfplots}
\pgfplotsset{compat=1.5}





\newtheorem{definition}{Definition}
\newtheorem{examp}{Example}
\newenvironment{example}{\begin{examp}\rm}{\end{examp}}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newenvironment{proof}{{\bf Proof:}}{$\square$ }




\def\vec#1{\mathbf{#1}}
\def\svec#1{\vec{#1}}
%\def\svec#1{\overset{\leadsto}{#1}}
%\def\substr#1{{\textsf #1}}
\def\substr#1{{\bf\overline{#1}}}
\def\smallvectors#1{\mathcal{#1}}
\def\R{\mathbb{R}}
\def\P{\mathbb{P}}
\def\dotprod#1#2{\langle#1,#2\rangle}
\def\degree#1{d(#1)}


\def\OBV#1{\phi(#1)}
\def\df{\Upsilon}
\def\basicdf{\upsilon}

\def\J{\vartheta}

\def\mynodes#1{N(#1)}
\def\mychar#1{C(#1)}
\def\norm#1{norm(#1)}
\def\nnodes#1{|\mynodes{#1}|}
\def\nchar#1{|\mychar{#1}|}


\def\num#1#2{f_{#1,#2}}


%\def\circularconvolution{\otimes}
\def\shuf#1#2{t_{#1}(#2)}

% Ideal Binary Operation
\def\magicoperation{\mo}
\def\mo{\odot}
\def\Mo{\bigodot}

% Real Binary Operation
\def\rop{\circ}
\def\comp{\circledcirc}


\def\prodmo{\times}
\def\shufprod{\otimes}


\def\cconv{\ast}
\def\shufcconv{\circledast}



\newcounter{properties}
\newtheorem{property}{Property}



\def\comment#1#2{}
%\def\comment#1#2{\textbf{[#1]}}{}
%\def\comment#1#2{\textbf{[#1] [}\emph{#2}\textbf{]}}
%\def\finalcomment#1#2{}
\def\ourcomment#1{}
%\def\ourcomment#1{\textbf{OUR COMMENT: [#1]}}

%%%% Minor revisions editing 

%\def\finalcomment#1#2{{\color{red}\textbf{[#2] [}\emph{#1}\textbf{]}}}
%\def\finalcomment#1#2{{\textbf{[}\emph{#2}\textbf{]}}}
%\def\myremove#1{{\color{red}\sout{#1}}}
%\def\myinsert#1{{\color{blue}#1}}

%%% PLAIN
\def\finalcomment#1#2{}
\def\myremove#1{}
\def\myinsert#1{#1}


\def\meta#1{\noindent\textbf{FLUSSO}: \emph{#1}}


%%%% Minor 2nd editing revisions editing 

%\def\finalsecondcomment#1#2{{\color{red}\textbf{[#1] [}\emph{#2}\textbf{]}}}
%\def\finalsecondcomment#1#2{{\textbf{[}\emph{#1}\textbf{]}}}
%\def\mysecondremove#1{{\color{red}\sout{#1}}}
%\def\mysecondinsert#1{{\color{blue}#1}}

%%% PLAIN
\def\finalsecondcomment#1#2{}
\def\mysecondremove#1{}
\def\mysecondinsert#1{#1}

\def\LD#1{[{\color{blue}L}D] {\color{blue}#1}}
\def\OCAR#1#2#3#4{[{\color{blue}O}{\color{brown}C}{\color{black}A}{\color{purple}R}] {\color{blue}#1} {\color{brown}#2} {\color{black}#3} {\color{purple}#4}}
%\def\arco#1#2{}


% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

%\jmlrheading{1}{2000}{1-48}{4/00}{10/00}{Fabio Massimo Zanzotto and Lorenzo Dell'Arciprete }

% Short headings should be running head and authors last names

\ShortHeadings{Distributed Convolution Kernels}{Zanzotto and Dell'Arciprete}
\firstpageno{1}

\begin{document}

\title{TITOLO 1: Distributed Convolution Kernels on Countable Sets\\TITOLO 2: Learning with Structures without Kernels and with Simpler Machines\\TITOLO 3: Learning with Structures, but without Kernels\\TITOLO 4: Learning with Structures, but without Kernels and without Recurrent Neural Networks}

\author{\name Fabio Massimo Zanzotto \email fabio.massimo.zanzotto@uniroma2.it
%\\
%      \addr Division of Computer Science and Department of Statistics\\
%       University of California\\
%       Berkeley, CA 94720-1776, USA
\AND
\name Lorenzo Dell'Arciprete \email lorenzo.dellarciprete@gmail.com\\
       \addr Department of Enterprise Engineering\\
  	University of Rome ``Tor Vergata"\\
       Rome, Italy
      }

%\editor{}

\maketitle
%{\keywords Kernel Methods, Convolution Kernels, Distributed Representations, Structured Kernels, Similarity among Structured Data}

\begin{abstract}

%\emph{Distributed tree kernels} (DTKs) \cite{Zanzotto2012193} explored a novel opportunity to use tree structured data in learning. Stemming from the distributed representations \cite{Hinton:1986,McClelland:Rumelhart:1986,Plate1995}, DTKs embed the huge feature spaces of tree fragments into lower dimensional spaces by \mysecondremove{compositionally}\mysecondinsert{recursively} computing vectors for tree structured data. This embedding gives the opportunity to use tree structured data in linear versions of kernel machines where learners and classifiers are computationally more efficient.
%
%
%
%
%In this paper we introduce the \emph{distributed convolution kernels on countable sets} (DCK) that extend the idea pursued with DTKs to the convolution kernels  on countable sets (CK) (as defined in \cite{Haussler99convolutionkernels}). 
%We first formally demonstrate that each CK has a corresponding DCK by introducing the \emph{distributed convolution structure on countable sets} (DCS). The distributed convolution kernel is the dot product between two distributed convolution structures and approximates the corresponding convolution kernel. 
%We then apply the idea to five convolution kernels: the tree kernel \cite{Collins2002} (generalizing what was done in \cite{Zanzotto2012193}), the subpath tree kernels \cite{Kimura:2011:SKR:2017863.2017871}, the route tree kernels \cite{Aiolli2009}, the string or sequence kernels \cite{Lodhi:2002:TCU:944790.944799}, and finally, the partial tree kernels \cite{Kashima:2002:KSD:645531.656021,Moschitti2006b}.  
%We formally describe the complexity of the distributed convolution kernels within the family of the linear support vector machines \cite{Joachims:2006:TLS:1150402.1150429,DBLP:journals/mp/Shalev-ShwartzSSC11} and we compare this to the complexity of the CKs used in kernelized support vector machines. 
%We finally investigate how well distributed convolution kernels approximate the convolution kernels.

%want to further explore this opportunity by investigating two issues. First, using the Johnson-Lindenstrauss Lemma, we formalize and demonstrate a lemma to assess the theoretical limits of the approach. Second, we demonstrate the possibility of defining a novel class of distributed tree kernels by extending the idea to two other tree kernels: 


\end{abstract}

\tableofcontents
\newpage


\section{Introduction}

\comment{[R2,C5][IssueC]}{Introduction: the position of the paper is extremely narrow, and the authors only connect to other work 
on tree kernels. It is thus likely to be ignored by many reader interested in ML for NLP. I believe this 
paper is part of a trend: using recursive structures to handle problems when one needs a proper 
representation for long term dependencies. The most vocal representative of this new trend is Richard 
Socher. It would also have been interesting to compare DTK to Recursive NN on some problems that are 
much more recent and critical that ones used for experiments in this paper, such as sentiment analysis.}

\LD{Structured data may be used to interpret nearly everything -- 
%sequences, trees and, in general, 
graphs are really powerful representations for natural and artificial objects.} 
In fact, natural objects -- bodies \cite{boh}, proteins \cite{boh}, natural language utterances \cite{boh} and, even, social networks \cite{boh} -- are often represented  with 
%graphs, which are 
%as graphs, which are 
labeled nodes connected by labeled edges.
Artificial objects are even linked more directly with structured data.
%The link between artificial objects and structured data is even more direct. 
%Artificial objects 
These objects have often been dots and segments in some design before being tangible. 
Modeling them as structured data means only to rediscover what is already there. 
%Even those artificial objects that have not been designed may be represented as graphs.
%formally designed are possibly conceptual structures that come to life. 
%For example, artificial objects as natural language sentences have words (nodes), which contribute to generate meaning with their interactions (edges). 
Consequently, machines that want to learn from observing objects are forced to deal with their structure.

\LD{\emph{How to fit structured data in learning machines} has thus become a compelling challenge, which not surprisingly has attracted a lot of attention.} 
Structures cannot fit naturally in \emph{flat} vector spaces of features, which generally underly learning machines. 
Hence, many strategies have emerged to overcome this limitation.

\LD{The proposed strategies can be roughly grouped in three families: (1) \emph{representing small sets of substructures}, (2) \emph{defining graph kernels} and (3) \emph{using recursive learning machines}.}
%% to use structured data in learning machines: 
%\begin{itemize}
%\item defining small sets of relevant substructures; 
%\item defining convolution kernels; and,
%\item recursively applying learning machines over structures. 
%\end{itemize}
%%It is not surprising that a lot of attention has been given to using structured data in machines that learn classifiers or regressors. 
%In the first family, learning machines rely on small sets of relevant substructures --
%that represent structured objects --
%for example, 
The first family defines small sets of relevant substructures --
specific edges in graphs \cite{Even-Zohar:2000:CAW:974305.974322} or specific paths in trees \cite{GildeaJurafsky:2002:CL} --
that are the features of the vector spaces where structured objects are represented. Features are active if structured objects have the related substructures. 
%Although computationally efficient, this strategy produces models that neglect large part of the information encoded in structures. 
%The other families of strategies solve this lack of coverage of the information encoded in structures. 
The second family relies on the definition of \emph{graph kernels} \cite{} that are similarity measures between structured objects.
% directly computed  without mapping objects into feature vectors.
Convolution kernels \cite{Haussler99convolutionkernels} are an important class of these graph kernels as 
%they perform a weighted count of common substructures of structures and, thus, 
they implicitly encode huge feature spaces of all possible substructures.  Hence, convolution kernels have attracted a lot of research and specific convolution kernels have been defined for different structures --  for example strings, sequences \cite{Lodhi:2002:TCU:944790.944799} and trees 
%\citep[for example,][]{DBLP:conf/nips/CollinsD01,Aiolli2009,Kimura:2011:SKR:2017863.2017871} 
(for example, \cite{DBLP:conf/nips/CollinsD01,Aiolli2009,Kimura:2011:SKR:2017863.2017871}). 
Differently, the third family uses a recursive application of learning machines on structured objects \cite{}. This strategy has been recently revitalized in the context of neural networks \cite{}. 
%Simple machines are applied many times on structured objects by recursively following their structures.

%In fact, fast linear machines use tiny fractions of structural information whereas full-structure-aware machines are computationally complex. Only  


\LD{The three families of strategies are not equivalent: 
computational complexity and awareness of structural information seem to be incompatible goals.}
Computationally-efficient linear learning machines (for example, \cite{Joachims:2006:TLS:1150402.1150429,DBLP:journals/mp/Shalev-ShwartzSSC11,Pegasos,OLL}) require explicit vector spaces. Hence, only the first family of strategies can be used. Unfortunately, as it represents structures with small sets of substructures, large part of structural information is lost. Thus, to fully exploit the information encoded in structures, one of the other two families is needed. But, consequently, computationally more complex learning machines are required: recursive learning machines and kernel machines, 
required by graph kernels, are more complex than linear machines. In fact, recursive learning machines  require a recursive application to each structured object and kernel machines (see \cite{cristianiniSVM})
%, such as support vector machines \cite{SVM_CV:1995} and kernelized on-line learning algorithms \cite{Cavallanti:2007:TBH:1296038.1296052,DBLP:conf/nips/DekelSS05,Orabona:2008:PBK:1390156.1390247}, 
apply kernel functions thousands of times during learning and classification. Thus, the complexity of graph kernel functions is amplified. Although many approaches 
%Consequently, controlling the computational complexity of convolution kernels has become a major issue 
\cite{Moschitti2006,Rieck:2010:ATK:1756006.1756022,Pighin:Moschitti:conll:2010,ICML2011Shin_503}
%. These approaches 
have drastically lowered the average execution time of kernel machines with graph kernels, their computational complexity 
%of these machines 
in the learning phase has never been reduced.
% to the one of linear learning machines. 

%\OCAR{In this paper, we investigated whether low computational complexity and fully awareness of structural information can coexist in an approach for defining learning machines.} {}{We stemmed from the idea of \emph{distributed trees} \cite{Zanzotto2012193} -- low dimensional vectors whose dot product can approximate tree kernels \cite{Collins2002}. By embedding the huge feature space of tree kernels in low dimensional vectors, distributed trees have opened the possibility to use linear learning machines. Thus, here, we explored further this possibility by generalizing the approach to convolution kernels on countable sets (CK)\footnote{As results on these paper are valid only for convolution kernels on countable sets, we frequently omit to specify \emph{'on countable sets'} for the sake of readability.}\cite{Haussler99convolutionkernels}. We thus introduced \emph{distributed convolution kernels on countable sets} (DCK). Then, we formally and empirically proved that these DCKs can approximate the fully structural awareness of convolution kernels in linear support vector machines  \cite{Joachims:2006:TLS:1150402.1150429,DBLP:journals/mp/Shalev-ShwartzSSC11}.}{}

%when learning with structured data
\OCAR{In this paper, we challenged the dogma that low computational complexity and fully awareness of structural information are incompatible goals.}{We propose to encode structured data in \emph{distributed convolution structures}, which are small vectors whose dot products approximate convolution kernels. Consequently, learning can be carried out with computationally-efficient linear machines.}{Our approximation is valid and effective for convolution kernels \cite{Haussler99convolutionkernels} restricted on discrete sets\footnote{Sets endowed with the Kronecker's delta function}. In fact, these restricted convolution kernels $K(x,y)$ between structures $x$ and $y$  were rewritten as weighted sums of Kroneker's delta functions $\delta(a,b)$ over substructures $a$ and $b$ of $x$ and $y$, respectively. 
By approximating this delta function $\delta(a, b)$  
%between substructures 
with dot products of small vectors $\df_{\Sigma}(a)$ and $\df_{\Sigma}(b)$ called  \emph{distributed substructures}, we showed that these convolution kernels $K(x,y)$ are approximated with dot products between \emph{distributed structures} $D_{\Sigma}(x)$ and $D_{\Sigma}(y)$. Distributed structures $D_{\Sigma}(z)$ are weighted sums of distributed substructures of structures $z$. 
Our main result is that both distributed substructures $\df_{\Sigma}(x)$ and distributed structures $D_{\Sigma}(x)$ are computed efficiently with two recursive formulations: the \emph{distributed convolution substructures} $\df(x)$ and the \emph{distributed convolution structures} $D(x)$.  
Hence, convolution kernels on discrete sets can be efficiently approximated with dot products between \emph{distributed convolution structures}.
We then realized 5 different formulations of distributed convolution structures to approximate tree kernels \cite{Collins2002}, subpath tree kernels \cite{Kimura:2011:SKR:2017863.2017871}, route kernels \cite{Aiolli2009}, sequence kernels \cite{Lodhi:2002:TCU:944790.944799} and partial tree kernels \cite{Kashima:2002:KSD:645531.656021,Moschitti2006b}. Experiments show that distributed convolution structures approximate original kernels on simulated and real tasks with statistical significance.}{ Thus, low computational complexity and fully awareness of structural information can coexist. Distributed convolution structures approximated convolution kernels and drastically reduce the complexity of learning and classification when applied to real kernels.}



The rest of the paper is organized as follows. Section \ref{sec:method} introduces and proves the method in two steps. First, a preliminary model that show how to approximate convolution kernels with distributed substructures $\df_{\Sigma}(a)$ and distributed structures $D_{\Sigma}(z)$. Second, a recursive formulation -- the distributed convolution substructures $\df(a)$  and the distributed convolution structures $D(z)$ -- 
that makes the method computationally efficient. Section \ref{sec:attk} describes the five realizations of the distributed convolution kernels: the distributed tree kernel (DTK), the distributed subpath tree kernel (DSTK), the distributed route tree kernels (DRTK), the distributed string or sequence kernel (DSK), and the distributed partial tree kernel (DPTK). 
Then, Section \ref{sec:complexity} formally investigates the complexity of the derived distributed convolution kernels along with linearized versions of kernel machines \cite{Joachims:2006:TLS:1150402.1150429,DBLP:journals/mp/Shalev-ShwartzSSC11}.
Section \ref{sec:experiments} empirically determines how well distributed convolution kernels approximate the corresponding convolution kernels.
Finally, Section \ref{sec:conclusions} draws some conclusions and it sketches the future work.


%\textbf{Rest of the intro to remove}
%
%
%
%Our result we present is based on the fact that each convolution kernel has an associated distributed convolution structure (DCS). The distributed convolution kernel is the dot product between two distributed convolution structures and \mysecondinsert{it} approximates the corresponding convolution kernel. The paper gives a formal and an empirical demonstration of the validity of this approximation.
%%We first formally demonstrate that each CK has a corresponding DCK by introducing the \emph{distributed convolution structure on contable sets} (DCS). 
%We then apply the idea to five convolution kernels: the tree kernels \cite{Collins2002} (generalizing what was done in \cite{Zanzotto2012193}), the subpath tree kernels \cite{Kimura:2011:SKR:2017863.2017871}, the route tree kernels \cite{Aiolli2009}, the string or sequence kernels \cite{Lodhi:2002:TCU:944790.944799}, and, finally, the partial tree kernels \cite{Kashima:2002:KSD:645531.656021,Moschitti2006b}.  
%We formally describe the complexity of the distributed convolution kernels within the family of the linear support vector machines and we compare this to the complexity of the CKs used in kernelized support vector machines. 
%We finally investigate how well distributed convolution kernels approximate the convolution kernels.
%
%
%
%In a recent work \cite{Zanzotto2012193}, we introduced the \emph{distributed tree kernels} as a different opportunity to use tree structured data in learning classification functions or regressors and in clustering algorithms. 
%
%Stemming from the distributed representations \cite{Hinton:1986,McClelland:Rumelhart:1986,Plate1995}, the key ideas are: (1) to embed the original huge feature spaces of the tree fragments into lower dimensional spaces and (2) to obtain vectors for the trees in the reduced space by composing vectors for the nodes. 
%
%
%
%A direct embedding of the huge space in the smaller is, in principle, possible with techniques like singular value decomposition or random indexing (see \cite{sahlgren05}) but it is impractical. By opening the possibility of using linearized versions of kernel machines, DTKs drastically reduce the complexity of the overall learning and classification phases with respect to the corresponding kernelized versions, with the initial formulation of the tree kernels \cite{Collins2002} or with the more efficient approaches \cite{Moschitti2006,Rieck:2010:ATK:1756006.1756022,Pighin:Moschitti:conll:2010,ICML2011Shin_503}.  
%
%
%
%%whether it is possible to define
% learning machines that are both
%
% can be used only in combination with the strategy of selecting small sets of substructures.   
%
% Kernel machines or recursive neural networks are needed to fully use structural information.  
%
%On the contrary, 
%
%%-- and in many areas -- for example, biology \cite{Vert2002,Hashimoto:2008:MST:1452595.1452601}, computer security \cite{Dussel:2008:IAL:1496255.1496280}, and natural language processing \cite{DBLP:conf/nips/CollinsD01,Zhang:2003:QCU:860435.860443,MoschittiPighinBasili-SRL-CL-2008,Zanzotto2009c}. 
%
%
%
%
%
%\cite{Haussler99convolutionkernels} shows that CKs are valid kernel functions and the underlying feature spaces of substructures are clearly defined. 
%
%
%
%
%To learn with structure data we need complex machines: kernel machines or recursive neural networks.
%
%
%
%
%
%
%Kernel machines are the tools for successfully using convolution kernels, but \mysecondremove{also} \mysecondinsert{these machines bring} a computational limitation. Faster linearized versions of kernel machines such as \cite{Joachims:2006:TLS:1150402.1150429,DBLP:journals/mp/Shalev-ShwartzSSC11} cannot be used.
%
%
%In a recent work \cite{Zanzotto2012193}, we introduced the \emph{distributed tree kernels} as a different opportunity to use tree structured data in learning classification functions or regressors and in clustering algorithms. Stemming from the distributed representations \cite{Hinton:1986,McClelland:Rumelhart:1986,Plate1995}, the key ideas are: (1) to embed the original huge feature spaces of the tree fragments into lower dimensional spaces and (2) to obtain vectors for the trees in the reduced space by composing vectors for the nodes. 
%A direct embedding of the huge space in the smaller is, in principle, possible with techniques like singular value decomposition or random indexing (see \cite{sahlgren05}) but it is impractical. By opening the possibility of using linearized versions of kernel machines, DTKs drastically reduce the complexity of the overall learning and classification phases with respect to the corresponding kernelized versions, with the initial formulation of the tree kernels \cite{Collins2002} or with the more efficient approaches \cite{Moschitti2006,Rieck:2010:ATK:1756006.1756022,Pighin:Moschitti:conll:2010,ICML2011Shin_503}.  
%
%%This complexity has always been an important issue for the applicability of these kernel functions. In the initial formulation and in more recent studies  where computation time is reduced, the complexity of the kernel computation always depends on the size of the trees. 
%
%%\emph{Distributed tree kernels}, instead, reduce the complexity to a constant value depending on the size of the low dimension spaces. 
%
%%\emph{Distributed tree kernels} have also other advantages. As directly exploiting vectors, the approach gives the possibility to use linear support vector machines for tree structured data. This possibility is not given with the traditional tree kernels (e.g., \cite{Collins2002}) even if classes are linearly separable on the explicit feature spaces of the tree fragments. 
%
%
%
%%The first issue to address is how we can assign a vector $\svec{x}$ in $\R^d$ to a structured object $x$. Directly assigning a random vector to each structured object  as a random index \cite{sahlgren05} of the space $X$  is infeasible. This space is combinatorial with respect to the ground sets. We here follow a different approach. In line with the compositional approach in the distributed representations \cite{Plate1995}, we define the mapping function $\df$ in a compositional way: the vector of the structure is obtained by composing the basic vectors of the terminal objects.
%
%
%
%
%
%
%%As representing trees in small dimension vectors is a promising approach, in this paper we want to go beyond the previous results by investigating two issues: the theoretical limits of the approach and the possibility of extending it to other tree-based kernels. 
%
%%Understanding the theoretical limits is extremely relevant as it is useful to know to which extent low dimension spaces can embed high dimension spaces. This issue is strictly related to a conjecture on the number of nearly-orthogonal vectors in a vector space \cite{HechtNielsen94}. We here report a proof based on the Johnson-Lindenstrauss Lemma \cite{JLL} that investigates a slightly modified version of the conjecture. 
%
%%We introduced the approach for the \cite{Collins2002}'s tree kernels, hereafter called parse tree kernels (PTK) as firstly devoted to parse trees. But, distributed 
%%wrong: tree kernels have clearly the potentiality to
%%tree kernels clearly have the potential to
%%be applied for many kernels defined over trees.  For completeness, we first discuss the application of the distributed idea to the parse tree kernels \cite{Collins2002} as presented in \cite{Zanzotto2012193}. Then, we show how the idea can be applied to the subpath tree kernels (STK) \cite{Kimura:2011:SKR:2017863.2017871} and to route tree kernels (RTK)  \cite{Aiolli2009}.
%%This approach thus opens the possibility to define a novel class of kernels. Existing tree kernels can be transformed in distributed counterparts: distributed parse tree kernels (DPTK), distributed subpath tree kernels (DSTK), and distributed route tree kernels (DRTK).
%
%
%%(3) application of the distributed comnvolution structures to existing convolution kernels
%%(4) analysis of the computational complexity the linear SVM with distributed convolution kernels and the kernelized version of SVM with the related convolution kernels
%%(5) empirical investigation of the property in equation (\ref{the_core_property})







\section{The method: Distributed Convolution Kernels on Structured Objects}
\label{sec:method}


\OCAR{When building learning machines over structures,}{ low computational complexity and fully awareness of structural information seem to be incompatible goals.}{A spark to accomplish both goals
% in a single model 
is hidden in one of the current strategies for learning with structures: convolution kernels \cite{Haussler99convolutionkernels}. Stemming from this spark, we propose to approximate convolution kernels with dot products of \emph{distributed convolution structures}, which are low dimensional vectors.}{By representing structures with low dimensional vectors, we enable learning over structured data with linear learning machines. Since linear learning machines have better complexity with respect to kernel machines, this section prove that these two goals can coexist.}


The section introduces our method in two steps. First, Section \ref{sec:preliminaries_DF} introduces the model and describes the preliminary result on approximating convolution kernels using \emph{distributed substructures} $\df_{\Sigma}(a)$ and \emph{distributed structures} $D_{\Sigma}(x)$. Second, Section \ref{sec:fra} presents the recursive and efficient formulation --  the \emph{distributed convolution substructures} $\df(a)$ and the \emph{distributed convolution structures} $D(x)$ -- and formally proves that each convolution kernel has corresponding distributed convolution structure that approximate it.


\subsection{Preliminaries and Direct Formulation}
\label{sec:preliminaries_DF}

This section introduces the basic elements and the first formulation of  our method: a recursive definition for structured objects, the result that convolution kernels on discrete set can be expressed as summations of Kronecker's delta on the substructures, a basic set of nearly-orthogonal unit vectors for defining \emph{distributed substructures} $\df_{\Sigma}(a)$ to approximate Kronecker's delta between substructures and, finally, the proof that convolution kernels can be approximated with dot products between \emph{distributed structures} $D_{\Sigma}(x)$. A full map of relevant symbols is instead given in Appendix \ref{section:symbols}. 


\subsubsection{Structured objects}

\LD{To propose an innovative way to treat structured objects in learning, we need a formal definition of what structured objects are.} 

%Although simple, the definition of  structured objects in convolution kernels \cite{Haussler99convolutionkernels} is unsatisfactory to explain actual convolution kernels. Hence, we proposed a recursive definition for structured objects that better subsumes structured objects in actual convolution kernels. 

%As our approach changes the way structured data is used in learning machines, we need first to define what structured data are. 




\LD{Convolution kernels use a simple and effective definition for structured objects \cite{Haussler99convolutionkernels} but this definition is not specific enough for actual convolution kernels.} In fact, to prove general properties of convolution kernels, 
%this definition is perfect. 
%For this definition, 
structured objects are decomposed only at the first level. Objects $x$ are thus seen through their direct subparts. Unfortunately, many, if not all, actual convolution kernels rely on slightly more complex recursive definitions that are different case by case \cite{Collins2002,Kimura:2011:SKR:2017863.2017871,Aiolli2009,Lodhi:2002:TCU:944790.944799,Kashima:2002:KSD:645531.656021,Moschitti2006b,Croce:2011:SLS:2145432.2145544,Mehdad:2010:SST:1857999.1858144}.


\finalcomment{In Section 2, the authors show a lot of goodwill and try to simplify the analysis of the paper to make it intuitive. I have, however, struggled with the notations (again). What is $G_{X_i}$ ? isn't there a unique ground set? $\omega_i$ is used in the middle of p.5 in a string context, it's used 3 lines below as (what I guess is) a real number. In the same page, "A possible set R(t)", but what such sets R actually mean is not defined. Bottom of  p.6 and p.7, the set S(z) has no definition either.}{R3.2}
%\cite{Haussler99convolutionkernels} \mysecondremove{exploits}\mysecondinsert{defines convolution kernels on} the first level of decomposition of an object $x$, but many, if not all, actual convolution kernels %\cite{Collins2002,Kimura:2011:SKR:2017863.2017871,Aiolli2009,Lodhi:2002:TCU:944790.944799,Kashima:2002:KSD:645531.656021,Moschitti2006b,Croce:2011:SLS:2145432.2145544,Mehdad:2010:SST:1857999.1858144} are defined through recursive functions and, thus, exploit recursive definitions of structured objects. For these convolution kernels, 



\LD{We thus proposed a \emph{recursive definition of structured objects} that generalizes definitions given in actual convolution kernels.} In our view, \textbf{structured objects} $x \in X$ are either terminal objects $x \in G_X$ that cannot be furthermore decomposed or non-terminal objects that can be decomposed into \myinsert{$M$} parts $\substr{x} = (x_1,...,x_M) \in X_1 \times \ldots \times X_M$. Parts $x_i \in X_i$ are again structured objects. 
%that can be decomposed into parts  $\substr{x} = (x_1,...,x_M) \in X_1 \times \ldots \times X_M$. 
%In the following, we refer to a structured object by $x$ when its decomposition into parts is not relevant and by $\substr{x}$ when the focus is on its decomposition. 
Thus, $X$ is a set of objects, $X_i$ are sets containing the $i$-th parts of objects $x \in X$, and $G_X$ is a \emph{ground set} that contains terminal objects including the empty object $\emptyset$. 
Finally, given a structured object $x$, we define $R(x)$ as the set of all possible decompositions into parts of $x$.


\LD{The recursive definition of structured objects adapts to specific convolution kernels by 
specializing the decomposition strategy and 
%, the set of valid substructures $S$,  
the sets $X$, $X_i$ and $R(x)$.}
The following examples of recursive structured objects aim to clarify the recursive definition, the related notation, and how it adapts to specific kernels:
%. Structured objects as defined above include trees and other objects that can be recursively define: 
\begin{itemize}
\item
In sequence kernels \cite{Lodhi:2002:TCU:944790.944799}, a sequence is $s=\myremove{w_1 \ldots w_k}\myinsert{\texttt{w}_1 \ldots \texttt{w}_k} \in C^{*}$ and its  decomposition into parts is $\substr{s}=(\texttt{w}_1,\texttt{w}_2 \ldots \texttt{w}_k) \in C \times C^{*}$.  For example, given the sequence $\texttt{W}_1 \texttt{W}_2 \texttt{W}_3 \texttt{W}_4$, a decomposition into parts is $(\texttt{W}_1,\texttt{W}_2 \texttt{W}_3 \texttt{W}_4)$. The set $R(s)$ for the same sequence is:
$$
R(\texttt{W}_1 \texttt{W}_2 \texttt{W}_3 \texttt{W}_4) = \{(\texttt{W}_1,\texttt{W}_2 \texttt{W}_3 \texttt{W}_4), (\texttt{W}_2,\texttt{W}_3 \texttt{W}_4), (\texttt{W}_3,\texttt{W}_4)\}
$$
%and the set $S(s)$ for 2-element sequences with holes is:
%$$
%S(\texttt{W}_1\texttt{W}_2\texttt{W}_3\texttt{W}_4) = \{\texttt{W}_1, \texttt{W}_2,  \texttt{W}_3, \texttt{W}_4,  \texttt{W}_1\texttt{W}_2,  \texttt{W}_1\texttt{W}_3,  \texttt{W}_1\texttt{W}_4, \texttt{W}_2\texttt{W}_3, \texttt{W}_2\texttt{W}_4, \texttt{W}_3\texttt{W}_4\}
%$$

\item
In tree kernels \cite{Collins2002}, given a tree $t \in T$, a decomposition into parts is $\substr{t} = (r,c_1,\ldots, c_k) \in N \times T^{k}$ where $N$ is the set of nodes, $r$ is the root of the tree and $c_i \in T$ are the subtrees of $t$ rooted in the $i$-th child of the root $r$.
For example, the tree in Fig. \ref{sampletree} can be decomposed as:
\begin{center}
 \texttt{(A,(B W1),(C (D W2)(E W3)))}
\end{center}
where \texttt{A} is the root label, \texttt{(B W1)} and \texttt{(C (D W2)(E W3))} are the two children subtrees of \texttt{A}.
A possible set $R(t)$ is the one used for the parse tree kernel (see Section \ref{sec:T} for details), that includes the decomposition of the subtree rooted in each non-terminal node of tree $t$:\\
\begin{center}
 $R(t)=\{$\texttt{(A,(B W1),(C (D W2)(E W3)))},\texttt{(B,W1)},\\\texttt{(C,(D W2),(E W3))},\texttt{(D,W2)},\texttt{(E,W3)}$\}$
\end{center}
%Whereas, a possible set $S(t)$ of all the substructures of the given tree is:
%\begin{center}
%\begin{eqnarray*}
%S(t
%%\begin{array}{c}\parsetree{(.A. (.B. .W1.) (.C. (.D. .W2.)(.E. .W3.)))}\end{array}
%)&=&\{
%\begin{array}{c}\parsetree{(.A. . B . . C  .)}\end{array},
%\begin{array}{c}\parsetree{(. B . .W1.)}\end{array},
%\begin{array}{c}\parsetree{(.A. (. B . .W1.) . C .)}\end{array},
%\begin{array}{c}\parsetree{(.A. .B. (.C. . D . . E .))}\end{array},
%\begin{array}{c}\parsetree{(.A. .B. (.C. (. D . .W2.) . E . ))}\end{array},
%\begin{array}{c}\parsetree{(.A. .B. (.C. . D . (. E . .W3.)))}\end{array},
%\\&&
%\begin{array}{c}\parsetree{(.A. .B. (.C. (.D. .W2.)(.E. .W3.)))}\end{array},
%\begin{array}{c}\parsetree{(.A. (.B. .W1.) (.C. (.D. .W2.)(.E. .W3.)))}\end{array},
%\begin{array}{c}\parsetree{(.C. (. D . .W2.) . E . )}\end{array},
%\begin{array}{c}\parsetree{(.C. . D . (. E . .W3.))}\end{array},
%\begin{array}{c}\parsetree{(.C. (.D. .W2.)(.E. .W3.))}\end{array},
%\begin{array}{c}\parsetree{(. E . .W3.)}\end{array},
%\begin{array}{c}\parsetree{(. D . .W2.)}\end{array}
%\}
%\end{eqnarray*}
%\end{center}


\end{itemize}




\finalsecondcomment{R3.1}{in definition 1,  "zeros" or "empty objects" appear out of nowhere. Are those objects in $G_X$? The definition of convolution kernels is itself recursive. In the definition of the convolution kernel, the case where either x or y = $\emptyset$ is never considered, so how can it appear in the recursive formulation? Something is clearly wrong or missing. What is the "weight" of $w_\emptyset$?}







\subsubsection{Convolution Kernels as Dot Products on Spaces of Substructures}
\label{sec:convolution_kernel_def}


\OCAR{Clearly, the recursive definition of structured objects does not change a basic result of convolution kernels $K(x,y)$ \cite{Haussler99convolutionkernels}:}{$K(x,y)$ restricted to discrete sets are dot products of vectors $\vec{x},\vec{y} \in \R^m$ where base vectors $\OBV{a}\in \R^m$ represent substructures $a$ of $x$ and $y$.}{Discrete sets are sets endowed with the Kronecker's delta function $\delta(a,b)$ that 
is one if $a = b$ and zero otherwise. Hence, $\dotprod{\OBV{a}}{\OBV{b}} =\delta(a,b)$. Thus , the basic result is more formally:
\begin{equation}
K(x,y) = \dotprod{\vec{x}}{\vec{y}} 
%= \displaystyle\sum_{a\in S(x), b \in S(y)} \omega_{a}\omega_{b} \delta(a,b) 
= \dotprod{\displaystyle\sum_{a\in S(x)} \omega_{a} \OBV{a}}{\displaystyle\sum_{b \in S(y)} \omega_{b} \OBV{b}}
\label{K_as_deltas}
\end{equation}
\finalsecondcomment{R3.4}{"$w_a$ and $w_b$ are the weights of substructures a and b." what is a weight mathematically speaking? if $w_a$ is a weight, what part of a total mass does $w_a$ represent?}
where $S(x)$ and $S(y)$  are discrete sets of all substructures of $x$ and $y$ that are relevant for the given kernel, $\omega_{a}$ and $\omega_{b}$ are the values (or weights) of structures $a$ and $b$.}
%
%The $\delta(a,b)$ function is defined on the discrete, possibly infinite, set $S = \bigcup_{z} S(z)$ that is the union of sets $S(z)$ for all the considered structured objects $z$. 
{}

\LD{This section provides an alternative demonstration of the above result in our settings since this is an important step to demonstrate that convolution kernels can be efficiently approximated with dot products of  low dimensional vectors.} The purpose is to fix notation and to provide basic tools for our challenge.



%We here provide a reformulation of the definition of c
\LD{The first step of the demonstration is to reformulate convolution kernels to make explicit their behavior on structured objects that are recursively defined and are restricted to discrete sets.} 
%The definition was restricted to convolution kernels on countable sets. 
Thus, the basic sets of structured objects -- $X$, $X_i$ and $G_X$ -- are discrete sets.
\finalcomment{Equation 1 seems to require that both x and y must be decomposed into the same number of parts. This should be addressed in some way}{R1.1}
\finalcomment{Definition 1 is narrower than Haussler's definition of convolution kernels due to the delta function. It would probably be best to note this fact.}{R1.2}
\finalsecondcomment{R3.2}{Definition 1 is not, as a matter of fact, Haussler's definition for convolution kernels. His definition is far more clear. Please differentiate your definition from Haussler's.}

\begin{definition}
\label{convolution_kernel_def}
A convolution kernel on discrete sets is defined as follows:
\begin{displaymath}
K(x,y) = 
\begin{cases}
\omega_x\omega_y \delta(x,y) & \text{if $x$ or $y$ \mysecondremove{can not be decomposed}\mysecondinsert{belongs to $G_X$}}\\
\displaystyle\sum_{\substr{x} \in R(x), \substr{y} \in R(y)} \displaystyle\prod_{i=1}^{M_{\myinsert{max}}} K_i(x_i,y_i)& \text{otherwise}
\end{cases}
%\label{convolution_kernel}
\end{displaymath}
where $K_i(x_i,y_i)$ are convolution kernels, possibly recursively defined\myinsert{, and $M_{max}=\max\{M_x,M_y\}$ being $M_x$ and $M_y$ the numbers of parts that, respectively, $\substr{x}$ and $\substr{y}$ are decomposed in. Finally, $x_i=\emptyset$ if $i>M_x$, $y_i=\emptyset$ if $i>M_y$}, \mysecondinsert{$\omega_{\emptyset}=0$, and $\delta(\emptyset,y)=\delta(x,\emptyset)=0$ for any $x$ and $y$}.
\end{definition}



%\LD{The second step is to define sets $S(x)$  that are the sets of substructures for structures $x$ that are specific for a convolution kernel $K(x,y)$.} 

\LD{The second step is to define sets $S(x)$ -- that contain substructures for structures $x$ -- for a specific convolution kernel and the subsequent set $S$ collecting all sets S(x) for structures in $X$.} Elements of these sets are substructures. 
A substructure of $x$ is a structured object that is a subpart of $x$ and it is considered relevant from the specific convolution kernel. For example, in sequence kernels, a relevant subsequence of a sequence $s$ is a sequence with a subset of the elements of $s$ but in the same order. Thus, for each convolution kernel,  we introduced the related sets $S(x)$ recursively defined as follows:
\begin{definition}
\label{substructure_function}
A convolution kernel \mysecondinsert{$K$} has an associated substructure function $S(x)$, defined as:
\begin{equation}
 S(x) = 
\begin{cases}
\mysecondinsert{\{x\}} & \mbox{if \mysecondinsert{$x\in G_X$} \mysecondremove{is a terminal object}}\\
 \bigcup_{\substr{x} \in R(x)} {\mathbb S}(\substr{x}) & \mbox{otherwise}
\end{cases}
\end{equation}
where ${\mathbb S}(\substr{x}) = S_1(x_1) \times  \ldots \times S_M(x_M)$. \mysecondremove{Functions $S_i$ are recursively induced by kernels $K_i$.} \mysecondinsert{By recursion, functions $S_i(x_i)$ are the substructure functions associated to convolution kernels $K_i$ decomposing the convolution kernel $K$ (see Definiton \ref{convolution_kernel_def}).}
\finalsecondcomment{R3.7}{Definition 7: "Functions $S_i$" are not defined. What are the input and output spaces of each of these functions? Because S(x) = x if x is in $G_X$, it seems S is a function from $G_X \rightarrow G_X$. On the other hand, it seems that S functions might be set valued, in which case S(x) should be $\{x\}$.}
\finalsecondcomment{R3.8}{"Functions Si are recursively induced by kernels Ki." I do not understand the meaning of this statement.}
\end{definition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% DEFINIZIONE DI S
Then, the set $S$ was derived by collecting all valid substructures for structured objects in $X$, that is,  $S = \bigcup_{x \in X} S(x)$. The set $S$ is possibly infinite
and it is in principle different from the set $X$ of the structures although these two sets can share the same definition of structured object. For example, in tree kernels \cite{Collins2002} applied to parse trees of natural language utterances, the set $X$ contains full parse trees whereas the set $S$ contains fragments of these parse trees. The two sets shares the same structured data type, but the nature of the two sets is different.  

\LD{Thus, we proved that 
that convolution kernels $K(x,y)$ restricted to discrete sets are dot products of vectors $\vec{x},\vec{y} \in \R^m$ where base vectors $\OBV{a}\in \R^m$ represent substructures $a \in  S$.
%convolution kernels on discrete sets are sums of delta functions over sets of relevant substructures defined over sets $S$.
} Thus, if the basic sets  $X$, $X_i$ and $G_X$ are endowed with Kroneker's delta, also the set $S$ is endowed with it.


\finalsecondcomment{R3.9}{Proof of lemma 8: by induction on what?}
\finalcomment{Lemma 8: also seems to require that a and b decompose into the same number of parts, which will not be true in general. This needs to be addressed.}{R1.6}
\finalcomment{proof of lemma 8" "and assuming that $\omega_{\bar{a}}$" : do you assume this or do you define this? if this is a definition, wouldn't it be better to have it earlier?}{R3.8}
\begin{lemma}
\label{k_in_deltas}
A convolution kernel on discrete sets can be expressed in the form:
\begin{equation}
K(x,y) = \dotprod{\vec{x}}{\vec{y}}=\dotprod{\displaystyle\sum_{a\in S(x)} \omega_{a} \OBV{a}}{\displaystyle\sum_{b \in S(y)} \omega_{b} \OBV{b}}
\label{k_to_be_demonstrated}
\end{equation}
%and:
%\begin{displaymath}
%\delta(a,b)  = \prod_k \delta(a_k,b_k) 
%\end{displaymath}
%where $S(x)$ and $S(y)$ are the substructures of $x$ and $y$. 
\end{lemma}

\begin{proof}
%As convolution kernels are generally defined through recursive functions, t
The proof is by \mysecondinsert{structural} induction.
\begin{paragraph}{Basic step}
Let $x$ and $y$ be structures that are not decomposable into parts, i.e. $S(x)=\{x\}$ and $S(y)=\{y\}$. 
Equation (\ref{k_to_be_demonstrated}) becomes $K(x,y) = \omega_{x}\omega_{y} \delta(x,y) = \dotprod{\omega_{a} \OBV{a}}{\omega_{b} \OBV{b}}$.

%, that is exactly the function $K(x,y)$ directly defined on a countable set endowed with \mysecondremove{a}\mysecondinsert{the} Kroneker delta function.

\end{paragraph}
\begin{paragraph}{Induction step}
By induction, we have that Equation (\ref{k_to_be_demonstrated})  holds for each $K_i(x_i,y_i)$ of the definition of the convolution kernel:
\begin{displaymath}
K_i(x_i,y_i) =  \dotprod{\displaystyle\sum_{a_i\in S(x_i)} \omega_{a_i} \OBV{a_i}}{\displaystyle\sum_{b_i \in S(y_i)} \omega_{b_i} \OBV{b_i}} = 
\displaystyle\sum_{a_i \in S(x_i), b_i \in S(y_i)} \omega_{a_i}\omega_{b_i} \delta(a_i,b_i)
\end{displaymath}
Thus, we can express the convolution kernel $K(x,y)$:
\begin{displaymath}
K(x,y) = \displaystyle\sum_{\substr{x} \in R(x), \substr{y} \in R(y)} \hspace{.3em} \displaystyle\prod_{i=1}^{M_{\myinsert{max}}} \hspace{.3em} \displaystyle\sum_{a_i \in S_i(x_i), b_i \in S_i(y_i)} \omega_{a_i}\omega_{b_i}\delta(a_i,b_i)
\end{displaymath}
\myinsert{where $M_{max}=\max\{M_x,M_y\}$ being $M_x$ and $M_y$ the numbers of parts that, respectively, $\substr{x}$ and $\substr{y}$ are decomposed in. Finally, $x_i=\emptyset$ if $i>M_x$, $y_i=\emptyset$ if $i>M_y$, and $S_i(\emptyset)=\{\emptyset\}$.}
Hence:
\begin{displaymath}
K(x,y) = \displaystyle\sum_{\substr{x} \in R(x), \substr{y} \in R(y)} \hspace{.3em} \displaystyle\sum_{\substr{a} \in  {\mathbb S}(\substr{x}), \substr{b} \in {\mathbb S}(\substr{y})} \hspace{.3em} \displaystyle\prod_{i=1}^{M_{\myinsert{max}}} \omega_{a_i}\omega_{b_i} \displaystyle\prod_{j=1}^{M_{\myinsert{max}}}\delta(a_j,b_j)
\end{displaymath}
%where ${\mathbb S}(\substr{x}) = S_1(x_1) \times  \ldots \times S_M(x_M)$ and ${\mathbb S}(\substr{y}) = S_1(y_1) \times  \ldots \times S_M(y_M)$.\\
according to Definition \ref{substructure_function}.\\
Finally, since $\delta(\substr{a},\substr{b})= \displaystyle\prod_{i=1}^{M_{\myinsert{max}}}\delta(a_i,b_i)$ and \myremove{assuming that} \myinsert{defining} $\omega_{\substr{a}}= \displaystyle\prod_{i=1}^{M_{\myinsert{max}}} \omega_{a_i}$ and  $\omega_{\substr{b}}= \displaystyle\prod_{i=1}^{M_{\myinsert{max}}} \omega_{b_i}$,
%\end{displaymath}
we obtain:
\begin{displaymath}
K(x,y) = \displaystyle\sum_{a \in S(x), b \in S(y)} \omega_{a}\omega_{b} \delta(a,b) = \dotprod{\displaystyle\sum_{a\in S(x)} \omega_{a} \OBV{a}}{\displaystyle\sum_{b \in S(y)} \omega_{b} \OBV{b}}
\end{displaymath}
\end{paragraph}
\end{proof}

\LD{The above result is  the avenue to approximate convolution kernels on discrete sets.}

%\OCAR{The above result opens the avenue to approximate convolution kernels on discrete sets.}{ We need to show that it is possible to approximate the Kronecker's delta $\delta(a,b)$ with the dot product between low-dimensional vectors.}{ This is a second step towards the approximation of convolution kernels $K(x,y)$ with dot products of distributed convolution structures $D(x)$ and $D(y)$.}{}



\subsubsection{Direct Embedding Spaces of Substructures in Reduced Spaces: Potential and Limits}
\label{sec:direct_embedding}

\LD{Given that convolution kernels are dot product in spaces $\R^m$ of substructures and using the  theory on the Johnson\&Lindestrauss Tranform \cite{JLL}, it is possible to derive a first conclusion: convolution kernels can be approximated with dot product of \emph{distributed structures} $D_{\Sigma}(z)$ that are vectors in $\R^d$ with $d<<m$.} This section shows this first conclusion and analyzes its limits that Section \ref{sec:fra} resolves.


\LD{\emph{Distributed structures} $D_{\Sigma}$ are functions mapping structures in small vectors in $\R^d$: 
$$
D_{\Sigma}(z)= \displaystyle\sum_{c\in S(z)} \omega_{c} \df_{\Sigma}(c)
$$
where $\df_{\Sigma}$ is an injective function between substructures $S$ and vectors in $\R^d$. }
The aim is to build these functions such that:
$$
\P(K(x,y) - \varepsilon\J(x,y)<\dotprod{D_{\Sigma}(x)}{D_{\Sigma}(y)}<K(x,y) + \varepsilon \J(x,y) > 1 - \theta
$$
where $\varepsilon$ and $\theta$ are small and $\J(x,y)$ is an approximation factor depending on $x$ and $y$.



\LD{To approximate convolution kernels, the injective function $\df_{\Sigma}$  can be derived from practices in embedding with Johnson-Lindestrauss Transform (JLT) \cite{JLL} as adapted by random indexing \cite{random_indexing} or random projection \cite{random_projection}.} The general idea is that vectors  $\df_{\Sigma}(a)$ for substructures $a$ are drawn out of a d-dimensional Gaussian distribution $\vec v\sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ \cite{Indyk:1998:ANN:276698.276876,JLLsimple_demonstration}. 
%Then, we have two options. The first option is to build sets $NOV(\varepsilon,\theta)$ with $\theta=0$ in a polynomial time \cite{JLLsimple_demonstration} by adding only those vectors that are nearly orthogonal with respect to existing ones in a growing set $NOV(\varepsilon,\theta)$. The second option is to use  $\mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ as an virtually infinite $NOV(\varepsilon,\theta)$ drawing out vectors when needed.
We showed that given $\vec{a},\vec{b} \sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$, $\varepsilon$ and $d$,  it is possible to compute $\overline{\theta}$ such that:
\begin{equation}
\P(\delta(\vec{a},\vec{b}) - \varepsilon \leq \dotprod{\vec{a}}{\vec{b}} \leq \delta(\vec{a},\vec{b}) + \varepsilon) \geq 1- \overline{\theta}
\label{eq:probs}
\end{equation}
(see Appendix \ref{properties_of_normal_vectors}).
For example, $\overline{\theta} \propto 10^{-xxx}$ with $\varepsilon = 0.1$ and $d=yyy$. Hence, there is a very high probability that $\delta(\vec{a},\vec{b}) - \varepsilon \leq \dotprod{\vec{a}}{\vec{b}} \leq \delta(\vec{a},\vec{b}) + \varepsilon$.

%\LD{Thus, the core part of the answer is showing the existence of sets $NOV(\varepsilon,\theta)$ of nearly-orthogonal unit vectors in $\R^d$ with a cardinality $m=|NOV(\varepsilon,\theta)|>>d$.} %Consequently, the function $\df$ maps substructures in $S$ to vectors in $NOV(\varepsilon,\theta)$. 
%These sets should contain low-dimensional vectors such that, for each $\vec{a},\vec{b} \in {NOV(\varepsilon,\theta)}$, the dot product $\dotprod{\vec{a}}{\vec{b}}$ has the following property: 
%\begin{equation}
%\P(\delta(\vec{a},\vec{b}) - \varepsilon \leq \dotprod{\vec{a}}{\vec{b}} \leq \delta(\vec{a},\vec{b}) + \varepsilon) \geq 1- \theta
%\label{eq:nov}
%\end{equation}
%These sets along with the definition of a bijective mapping function $\df_{\Sigma}: S \rightarrow NOV(\varepsilon,\theta)$ guarantee that Equation \ref{eq:approx_delta_1} holds.
%% The orthonormal basis of a Hilbert space $\mathcal H$ of substructures is embedded in $\R^d$ 
%%to vectors in $NOV(\varepsilon,\theta)$  
%%since basis vectors in $\mathcal H$ represent substructures in $S$. 
%%The ideal situation is that $NOV(\varepsilon,\theta)$ is a discrete set of vectors
% %$\smallvectors{S}$ of the same cardinality of $S$ 
%%and $\df$ is a bijective mapping between the two sets. 

We refer to Equation \ref{eq:probs} as the property of vectors of being nearly orthonormal.

We thus demonstrated the following lemma:
\begin{lemma} 
\label{first_formulation}
For each convolution kernel on discrete sets $K(x,y)$, given an injective function $\df: S \rightarrow \R^d$ that maps substructures $a\in S$ to vectors $v \sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ and given the previously defined function $D_{\Sigma}$, the following property holds:
$$
\P(K(x,y) - \varepsilon\J(x,y)<\dotprod{D_{\Sigma}(x)}{D_{\Sigma}(y)} < K(x,y)  + \varepsilon \J(x,y)) > 1 - \theta
$$
where 
$\J(x,y) = \|\vec{x}\|^2 + \|\vec{y}\|^2 +  K(x,y)$.
\end{lemma}
\begin{proof}
%Let $\df$ be the matrix $\df=(\df(v_1)\ldots\df(v_k)\df(a_1)\ldots\df(a_h)\df(b_1)\ldots\df(b_g))$ where $v_i \in S(x) \cap S(y)$,  $a_i \in S(x) - S(x) \cap S(y)$ and $b_i \in S(y) - S(x) \cap S(y)$.
%Let $\Omega_x$ and $\Omega_y$ represent weigths $\omega$ as vectors.
We first observe that 
$$
\displaystyle\sum_{a\in S(x)} \omega_{a}^2 = \|\vec{x}\|^2 \text{ and }\displaystyle\sum_{b\in S(y)} \omega_{b}^2 = \|\vec{y}\|^2
$$ 
given the definition of $x$ and $y$ (see equation \ref{k_to_be_demonstrated}). 
Hence, distributed structures $D_{\Sigma}(x)$, $D_{\Sigma}(y)$ and $D_{\Sigma}(x)+D_{\Sigma}(y)$ behave as random vectors drawn from $\mathcal{N}(0,\frac{1}{d}\mathbb{I}_d\|\vec{x}\|^2)$, $\mathcal{N}(0,\frac{1}{d}\mathbb{I}_d\|\vec{y}\|^2)$ and $\mathcal{N}(0,\frac{1}{d}\mathbb{I}_d\|\vec{x}+\vec{y}\|^2)$ as $D_{\Sigma}(\cdot)$ are linear combinations of independent normal random vectors $\df_{\Sigma}(\cdot)$.

From the property in equation (\ref{eq:probs}), the following probabilities hold: 
%\begin{equation}
%\P(\|\df\Omega_x\|^2 > (1 + \varepsilon) \|\Omega_x\|^2, \|\df\Omega_x\|^2 < (1 - \varepsilon) \|\Omega_x\|^2  ) < \overline{\theta}
%\label{eq:1}
%\end{equation}
%\begin{equation}
%\P(\|\df\Omega_y\|^2 > (1 + \varepsilon) \|\Omega_y\|^2, \|\df\Omega_y\|^2 < (1 - \varepsilon) \|\Omega_y\|^2  ) < \overline{\theta}
%\label{eq:2}
%\end{equation}
%$$
%\P(\|\df(\Omega_x+\Omega_y)\|^2 > (1 + \varepsilon) \|(\Omega_x+\Omega_y)\|^2, \|\df(\Omega_x+\Omega_y)\|^2 < (1 - \varepsilon) \|(\Omega_x+\Omega_y)\|^2  ) < \overline{\theta}
%$$
\begin{equation}
\P(\|D_{\Sigma}(x)\|^2 > (1 + \varepsilon) \|\vec{x}\|^2, \|D_{\Sigma}(x)\|^2 < (1 - \varepsilon) \|\vec{x}\|^2  ) < \overline{\theta}
\label{eq:1}
\end{equation}
\begin{equation}
\P(\|D_{\Sigma}(y)\|^2 > (1 + \varepsilon) \|\vec{y}\|^2, \|D_{\Sigma}(y)\|^2 < (1 - \varepsilon) \|\vec{y}\|^2  ) < \overline{\theta}
\label{eq:2}
\end{equation}
$$
\P(\|D_{\Sigma}(x)+D_{\Sigma}(y)\|^2 > (1 + \varepsilon) \|\vec{x}+\vec{y}\|^2, \|D_{\Sigma}(x)+D_{\Sigma}(y)\|^2 < (1 - \varepsilon) \|D_{\Sigma}(x)+D_{\Sigma}(y)\|^2  ) < \overline{\theta}
$$
as vectors $\vec{v} \sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d B^2)$ behave as vectors $B\vec{v}'$ where $\vec{v}' \sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$.


The union of the above events has a probability $p<3\overline{\theta} = \theta$.


As 
$$
\|D_{\Sigma}(x)+D_{\Sigma}(y)\|^2 = \|D_{\Sigma}(x)\|^2 + \|D_{\Sigma}(y)\|^2 + 2 \dotprod{D_{\Sigma}(x)}{D_{\Sigma}(y)}
$$
and
$$
\|\vec{x}+\vec{y}\|^2  = \|\vec{x}\|^2 + \|\vec{y}\|^2 + 2 \dotprod{\vec{x}}{\vec{y}}
$$
we rewrite:
$$
\|D_{\Sigma}(x)+D_{\Sigma}(y)\|^2 > (1 + \varepsilon) \|\vec{x}+\vec{y}\|^2
$$
as:
$$
 \|D_{\Sigma}(x)\|^2 + \|D_{\Sigma}(y)\|^2 + 2 \dotprod{D_{\Sigma}(x)}{D_{\Sigma}(y)} > (1 + \varepsilon) (\|\vec{x}\|^2 + \|\vec{y}\|^2 + 2 \dotprod{\vec{x}}{\vec{y}})
$$
From equations (\ref{eq:1}) and (\ref{eq:2}) , we can derive the following:
$$
- \|D_{\Sigma}(x)\|^2 - \|D_{\Sigma}(y)\|^2 > -\|\vec{x}\|^2 -\|\vec{y}\|^2 +  \varepsilon\|\vec{x}\|^2 + \varepsilon\|\vec{y}\|^2 
$$
Hence, the event:
$$
 \dotprod{D_{\Sigma}(x)}{D_{\Sigma}(y)} >\dotprod{\vec{x}}{\vec{y}} + \varepsilon (\|\vec{x}\|^2 + \|\vec{y}\|^2 +  \dotprod{\vec{x}}{\vec{y}})
$$
holds with a probability $3\theta$.
In the same way, it is possible to derive that:
$$
 \dotprod{D_{\Sigma}(x)}{D_{\Sigma}(y)} >\dotprod{\vec{x}}{\vec{y}} - \varepsilon (\|\vec{x}\|^2 + \|\vec{y}\|^2 +  \dotprod{\vec{x}}{\vec{y}})
$$
Finally, since the two equations hold with a probability $3\theta$, the following probability holds:
$$
\P(K(x,y) - \varepsilon\J(x,y)<\dotprod{D_{\Sigma}(x)}{D_{\Sigma}(y)}<K(x,y) + \varepsilon \J(x,y) )> 1 - \theta
$$
where $K(x,y) = \dotprod{\vec{x}}{\vec{y}}$ and 
$\J(x,y) = \|\vec{x}\|^2 + \|\vec{y}\|^2 +  K(x,y)$.
%As 
%$$
%\|\df(\Omega_x+\Omega_y)\|^2 = \|\df\Omega_x\|^2 + \|\df\Omega_y\|^2 + 2 \dotprod{\df\Omega_x}{\df\Omega_y}
%$$
%and
%$$
%\|\Omega_x+\Omega_y\|^2  = \|\Omega_x\|^2 + \|\Omega_y\|^2 + 2 \dotprod{\Omega_x}{\Omega_y}
%$$
%we rewrite:
%$$
%\|\df(\Omega_x+\Omega_y)\|^2 > (1 + \varepsilon) \|(\Omega_x+\Omega_y)\|^2
%$$
%as:
%$$
%\|\df\Omega_x\|^2 + \|\df\Omega_y\|^2 + 2 \dotprod{\df\Omega_x}{\df\Omega_y)} > (1 + \varepsilon) (\|\Omega_x\|^2 + \|\Omega_y\|^2 + 2 \dotprod{\Omega_x}{\Omega_y})
%$$
%From equations (\ref{eq:1}) and (\ref{eq:2}) , we can derive the following:
%$$
%- \|\df\Omega_x\|^2 - \|\df\Omega_y\|^2 > -\|\Omega_x\|^2 -\|\Omega_x\|^2 +  \varepsilon\|\Omega_x\|^2 + \varepsilon\|\Omega_y\|^2 
%$$
%Hence, the event:
%$$
%\dotprod{\df\Omega_x}{\df\Omega_y} > \dotprod{\Omega_x}{\Omega_y} + \varepsilon (\|\Omega_x\|^2 + \|\Omega_y\|^2 +  \dotprod{\Omega_x}{\Omega_y})
%$$
%holds with a probability $3\theta$.
%In the same way, it is possible to derive that:
%$$
%\dotprod{\df\Omega_x}{\df\Omega_y} < \dotprod{\Omega_x}{\Omega_y)} - \varepsilon (\|\Omega_x\|^2 + \|\Omega_y\|^2 +  \dotprod{\Omega_x}{\Omega_y)})
%$$
%Finally, since the two equations hold with a probability $\theta^3$, the following probability holds:
%$$
%\P(K(x,y) - \varepsilon\J(x,y)<D_{\Sigma}(x,y)<K(x,y) + \varepsilon \J(x,y) > 1 - 3\theta
%$$
%where $K(x,y) = \dotprod{\Omega_x}{\Omega_y}$, 
%$D_{\Sigma}(x,y) = \dotprod{\df\Omega_x}{\df\Omega_y}$ and 
%$\J(x,y) = \|\Omega_x\|^2 + \|\Omega_y\|^2 +  K(x,y)$.
\end{proof}


%\meta{NON SI PUO' FARE IL MAPPING DIRETTO SU QUEI VETTORI}

\LD{Unfortunately, this first solution is impractical as it is not trivial to directly build bijective mapping functions $\df_{\Sigma}$ and, thus, the function $D_{\Sigma}$.} 
%Hilbert spaces $\mathcal H$ of substructures are potentially infinite dimensional and, thus, we would need large sets $NOV(\varepsilon,\theta)$  of vectors. 
In fact, structures $x$ have exponentially large sets of substructures $S(x)$. Thus, even if we solve the problem of directly defining function $\df$, the direct computation of distributed structures $D_{\Sigma}(x)$ is implausible. 

The next section describes our model on how to efficiently build both a bijective function $\df$ and a function $D$ with a basic vector composition operation $\mo$ and two recursive definition: one for the distributed substructures and one for the distributed substructures that will be named distributed convolution structures. 

%This section explorers possible sets $\mathcal S$ whereas the function $\df$ is discussed in Section \ref{sec:compositional_definition} as a simple direct mapping from a set $S$ to a set $\mathcal S$ is unfeasible. Techniques like singular value decomposition or random indexing (see \cite{sahlgren05}) are impractical in this case as $S$ is huge, although possible. 


%\comment{R3, C3}{you use versors, can you provide a definition?}
%\mysecondinsert{Let $G_S$ be the ground set relevant for substructures in $S$. Substructures in $S$ are again structured objects and, thus, follow the definition of structured objects. As elements in $S$ are substructures of structured object in $X$, we can assume that $G_S=G_X$.} \mysecondremove{Let $G_X$ be the ground set relevant for objects in $X$.} We \mysecondinsert{then} define $\smallvectors{G_X} \subset \R^d$ as the direct random index of $G_X$ \myinsert{\cite{sahlgren05}}.  \finalcomment{Section 3.1: The reference to "direct random index" requires a citation.}{R1.4.1}
%%The set $\smallvectors{G_X}$ is a set of nearly orthogonal versors, where a versor is a vector of norm one. 
%Vectors $\svec{g} \in \smallvectors{G_X}$ are built drawing elements $\svec{g}_i$ from a normal distribution $\mathcal{N}(0,1)$ and normalizing the final results so that they are unit vectors, i.e. $||\svec{g}|| = 1$. These conditions are sufficient to guarantee that, at least, \myremove{each vector in}$\smallvectors{G_X}$ \mysecondinsert{is a set of nearly orthonormal vectors $NOV(\epsilon,\theta)$ as the Johnson-Lindenstruass Lemma holds for vectors in $\smallvectors{G_X}$}\mysecondremove{is statistically nearly-orthogonal with respect to the others}\myinsert{(see \cite{JLLsimple_demonstration})}. 




\subsection{From Distributed Structures to Distributed \emph{Convolution} Structures }
\label{sec:fra}

We showed that convolution kernels on discrete sets are approximated with dot products between small vectors called distributed structures. This is a positive result although a direct mapping between structures and these vectors in impossible. 


A theory to efficiently compute \emph{distributed structures} is the major result of the paper. To this aim, we introduced 
%the \emph{distributed convolution substructures} $\df(a)$ and 
the \emph{distributed convolution structures} $D(x)$ that compute
%both \emph{distributed substructures} and 
\emph{distributed structures} by recursively composing vectors $v \sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ that represent basic elements $G_X$ of structured objects. We then showed that each convolution kernel $K(x,y)$ on discrete sets has a related distributed convolution structure function $D$ such that  the dot products between \emph{distributed convolution structures}  approximate convolution kernels. 
  

This section describes how we obtained the above result. We first introduce the \emph{shuffled circular convolution} as basic vector composition function and we show its properties. We then describe how to produce bijective $\df$ using this composition function to produce \emph{distributed convolution substructures}. Finally, we introduce \emph{distributed convolution structures} $D$ associated to convolution kernels and we show that their dot product approximate the kernels. 



\subsubsection{\mysecondinsert{A} Composition Function preserving Vector Near-orthogonality}
%\subsubsection{An ideal vector composition operation and properties of the tree derived vectors}
\label{sec:ideal}

\LD{In this section, we introduce the \emph{shuffled circular convolution} $\mo$, which is a binary function with \emph{two important features}: (1) it guarantees that different expressions with the same vectors $v \sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$  have different results; (2) it preserves multivariate normality and near orthogonality when composing up to $m$ vectors $v \sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ where $m$ depends on $d$.}
{Hence, this is the basic function to build  the recursive definition for the distributed substructures $\df$ and the distributed convolution structures $D$.}
% We first introduce the function $\mo$ and its properties and we then empirically show its ability to preserve nearly orthogonality.}

%We here introduce the \emph{ideal} properties of the \mysecondinsert{binary} \mysecondremove{vector composition} function $\mo$ \mysecondinsert{on vectors}, such that the elements of the set \mysecondinsert{$\smallvectors{S} = \{\df(x)|x \in S\}$}\mysecondremove{${\mathcal X} = \{\df(x)|x \in X\}$} have the two desired properties \mysecondinsert{(Property \ref{near_norm} and Property \ref{near_orth})}. \mysecondremove{We first introduce the properties of the composition function and then we prove the properties of $\df(x)$.}




%\paragraph{Circular convolution}\label{comp:circonv}
%\LD{The \textbf{shuffled circular convolution} $\mo$ is our basic operation  for combining two nearly orthogonal unit vectors.} It  is defined as follows:
\LD{The \emph{shuffled circular convolution} $\mo$ is defined as follows:
$$
\vec u \mo \vec v = \Phi_1 \vec u \cconv \Phi_2 \vec v
$$}
We combined circular convolution $\cconv$ and vector shuffling that uses two permutation matrices $\Phi_1$ and $\Phi_2$. 
Circular convolution is a popular operation in signal processing and has been used for purposes similar to ours -- encoding flat structures -- for distributed representations \cite{Plate1995}. 
%In circular convolution, components of composed vectors $\vec v = \vec a \cconv \vec b$ are:
%$$
%v_i = \displaystyle\sum\limits_{j=1}^{d} a_j b_{(i+1-j \mod d)}
%$$
Unfortunately, circular convolution is commutative and associative. Consequently, different sequences of applications of the circular convolution to the same vectors yield the same result. Order is not encoded. We used vector shuffling to take order into consideration as done in random indexing models to encode word order \cite{Sahlgren:premutations:2008}. This shuffling is realized with two non-circular permutation matrices $\Phi_1$ and $\Phi_2$. 
%Circular permutation matrices are matrices encoding vector shifting. 
%Vector permutations have been used to encode word order . We here used it to encode sequences of substructures. 
%A shuffling function $P$ is a permutation matrix $P\vec v$ permutes components of vectors $\vec v$.
% according to a fix vector of indexes $P = (P_1,\ldots,P_{d})$. Components of shuffled vectors are $s_{\vec{P}}(\vec{v})_i = v_{P_i}$.
%$P$ is not a circulant matrix.


%The definition of the ideal \mysecondinsert{binary} \mysecondremove{composition} function follows:
\LD{The  properties of the shuffled circular convolution $\mo$ are the following:} 
%properties on $\R^d$ and properties on $NOV(\varepsilon,\theta)$.

\finalcomment{definition 3: I don't think this can be regarded as a definition, since it does not define anything rigorously ("with a very high degree k") and we do not know whether such an operation exists at all, since this has not been clarified by the authors themselves clearly.}{R3.7}
\begin{property}
%[Properties in $\R^d$]
\label{ideal_operation}
Given $\svec{a}$, $\svec{b}$, $\svec{c}$, $\svec{d} \in {\R^d}$, $\vec e = (1,0,...,0) \in \R^d$ and a scalar $s \in \R$, the following properties hold:
\begin{enumerate}
\renewcommand{\labelenumi}{\ref{ideal_operation}.\arabic{enumi}}
 \renewcommand{\labelenumii}{\Roman{enumii})}
\item Non-associativity: $\svec{a} \magicoperation (\svec{b} \magicoperation \svec{c})\neq (\svec{a} \magicoperation \svec{b}) \magicoperation \svec{c}$ \label{p_associativity}
%\item Distributivity over the sum: $(\svec{a}+\svec{b})\magicoperation \svec{c}=\svec{a}\magicoperation\svec{c}+\svec{b}\magicoperation\svec{c}$ and $\svec{c}\magicoperation(\svec{a}+\svec{b}) =\svec{c}\magicoperation\svec{a}+\svec{c}\magicoperation\svec{b}$\label{p_dist}
%\item newtext{R3}{Bilinear form: $(s\svec{a}) \mo \svec{b} = \svec{a} \mo (s\svec{b}) = s(\svec{a} \mo \svec{b})$ \label{p_bilin}}
\item \label{p_dist} Bilinear form:
\begin{tabular}{p{10cm}}
%\begin{enumerate}
%\item 
I)~$(\svec{a}+\svec{b})\magicoperation \svec{c}=\svec{a}\magicoperation\svec{c}+\svec{b}\magicoperation\svec{c}$;
%\item 
II)~$\svec{c}\magicoperation(\svec{a}+\svec{b}) =\svec{c}\magicoperation\svec{a}+\svec{c}\magicoperation\svec{b}$;
%\item 
III)~$(s\svec{a}) \mo \svec{b} = \svec{a} \mo (s\svec{b}) = s(\svec{a} \mo \svec{b})$
\end{tabular}
\item Non-commutativity with a degree $k$. The degree of non-commutativity $k$ is the lowest value such that for any $j<k$ and any permutation $[p_1,p_2,\ldots,p_j]$ of $[1,2,\ldots,j]$,  
$\vec c_1 \magicoperation \vec c_2 \ldots \magicoperation \vec c_j  \neq \vec c_{p_1} \magicoperation \vec c_{p_2} \ldots \magicoperation \vec c_{p_j} $ if $c_i \neq c_{p_i}$. \label{p_comm}
\item Identity elements: $\Phi_1^T\vec e$  and $\Phi_2^T\vec e$ are the left and right identity elements, respectively \label{identity}
%\end{enumerate}
%\end{enumerate}
%\begin{enumerate}[start=4]
%Approximation Properties 
%\item 
%$\P(\displaystyle\prod_{i=1}^n||\svec{x_i}||^2 - \epsilon < ||\svec{t}||^2 <\displaystyle\prod_{i=1}^n||\svec{x_i}||^2  + \epsilon)>1-\theta$ \label{p_modules}
%\item 
%\mysecondinsert{$\P(|\dotprod{\svec{a}}{\svec{t}}| < \epsilon)>1-\theta$}
%\label{property_a}
%\item 
%\mysecondremove{$|\svec{a} \mo \svec{b} \cdot \svec{c} \mo \svec{d}|<\epsilon$  if $|\svec{a} \cdot \svec{c}|<\epsilon$ or $|\svec{b} \cdot \svec{d}|<\epsilon$} 
%\mysecondinsert{$P(|\svec{a} \mo \svec{b} \cdot \svec{c} \mo \svec{d}|<\epsilon)>1-\theta$ if $P(|\svec{a} \cdot \svec{c}|<\epsilon)>1-\theta$ or $P(|\svec{b} \cdot \svec{d}|<\epsilon)>1-\theta$ }
%\label{property_b}
\end{enumerate}
\end{property}

\noindent\LD{These properties can be proven.}
Matrices $\Phi_1$ and $\Phi_2$ guarantee Property \ref{ideal_operation}.\ref{p_associativity} since $\vec a \mo (\vec b \mo \vec c) = \Phi_1\vec a \cconv \Phi_2(\Phi_1 \vec b \cconv \Phi_2 \vec c ) \neq \Phi_1(\Phi_1\vec a \cconv \Phi_2 \vec b )\cconv \Phi_2 \vec c ) =  (\vec a \mo \vec b) \mo \vec c$.  As a consequence,  the symbol $\Mo_{i=1}^k \svec{x}_i$ is intended
%instead used to indicate the application of $\mo$ to a sequence of vectors $\svec{x}_i$. It is defined 
as follows $\Mo_{i=1}^k \svec{x}_i= \svec{x}_1 \mo (\Mo_{i=2}^k \svec{x}_i)$.
The bilinear form of $\mo$ (Properties \ref{ideal_operation}.\ref{p_dist}) is inherited from circular convolution $\cconv$. In fact, circular convolution can be expressed as matrix multiplication. Hence, Properties \ref{ideal_operation}.\ref{p_dist} hold as $\mo$ is a matrix multiplication of permutation and circular convolution matrices.
Although we could not formally prove the non-commutativity of $\mo$, extensive experimental validation shows that the Property \ref{ideal_operation}.\ref{p_comm} holds for very high values of $k$. We took $k$ up to 100 with $10^6$ permutations $[p_1,p_2,\ldots,p_j]$ and the property was not falsified provided that $\Phi_1$ and $\Phi_2$ were not circulant matrices. The necessity of this last condition can be formally proven\footnote{If $\Phi_1$ and $\Phi_2$ are circulant matrices, $\Phi_i(\vec a \cconv \vec b) = (\vec a \cconv \Phi_i\vec b) =  (\Phi_i\vec a \cconv \vec b)$. In fact, $\vec a \cconv \vec b = A \vec b$ where $A$ is a circulant matrix with $a$ as first column. Thus, if $\Phi_i$ is a circulant matrix, $\Phi_i A =  A\Phi_i$ from where $\Phi_i(\vec a \cconv \vec b) = (\vec a \cconv \Phi_i\vec b)$. The same applies for $\vec b$ as $\cconv$ is commutative. Hence, $\Phi_i(\vec a \mo \vec b) = (\Phi_1\vec a \cconv \Phi_i\Phi_2\vec b)$ and, with recursive applications of $\mo$, permutations will accumulate to the last vector. Thus, commutative properties hold.}.
%, it is possible to prove a necessary condition: $\Phi_1$ and $\Phi_2$ have to be  non-circulant matrices.
%It is possible to demonstrate that there exist shufflings $s_P(\cdot)$ such that Property \ref{ideal_operation}.\ref{p_comm} holds for $k$ that is proportional to $d!$. 




\LD{Non-associativity (Prop.~\ref{ideal_operation}.\ref{p_associativity}) and non-commutativity (Prop.~\ref{ideal_operation}.\ref{p_comm}) plus an additional lemma hereafter introduced guarantee the first important feature of $\mo$: different $\mo$-expressions $e$ combing vectors $v \sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ with $\mo$ produce different vectors.} 
In fact, a valid $\mo$-expression $e$ is either a symbol that represent a vector drawn from $\mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ or $e=e_1 \mo e_2$ where $e_1$ and $e_2$ are valid expressions. Consequently, given the $\mo$-expressions $e_1$, $e_2$ and $e_3$, $e_1 \mo (e_2 \mo e_3) \neq (e_1 \mo e_2) \mo e_3$ as $\mo$ is non-associative and $e_1 \mo (e_2 \mo e_3) \neq e_3 \mo (e_2 \mo e_1)$ as $\mo$ is non-commutative.  Moreover,  it is possible to show the additional lemma:
\begin{lemma}
\label{survived_property}
Given $\svec{a}$ as the vector derived from an $\mo$-expression with more than one symbol and $\svec{b}$ directly drawn from $\mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$, 
$\svec{a} \neq \svec{b}$
\end{lemma}
by using the non-commutative property and the fact that it is very unlike to obtain identity elements (Prop.~\ref{ideal_operation}.\ref{identity}) by combining vectors directly drawn from $\mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$.
Hence, different expressions $e$ produce different computations and, thus, different vectors. 
We used $\mo$ to define the bijective function $\df$ between structures and vectors as shown in Sec.~\ref{sec:compositional_definition}.

% In fact, the properties guarantee that the order of application of  $\mo$ to a sequence of vectors $\svec{x}_i$ is relevant. Hence, different structures will have different vectors. To stress that order is important, we use brackets whenever needed. The symbol $\Mo_i \svec{x}_i$ is intended
%%instead used to indicate the application of $\mo$ to a sequence of vectors $\svec{x}_i$. It is defined 
%as follows: $\Mo_{i=1}^1 \svec{x}_i= \svec{x}_1$ and $\Mo_{i=1}^k \svec{x}_i= \svec{x}_1 \mo (\Mo_{i=2}^k \svec{x}_i)$.  

%, which are not modified by the shuffling function $s_P(\cdot)$. 
%\comment{R1, C6}{"For shuffling, Property 4.1 holds for k that is proportional to d!" This statement seems too strong. Isn't it that "there exist shufflings such that ..."}
%\finalcomment{Page 21 paragraph on Shuffling, last line: This requires a citation}{R1.13}



%\paragraph{Shuffling}\label{tran:shuffle}
%A slightly more complex transformation is the shuffling of the elements in the vector, that is a  random permutation of the $d$ elements of the vector. The shuffling function is $shuf_{\vec{K}}(\vec{x})_i = x_{K_i}$, where $\vec{K}=(K_1,\ldots,K_{d})$ is a permutation of the vector $(1,\ldots,d)$. We can define two different permutations $\vec{K_a}$ and $\vec{K_b}$ for the two transformations $\shuf{a}{\cdot}$ and $\shuf{b}{\cdot}$. Notice that for $\vec{K_0}=(1,\ldots,d)$ we have $shuf_{\vec{K_0}}(\cdot) = I(\cdot) $.

%Properties on $NOV(\varepsilon,\theta)$ aims to state that 
\LD{With an empirical investigation, we show that $\mo$ has a second important feature: it preserves (1) multivariate normality and (2) near orthogonality when composing up to $m$ vectors $v \sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ where $m$ depends on $d$.}
%\LD{Studying how $\mo$ preserves near-orthonormality is done empirically.} We investigated how the probability in Equation \ref{eq:probs} changes when combining many vectors $v \sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ with $\mo$.
% produces vectors that are nearly othonormal with respect to existing ones. Clearly, there is an upper bound to this compositions as the cardinality of sets of the type $NOV(\varepsilon,\theta)$ have an upper bound depending on $\R^d$ (cf. Section \ref{sec:nov}). We explored these limitations by analyzing how the number $n$ of possible compositions of $\mo$ is related to $\varepsilon$ and $\theta$.

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
legend entries={$\smallvectors{S}_i$, $\smallvectors{G}_i$},
ymin = 0,
ymax = 15,
xlabel={dimension $d$},
ylabel={max \# of compositions $m$}]
\addplot+[error bars/.cd,
	y dir=both,y explicit]
	table[x=d,y=avg,y error=mean_sd] 
	{data/circular_convolution.dat};
\addplot+[error bars/.cd,
	y dir=both,y explicit]
	table[x=d,y=avg_g,y error=mean_sd_g] 
	{data/circular_convolution.dat};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{Average number of $\mo$ compositions preserving multivariate normality  $\mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ for $\smallvectors{G}_i$ or $\smallvectors{S}_i$: $d$ is the dimensionality of the space $\R^d$ of the multivariate normal and $m$ is the maximum number of compositions such that $\smallvectors{G}_i$ or $\smallvectors{S}_i$  are  multivariate normal for $i<m$}
\label{fig:cc_property}
\end{figure}


For the set up of the empirical investigation, we generated sets $\smallvectors{S}_n$ containing vectors obtained combining up to $n$ vectors $v \sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ for different $d$.  
$\smallvectors{S}_n$ is built as follows: we first generated a set $\smallvectors{G}_1=\smallvectors{S}_1$ of vectors $\vec v\sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$.
We then generated sets $\smallvectors{G}_i $ of  vectors $\vec{v}=\vec{v}_{j}\mo \vec{v}_{i-j}$ where $j$ is randomly selected in $\{1,\ldots,i-1\}$ and $\vec{v}_{k}$ are randomly picked in $\smallvectors{G}_k$. Finally, we set $\smallvectors{S}_n = \bigcup_{i=1}^n \smallvectors{G}_i$. 
%We then have a set $\smallvectors{C}$ containing 10,000 vectors  $\vec v\sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ to compare against.

%Our aim is to compare sets $\smallvectors{S}_n$ to $\smallvectors{C}$ with respect to being $NOV(\varepsilon,\theta)$.
%
%\LD{To show that $\mo$ preserves near-orthonormalilty, we compared $\smallvectors{S}_n$ and $\smallvectors{C}$ where $\smallvectors{S}_n$ contains vectors obtained combining up to $n$ vectors of a basic set $\smallvectors{S}_1$  and $\smallvectors{C}$ contains vectors  $\vec v\sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$. }  
%% by combining vectors in ${\mathcal G_X} \subset NOV(\varepsilon,\theta)$
%%For that goal, we first defined the sets $\smallvectors{S}_n$ of vectors obtained by combining $n$ vectors in  ${\mathcal G_X}$.  
%%The sets $\smallvectors{S}_n$ were defined as follows. 
%%Let $\smallvectors{S}$ the set of vectors containing vectors obtained with composition of vectors in ${\mathcal G_X}$.Then, $\vec{a} \in \smallvectors{S}$ if (1) $\vec a \in {\mathcal G_X}$ or (2) $\vec a = \vec b \mo \vec c$ where $\vec b, \vec c \in \smallvectors{S}$. Let $\degree{\vec{a}}$ be the degree of composition defined as the number of times the function $\mo$ is applied to vectors in ${\mathcal G_X}$ to obtain the vector $\vec{a}$. Thus, $\degree{\vec{a}}=0$ if $\vec a \in {\mathcal G_X}$ and   $\degree{\vec{a}}=\degree b + \degree c + 1$  if $\vec a = \vec b \mo \vec c$ where $\vec b, \vec c \in \smallvectors{S}$. Finally, let $\smallvectors{S}_n = \{ \vec a \in  \smallvectors{S} \vert \degree{\vec a} \leq n \}$. 
%%
%%
%%\LD{We empirically proved properties on $NOV(\varepsilon,\theta)$ with different $n$ to understand how many basic vectors we can compose with a reasonable probability that the two properties hold.} 
%In our experiments,  $\smallvectors{C}$ contains $10,000$ vectors  $\vec v\sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$. On the oher hand, 
%$\smallvectors{S}_n$ is built as follows. We first generated a set $\smallvectors{G}_1=\smallvectors{S}_1$ of $1000$ vectors $\vec v\sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$. $\smallvectors{S}_1$ is $NOV(\varepsilon,\theta)$ as discussed in Sec. \ref{sec:nov}. 
%We then generated sets $\smallvectors{G}_i $ of  $1000$ 
%vectors 
%$\vec{v}=\vec{v}_{j}\mo \vec{v}_{i-j}$ 
%where $j$ is randomly selected in $\{1,\ldots,i-1\}$ and $\vec{v}_{k}$ are randomly picked in $\smallvectors{G}_k$. Hence, $\smallvectors{S}_n = \bigcup_{i=1}^n \smallvectors{G}_i$. Our aim is to compare sets $\smallvectors{S}_n$ to $\smallvectors{C}$ with respect to being $NOV(\varepsilon,\theta)$.
%

\begin{figure}[h!]

%prova figura con tikzp

\begin{center}
\ref{legend_probs}\\
\begin{tabular}{cc}
$d=8,000$ & $d=8,000$ \\
\begin{tikzpicture}
\begin{axis}[
legend columns=-1,
legend entries={$\varepsilon=0.1$, $\varepsilon=0.05$, $\varepsilon=0.04$, $\varepsilon=0.03$, $\varepsilon=0.02$, $\varepsilon=0.01$},
legend to name=legend_probs,
ymin = 0,
ymax = 1.01,
xlabel={\# of compositions},
ylabel={$\P(1- \varepsilon \leq \dotprod{\vec{a}}{\vec{a}} \leq 1 + \varepsilon) $}]
\addplot table  [x=nconv,y=eps01] {data/convolutions/norme_convolutions.dat};
\addplot table  [x=nconv,y=eps005] {data/convolutions/norme_convolutions.dat};
\addplot table  [x=nconv,y=eps004] {data/convolutions/norme_convolutions.dat};
\addplot table  [x=nconv,y=eps003] {data/convolutions/norme_convolutions.dat};
\addplot table  [x=nconv,y=eps002] {data/convolutions/norme_convolutions.dat};
\addplot table  [x=nconv,y=eps001] {data/convolutions/norme_convolutions.dat};
\end{axis}
\end{tikzpicture}
&
\begin{tikzpicture}
\begin{axis}[
%legend entries={$0.1$, $0.01$},
ymin = 0,
ymax = 1.01,
xlabel={\# of compositions},
ylabel={$\P(- \varepsilon \leq \dotprod{\vec{a}}{\vec{b}} \leq \varepsilon) $}]
\addplot table  [x=nconv,y=eps01] {data/convolutions/sp_convolutions.dat};
\addplot table  [x=nconv,y=eps005] {data/convolutions/sp_convolutions.dat};
\addplot table  [x=nconv,y=eps004] {data/convolutions/sp_convolutions.dat};
\addplot table  [x=nconv,y=eps003] {data/convolutions/sp_convolutions.dat};
\addplot table  [x=nconv,y=eps002] {data/convolutions/sp_convolutions.dat};
\addplot table  [x=nconv,y=eps001] {data/convolutions/sp_convolutions.dat};
\end{axis}
\end{tikzpicture}
\\
$d=16,000$ & $d=16,000$ \\
\begin{tikzpicture}
\begin{axis}[
legend columns=-1,
legend entries={$\varepsilon=0.1$, $\varepsilon=0.05$, $\varepsilon=0.04$, $\varepsilon=0.03$, $\varepsilon=0.02$, $\varepsilon=0.01$},
legend to name=legend_probs,
ymin = 0,
ymax = 1.01,
xlabel={\# of compositions},
ylabel={$\P(1- \varepsilon \leq \dotprod{\vec{a}}{\vec{a}} \leq 1 + \varepsilon) $}]
\addplot table  [x=nconv,y=eps01] {data/convolutions/norme_convolutions16k.dat};
\addplot table  [x=nconv,y=eps005] {data/convolutions/norme_convolutions16k.dat};
\addplot table  [x=nconv,y=eps004] {data/convolutions/norme_convolutions16k.dat};
\addplot table  [x=nconv,y=eps003] {data/convolutions/norme_convolutions16k.dat};
\addplot table  [x=nconv,y=eps002] {data/convolutions/norme_convolutions16k.dat};
\addplot table  [x=nconv,y=eps001] {data/convolutions/norme_convolutions16k.dat};
\end{axis}
\end{tikzpicture}
&
\begin{tikzpicture}
\begin{axis}[
%legend entries={$0.1$, $0.01$},
ymin = 0,
ymax = 1.01,
xlabel={\# of compositions},
ylabel={$\P(- \varepsilon \leq \dotprod{\vec{a}}{\vec{b}} \leq \varepsilon) $}]
\addplot table  [x=nconv,y=eps01] {data/convolutions/sp_convolutions16k.dat};
\addplot table  [x=nconv,y=eps005] {data/convolutions/sp_convolutions16k.dat};
\addplot table  [x=nconv,y=eps004] {data/convolutions/sp_convolutions16k.dat};
\addplot table  [x=nconv,y=eps003] {data/convolutions/sp_convolutions16k.dat};
\addplot table  [x=nconv,y=eps002] {data/convolutions/sp_convolutions16k.dat};
\addplot table  [x=nconv,y=eps001] {data/convolutions/sp_convolutions16k.dat};
\end{axis}
\end{tikzpicture}
\end{tabular}
\end{center}

\caption{Probability $\P(\delta(\vec{a},\vec{b}) - \varepsilon \leq \dotprod{\vec{a}}{\vec{b}} \leq \delta(\vec{a},\vec{b}) + \varepsilon) \geq 1- \overline{\theta}$ of the sets $\smallvectors{S}_i$ for different values of $\varepsilon$ and two different values $d$ (8,000 and 16,000). Plots are divided for $\delta(\vec{a},\vec{b}) = 0$ and $\delta(\vec{a},\vec{b}) = 1$.}
\label{fig:probability}
\end{figure}

\LD{In a first experiment, we showed that $\mo$ preserves multivariate normality for $m$ compositions where $m$ depends on $d$.} The experiment wants to empirically answer to the question: does $m$ increase if $d$ does? To this extent, we used the Royston's Test \cite{10.2307/2347291} to assess whether vectors in $\smallvectors{S}_i$ are distributed as  $\mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$. 
For each $d$, we generated 
%40 sequences of 
$\smallvectors{S}_i$ for $i=1 \ldots 20$, where the base sets $\smallvectors{G}_i$ contain $100$ vectors. Then, we determined the maximum $m$ such that  $\smallvectors{S}_i$ with $i \leq m$ satisfies the Royston's Test. Finally, we repeated this procedure $40$ times and averaged the results.
%Then, we computed the average and the standard deviation of $m$ over the 40 sequences. 
Due to the computational limits of the Royston's Test, we tested for $d=10$, $20$, $30$, $40$ and $50$. Results confirm that, increasing $d$, $\mo$ preserves the distribution $\mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ for the sets  $\smallvectors{G}_i$  and  $\smallvectors{S}_i$  for increasingly more compositions (see Fig.~\ref{fig:cc_property}). This is an extremely important property of $\mo$.


\LD{In a second experiment, we showed that $\mo$ preserves nearly orthogonality of unit vectors, that is,  $\P(\delta(\vec{a},\vec{b}) - \varepsilon \leq \dotprod{\vec{a}}{\vec{b}} \leq \delta(\vec{a},\vec{b}) + \varepsilon) \geq 1- \overline{\theta}$  (Equation \ref{eq:probs}).} In this experiments,  $\smallvectors{S}_{i}$ are sets with 1,000 vectors and we considered  two values 8,000 and 16,000 as dimension $d$. Results show that $\mo$ easily preserves nearly orthogonality of vectors in $\smallvectors{S}_{i}$ (see Fig.~\ref{fig:probability}). $\P(- \varepsilon \leq \dotprod{\vec{a}}{\vec{b}} \leq \varepsilon) $ is high for a large number of compositions. Moreover, it also preserves the unit norm of vectors although the probability $\P(1 - \varepsilon \leq \dotprod{\vec{a}}{\vec{a}} \leq 1+\varepsilon) $ decreases as $\varepsilon$ decreases and \emph{\# of compositions} increases. Fortunately, increasing $d$ improves these probabilities.
%In fact,  the probability 
%$\mathbb{P}(\|\delta(\vec{a},\vec{b}) - \dotprod{\vec{a}}{\vec{b}}\| \leq \varepsilon)$ of $\smallvectors{C}$ is similar to $\smallvectors{S}_n$  with $n< XXX$ \textbf{to be continued} (see Figure \ref{fig:probability}). Moreover, the distribution of 
%$
%e = \delta(\vec{a},\vec{b}) - \dotprod{\vec{a}}{\vec{b}}
%$
%in  $\smallvectors{C}$ is remarkably similar to $\smallvectors{S}_{XXX}$ (see Figure \ref{fig:probability_distribution}).
%Both distributions are approximately normal. This is to be expected in the case of independently drawn random vectors in  $\smallvectors{C}$  because of the central limit theorem but it is not  necessarily obvious for $\smallvectors{S}_{XXX}$. This implies that the shuffled circular convolution $\mo$ does a good job in generating new vectors that are approximately independent and normally distributed.


%choose a pair of vectors in $G$, compute their convolution, and add the result back into $G$. We repeat this process until the final set is composed of $10000$ vectors, each of varying degree. Then, for each pair of vectors $v, w \in G$ we compute the error term:
%$$
%e = \delta(v, w) - \langle v, w \rangle
%$$
%and plot the distribution of $e$. We also compare this distribution with the distribution of $10000$ independent and normally-distributed vector (with no convolution computed). The two graphs are shown in figure (2), and the probabilities that $\|e\| \leq \varepsilon$ are reported in table (1) for different values of $\varepsilon$.



%\begin{figure}[h!]
%\begin{center}
%\includegraphics[width=0.5\textwidth]{figure_1.png}
%\caption{Distribution of $\delta(v, w) - \langle v, w \rangle$ for $10000$ normally distributed vectors (above) and for $10000$ convolutions (below). The solid line is a normal distribution with mean and variance equal to the mean and variance of the sample.}
%\label{fig:probability_distribution}
%\end{center}
%\end{figure}




\LD{Since the function $\mo$ guarantees bijectivity, preserves multivariate normality and nearly orthonormality up to a given number of compositions, it is a good composing function for defining $\df$ that build recursively distributed substructures.}

%\LD{Two final notes useful in the following: a lemma and the complexity of  $\mo$.}
%Second, the complexity of $\mo$  is $O(d\log{d})$ by using a Fast Fourier Transform algorithm.

\meta{NOT REVISED FROM HERE}

\subsubsection{Distributed Convolution Substructures: Small Vectors for Substructures}
\label{sec:compositional_definition}


%In this section, we define the function $\df$ that maps structured objects in distributed vectors  \mysecondinsert{such that Equation \ref{eq:approx_delta} holds}. The function $\df$ is \mysecondremove{composition}\mysecondinsert{recursive}, \mysecondremove{as} it exploits a \mysecondinsert{basic} binary  \mysecondremove{vector composition} function $\mo$ \mysecondinsert{on vectors} and a set of nearly-orthonormal vectors $\mysecondinsert{\smallvectors{G_X}}$ associated with the terminal objects. 


%\subsubsection{Substructures in Distributed Structures\comment{REVISED}{}}
%\label{sec:df}


  
%In the following, we first review the definition of structured objects for convolution kernels and, then, we introduce a compositional computation of distributed vectors for these objects.




%\finalcomment{$|| g || = 1$: in which norm?}{R3.5}
%\begin{property}\label{near_norm}
%\emph{(Nearly Unit Vectors)}
%A vector $\df(a) = \svec{a}$ representing a substructure $a$ is a nearly unit vector: $ 1 - \epsilon < ||\svec{a}||^2 < 1 + \epsilon$ \mysecondinsert{with a probability greater than $1-\theta$} where $||\svec{a}||$ is the ${\ell}^2$ norm (Euclidean norm)
%\end{property}
%\begin{property}\label{near_orth}\emph{(Nearly Orthogonal Vectors)}
%Given two different substructures $a$ and $b$, vectors $\df(a)=\svec{a}$ and $\df(b)=\svec{b}$ are nearly orthogonal:  if $a \neq b$, then $|\svec{a} \cdot \svec{b}| < \epsilon$ \mysecondinsert{with a probability greater than $1-\theta$}
%\end{property}
%\mysecondinsert{With these two properties, we have that Equation \ref{eq:approx_delta} holds\footnote{\mysecondinsert{To prove this, Property \ref{near_norm}  should be read as $P(1 - \epsilon < \svec{a} \cdot \svec{b} < 1 + \epsilon| a = b) > 1- \theta$ and Property \ref{near_orth} should be read as $P(- \epsilon < \svec{a} \cdot \svec{b} < \epsilon| a \neq b) > 1- \theta$}} with a probability greater than $1-\theta$, that is: 
%\begin{displaymath}
%P(\delta(a,b) - \epsilon <\df(a) \cdot \df(b)<\delta(a,b) + \epsilon)> 1- \theta
%\end{displaymath}
%We refer to sets  where the two properties hold as sets of nearly orthonormal vectors $NOV(\epsilon,\theta)$.}
%
%


%\subsubsection{Recursive Computation of Distributed Vectors}





%\finalcomment{The second paragraph of this Section is rather unclear to the point that I am not sure it it is correct.}{R1.4.2}
%\finalcomment{N(0,1) is $N(0,I_d)$. What is the "direct random index". Is is random or settled once for all in your analysis? Statements such as "These conditions are sufficient to guarantee that, at least, each vector in GX is statistically nearly-orthogonal with respect to the others". Without a proper statistical qualification, this statement is not informative nor rigorous enough. I don't know exactly what the probability that x'y >epsilon. Please do a bibliographic search and qualify this better. You could start with http://mathoverflow.net/questions/133853/inner-product-of-two-gaussian-random-vectors}{R3.6}

\OCAR{Using the properties of the shuffled circular convolution,}{ we showed that it is possible to recursively define a bijective function $\df$ that builds vectors $\df(x)$ using basic vectors for terminal objects.}{ Starting from a bijective function $\basicdf: G_X \rightarrow \smallvectors{G_X}$, we recursively defined $\df$ and, then, we proved that it is a bijective function between sets of substructures and vectors in $NOV(\varepsilon,\theta)$.}{}


\LD{The function $\df(x)$ is defined as follows:}
\begin{definition}
Let \mysecondremove{$x \in X$}\mysecondinsert{$x \in S$} be a structured object, $G_X$ the ground set for $X$, $\smallvectors{G_X} = NOV(\varepsilon,\theta)$, $\vec q \in G_X$ and $\basicdf: G_X \rightarrow \smallvectors{G_X}$ a bijective function. We recursively define $\df(x)$ as:
\begin{itemize}
\item[] $\df(x) = \basicdf(x) = \svec{x}$ if $x$ is a terminal object, i.e. $x \in G_X$
\item[] $\df(\substr{x}) = \df(x_1) \mo \Mo_{i=2}^M\left(\vec q \mo \df(x_i)\right) $ otherwise 
\end{itemize}
\end{definition}
For example, the tree $\tau$  in Fig. \ref{sampletree}  is represented by the vector obtained as:\\
\begin{displaymath}
\svec{\tau} = (\svec{A} \mo (\vec q \mo (\svec{B} \mo (\vec q \mo \svec{W1}))) \mo (\vec q \mo (\vec q \mo (\svec{C} \mo (\vec q \mo (\svec{D} \mo (\vec q \mo \svec{W2})))) \mo (\vec q \mo (\svec{E} \mo (\vec q \mo \svec{W3}))))))
\end{displaymath}
whereas the sequence $s$ = \texttt{W1 W2 W3} is represented by the following vector:
\begin{displaymath}
\svec{s} = (\svec{W1} \mo (\vec q \mo (\svec{W2} \mo (\vec q \mo \svec{W3}))))
\end{displaymath}






%\subsubsection{Representing trees through vectors}


%We will denote the vector representing a node $n$ simply by $\svec{n} \in {\mathcal N}$, keeping in mind that the actual vector depends on the node label, so that $\svec{n}_1 = \svec{n}_2$ if the two nodes share the same label $l = l(n_1) = l(n_2)$.





%Tree structure can be \comment{R1,C4}{univocally represented in a `flat' format using a parenthetical notation, this is unusual terminology} represented in a `flat' format using nested brackets. For example, the tree in Fig. \ref{sampletree} could be represented by the sequence:
%\begin{center}
% \texttt{(A (B W1)(C (D W2)(E W3)))}
%\end{center}
%This notation corresponds to a depth-first visit of the tree, augmented with parentheses so that the tree structure is defined as well.


%\subsubsection{Proving the Basic Properties for \mysecondinsert{Recursively}\mysecondremove{Compositionally}-obtained Vectors\comment{REVISED}{}}

%Having defined the ideal basic \mysecondinsert{binary}\mysecondremove{composition} function $\mo$, we can now focus on the two properties (Property \ref{near_norm} and Property \ref{near_orth}) needed to have vectors in \mysecondinsert{$\smallvectors{S}$}\mysecondremove{$\mathcal X$} as a nearly orthonormal basis of \myinsert{$\mathcal H$}\myremove{$\R^m$} embedded in $\R^d$.



\LD{We then proved that the function $\df$ is bijective with basic vectors in $NOV(\varepsilon,\theta)$.}
\begin{lemma}
\label{simpler_lemma}
Given two structures $x$ and $y$ in $S$ and the function $\df$, the following property holds:
$$
\delta(x,y) = 1 \iff \delta(\df(x),\df(y)) = 1
$$
\end{lemma}


\begin{proof}
This part of the statement is obvious: if $x = y$ then $\df(x)=\df(y)$.
We need to formally show that: if $x \neq y$ then $\df(x)\neq\df(y)$.

\textbf{Basic step}
Let $x$ be a terminal object, i.e. $x \in G_X$. Two cases are possible:\\
\textbf{\emph{Case 1}} $y$ is a terminal object too. As $x \neq y$, $\basicdf(x) \neq \basicdf(y)$  by construction of $\basicdf(\cdot)$.\\
\textbf{\emph{Case 2}} 
$y$ is not a terminal object. Hence, $\basicdf(x) \neq \df(y)$ for Lemma \ref{survived_property}.

\textbf{Induction step}
Let $\substr x = (x_1,\ldots,x_h)$ and $\substr y = (y_1,\ldots,y_k)$ be two different non-terminal structured objects. We have two cases:\\
\textbf{\emph{Case 1}} It exists a $z\leq k$ and $z\leq h$ such that $x_z \neq y_z$. By inductive hypothesis, $\df(x_z) \neq \df(y_z)$. Thus, $\Mo_{i=z}^k\df(x_i) \neq \Mo_{j=z}^h\df(y_j)$ as the first vectors are different $\df(x_z) \neq \df(y_z)$. Hence, 
as $\mo$ is non-associative, $\Mo_{i=1}^k\df(x_i) \neq \Mo_{j=1}^h\df(y_j)$ by recursively applying $\mo$ as the right vectors are always different.\\
\textbf{\emph{Case 2}} Otherwise, without loss of generality, we assume that $h < k$. Thus, $\df(x_h) \neq   \Mo_{j=h}^k\left(\vec q \mo \df(y_j)\right)$  as $\df(x_h)$ does not start with the vector $\vec q$ \end{proof}



%For property \ref{near_norm} (Nearly Unit Vectors), we need the following lemma:
%\begin{lemma}
%\label{near_norm_lemma} 
%Given a structured object $x \in \mysecondinsert{S}\mysecondremove{X}$, the norm of the vector $\df(x) \in \mysecondinsert{\smallvectors{S}}\mysecondremove{{\mathcal X}}$ is \mysecondinsert{$1 -\epsilon<||\df(x)||^2<1 +\epsilon$ with a probability greater than $1-\theta$}\mysecondremove{one}.
%\end{lemma}
%This lemma can be easily proven by using property \ref{ideal_operation}.\ref{p_modules}, knowing that vectors in ${\mathcal G_X}$ are unit vectors. 
%
%For property \ref{near_orth} (Nearly Orthogonal Vectors), we first need to recall the observation that, due to properties \ref{ideal_operation}.\ref{p_comm} and \ref{ideal_operation}.\ref{p_associativity}, a structured object $x$ generates a unique sequence of applications of function $\mo$ in $\df(x)$ representing its structure. 
%%Thus, a second tree $\tau'$, made up of the same node set of $\tau$, but with a different structure, leads to a different distributed tree $\df(\tau')$.
%We can now address the following lemma:
%
%\begin{lemma}
%\label{near_orth_lemma} 
%Given two different structured objects $x$ and $y$, the corresponding distributed vectors are nearly orthogonal: $|\df(x) \cdot \df(y)| < \epsilon$ \mysecondinsert{with a probability greater than $1-\theta$}.
%\end{lemma}
%\begin{proof}
%The proof is done by \mysecondremove{induction on the structure of $x$ and $y$}\mysecondinsert{structural induction\footnote{Structural induction is a standard way to prove properties for structured objects \cite{Burstall:1969:PPP,Aubin1979329}. We use this version of the induction principle to prove many of our Lemmas and Theorems. For a detailed description see \cite{DBLP:books/daglib/0011126}.}}. 
%
%\textbf{Basic step}
%Let $x$ be a terminal object, i.e. $x \in G_X$. Two cases are possible:\\
%\textbf{\emph{Case 1}} $y$ is a terminal object too and  $x \neq y$. Then, by the properties of vectors in ${\mathcal G_X}$, $|\df(x) \cdot \df(y)| = |\basicdf(x) \cdot \basicdf(y)| = |\svec{x} \cdot  \svec{y}| < \epsilon$  \mysecondinsert{with a probability greater than $1-\theta$};\\
%\textbf{\emph{Case 2}} 
%Otherwise, by Property \ref{ideal_operation}.\ref{property_a}, $|\df(x) \cdot \df(y)| = |\svec{x} \cdot  \df(y)| < \epsilon$  \mysecondinsert{with a probability greater than $1-\theta$} as $\df(y)$ is an arbitrary application of $\mo$ to vectors in ${\mathcal G_X}$.
%
%\textbf{Induction step}
%Let $x$ and $y$ be two different non-terminal structured objects. The expected property becomes:
%\begin{displaymath}
%|\df(\substr{x}) \cdot \df(\substr{y})|=|\Mo_{i=1}^k\df(x_i) \cdot \Mo_{j=1}^h\df(y_j)| < \epsilon.
%\end{displaymath}
%Without loss of generality, we assume that $k \leq h$. As $x$ and $y$ are two different non-terminal structured objects, there exists a subpart where these two objects are different. 
%We have two cases:\\
%\textbf{\emph{Case 1}} There exists $z < k-1$ such that $x_z \neq y_z$. By inductive hypothesis, $\mysecondinsert{P(}|\df(x_z) \cdot \df(y_z)|< \epsilon\mysecondinsert{)>1-\theta}$. Thus, $\mysecondinsert{P(}|\Mo_{i=z}^k\df(x_i) \cdot \Mo_{j=z}^h\df(y_j)| < \epsilon\mysecondinsert{)>1-\theta}$  by Property \ref{ideal_operation}.\ref{property_b} and, then, $\mysecondinsert{P(}|\Mo_{i=1}^k\df(x_i) \cdot \Mo_{j=1}^h\df(y_j)| < \epsilon\mysecondinsert{)>1-\theta}$ by recursively applying Property \ref{ideal_operation}.\ref{property_b} again. \\
%\textbf{\emph{Case 2}} If the above $z < k-1$ does not exist,  $x_k$ is different from $\substr{r} = (y_k, \ldots y_h)$. By inductive hypothesis, we have that $\mysecondinsert{P(}|\df(x_i) \cdot \df(\substr{r})| < \epsilon\mysecondinsert{)>1-\theta}$ and, then, $\mysecondinsert{P(}|\Mo_{i=1}^k\df(x_i) \cdot \Mo_{j=1}^h\df(y_j)| < \epsilon\mysecondinsert{)>1-\theta}$ by recursively applying Property \ref{ideal_operation}.\ref{property_b}.
%
%%
%%
%%We have two cases:
%%If $a \neq b$, $|\svec{a} \cdot \svec{b}| <\epsilon$. Then, $|\df(\tau_a) \cdot \df(\tau_b)|<\epsilon$ by Property \ref{ideal_operation}.\ref{property_b}. 
%%Else if $a=b$, then $\tau_{a_1} \ldots \tau_{a_k} \neq \tau_{b_1} \ldots \tau_{b_h}$ as $\tau_a \neq \tau_b$. Then, as $|\df(\tau_{a_1} \ldots \tau_{a_k}) \cdot \df(\tau_{b_1} \ldots \tau_{b_h})|<\epsilon$ is true by inductive hypothesis, $|\df(\tau_a) \cdot \df(\tau_b)|<\epsilon$ by Property \ref{ideal_operation}.\ref{property_b}. 
%
%
%
%%\textbf{\emph{Case 1}} 
%%Let $\tau_a$ be a tree with root production $a \rightarrow a_1 \ldots a_k$ and  $\tau_b$ be a tree with root production $b \rightarrow b_1 \ldots b_h$. The expected property becomes
%%$|\df(\tau_a) \cdot \df(\tau_b)|=|(\svec{a} \mo \df(\tau_{a_1} \ldots \tau_{a_k})) \cdot (\svec{b} \mo \df(\tau_{b_1} \ldots \tau_{b_h}))| < \epsilon$.
%%We have two cases:
%%If $a \neq b$, $|\svec{a} \cdot \svec{b}| <\epsilon$. Then, $|\df(\tau_a) \cdot \df(\tau_b)|<\epsilon$ by Property \ref{ideal_operation}.\ref{property_b}. 
%%Else if $a=b$, then $\tau_{a_1} \ldots \tau_{a_k} \neq \tau_{b_1} \ldots \tau_{b_h}$ as $\tau_a \neq \tau_b$. Then, as $|\df(\tau_{a_1} \ldots \tau_{a_k}) \cdot \df(\tau_{b_1} \ldots \tau_{b_h})|<\epsilon$ is true by inductive hypothesis, $|\df(\tau_a) \cdot \df(\tau_b)|<\epsilon$ by Property \ref{ideal_operation}.\ref{property_b}. 
%%
%%
%%\textbf{\emph{Case 2}}
%%Let $\tau_a$ be a tree with root production $a \rightarrow a_1 \ldots a_k$ and  $\tau_b = \tau_{b_1} \ldots \tau_{b_h}$ be a sequence of trees. The expected property becomes
%%$|\df(\tau_a) \cdot \df(\tau_b)|=|(\svec{a} \mo \df(\tau_{a_1} \ldots \tau_{a_k})) \cdot (\df(\tau_{b_1}) \mo \df(\tau_{b_2} \ldots \tau_{b_h}))|<\epsilon$. Since $|\svec{a} \cdot \df(\tau_{b_1})|<\epsilon$ is true by inductive hypothesis, $|\df(\tau_a) \cdot \df(\tau_b)|<\epsilon$ by Property \ref{ideal_operation}.\ref{property_b}. 
%%
%%\textbf{\emph{Case 3}}
%%Let $\tau_a = \tau_{a_1} \ldots \tau_{a_k}$ and  $\tau_b = \tau_{b_1} \ldots \tau_{b_h}$ be two sequences of trees.
%%The expected property becomes 
%%$|\df(\tau_a) \cdot \df(\tau_b)|=|(\df(\tau_{a_1}) \mo \df(\tau_{a_2} \ldots \tau_{a_k})) \cdot (\df(\tau_{b_1}) \mo \df(\tau_{b_2} \ldots \tau_{b_h}))|<\epsilon$.
%%We have two cases: 
%%If $\tau_{a_1} \neq \tau_{b_1}$, $|\df(\tau_a) \cdot \df(\tau_b)| <\epsilon$ by inductive hypothesis. Then, $|\df(\tau_a) \cdot \df(\tau_b)|<\epsilon$ by Property \ref{ideal_operation}.\ref{property_b}. 
%%Else, if $\tau_{a_1} = \tau_{b_1}$, then $\tau_{a_2} \ldots \tau_{a_k} \neq \tau_{b_2} \ldots \tau_{b_h}$ as $\tau_a \neq \tau_b$. Then, as $|\df(\tau_{a_2} \ldots \tau_{a_k}) \cdot \df(\tau_{b_2} \ldots \tau_{b_h})|<\epsilon$ is true by inductive hypothesis, $|\df(\tau_a) \cdot \df(\tau_b)|<\epsilon$ by Property \ref{ideal_operation}.\ref{property_b}. 
%\end{proof}

\LD{Hence,  distributed substructures $\df(x)$ are nearly orthogonal unit vectors in $NOV(\varepsilon,\theta)$ whenever their degree $\degree{\df(x)}$ is lower than $n$ as $\df$ is a bijective function mapping substructures to vectors in $NOV(\varepsilon,\theta)$.} 


\subsubsection{Distributed Convolution Structures for Distributed Convolution Kernels\comment{NEW}{}}
\label{sec:the_core_section}



We have now nearly all the elements in place to realize the %plan described in Section~\ref{sec:preliminaries}. 
goal of efficiently mapping structures in small vectors and, thus, having the possibility of using linear learning machines.
The last element is the definition of \emph{distributed convolution structures} $D(x)$ and to show that each convolution kernel on discrete sets has a related $D$ function. We then showed that $D(x)$ computes sums of \emph{distributed substructures} related to a particular convolution kernel $K(x,y)$. Hence, by reformulating Lemma \ref{first_formulation}, convolution kernels are approximated with dot products of \emph{distributed convolution structures}.


\emph{Distributed convolution structure} functions are defined as follows.
\begin{definition}\textbf{Distributed Convolution Structure}
\label{d_definition}
A function $D: X \rightarrow \R^d$ is a Distributed Convolution Structure function of a convolution kernel $K(x,y)$ if
\begin{displaymath}
D(x) = \begin{cases} 
		\omega_x\basicdf(x) & \mbox{if } x\in G_X\\ 
		\displaystyle\sum_{\substr{x} \in R(x)} \displaystyle\Mo_{i=1}^M D_i(x_i) & \mbox{otherwise} 
		\end{cases}
\end{displaymath}
where $D_i(\cdot)$ are distributed convolution structure functions.
\end{definition}

%\subsection{The full demonstration of the hypothesis}

%\comment{Questo forse si potrebbe inserire anche prima}
%The convolution kernel has the form:
%\begin{displaymath}
%K(x,y) = \displaystyle\sum_{\substr{x} \in R(x), \substr{y} \in R(y)} \hspace{.3em} \displaystyle\prod_{i=1}^M K_i(x_i,y_i)
%\end{displaymath}


%\subsection{To be done:}
%
%
%\begin{theorem}
%\begin{displaymath}
%K(x,y) \approx \displaystyle\sum_{x_i \in S(x)} \omega_{i} \svec{x_i} \cdot \displaystyle\sum_{y_j \in S(y)} \omega_{j}\svec{y_j} 
%\end{displaymath}
%and that:
%\begin{displaymath}
%\omega \svec{a} = \displaystyle\Mo_i\svec{a}_i \prod_i\omega_{i} 
%\end{displaymath}
%\end{theorem}



\finalsecondcomment{R3.10}{Proof of lemma 9: "The theorem is proved by structural induction". What is structural induction?}

Hence, we can prove the following lemma.

\begin{lemma}
\label{d_theorem}
If the ground set $G_X$  is endowed with a basic function $\basicdf(x) = \svec{x}$, that maps terminal objects to nearly orthonormal vectors in a set ${\mathcal G_X}$, that is, $\basicdf: G_X \rightarrow {\mathcal G_X}$, a convolution distributed representation:
\begin{displaymath}
D(x) = \displaystyle\sum_{\substr{x} \in R(x)} \displaystyle\Mo_{i=1}^M D_i(x_i)
\end{displaymath}
computes:
\begin{displaymath}
D(x) = \displaystyle\sum_{\substr{a} \in S(x)}\omega_{\substr{a}}\df(\substr{a}) 
\end{displaymath}
%where $S(x) = \bigcup_{\substr{x} \in R(x)}{\mathbb S}(\substr{x})$.
\end{lemma}

\begin{proof}
The theorem is proved by structural induction.

\textbf{Basic step} Let $x$ be a terminal object, we have that $D(x)=\omega_x \basicdf(x) = \omega_x \svec{x}$.

\textbf{Induction step}  Applying the inductive hypothesis, we have\\
\begin{displaymath}
D_i(x_i) = \displaystyle\sum_{a_i \in S(x_i)}\omega_{a_i}\df(a_i) 
\end{displaymath}
Hence:
\begin{displaymath}
D(x) = \displaystyle\sum_{\substr{x} \in R(x)} \displaystyle\Mo_{i=1}^M  \displaystyle\sum_{a_i \in S_i(x_i)}\omega_{a_i}\df(a_i) =
\end{displaymath}
(for the bilinear properties of $\mo$)
\begin{displaymath}
= \displaystyle\sum_{\substr{x} \in R(x)} \displaystyle\sum_{\substr{a} \in {\mathbb S}(\substr{x})}   \displaystyle\prod_{i=1}^M\omega_{a_i} \displaystyle\Mo_{j=1}^M \df(a_j) =
\end{displaymath}
%where $ {\mathbb S}(\substr{x}) = S_1(x_1) \times  \ldots \times S_M(x_m)$
(according to the definitions of $S(x)$ and ${\mathbb S}(\substr{x})$ in Definition \ref{substructure_function}))
\begin{displaymath}
= \displaystyle\sum_{\substr{a} \in S(x)}  \displaystyle\prod_{i=1}^{M}\omega_{a_i}  \df(\substr a) 
\end{displaymath}
%as $S(x) = \bigcup_{\substr{x} \in R(x)}{\mathbb S}(\substr{x})$.
Posing $\omega_{\substr a} = \displaystyle\prod_{i=1}^{M}\omega_{a_i}$, the lemma is shown. 
\end{proof}



\comment{[R2,C2]}{Section 3.2: the material in this section seems to offer an explanation of the fact that the performance 
of the DSTK and DRTK performance drops more than the DTK when lambda increases, and it is 
incomprehensible that the authors do not build the link (actually, this section would be better at the end 
of the paper). Why not add the omega weights in equation (5). Then instead of |S(T)| one would use the 
sum of the omegas, which drops much faster when lambda is small. Is there something I am missing 
here?}

Finally, the main theorem of the paper is the following: 
\finalcomment{Theorem 10: as for Lemma 8 above}{R1.7}
\finalsecondcomment{R3.11}{Theorem 10: are there some delta's missing in the last equation?}
\begin{theorem}
\label{The_theorem_of_the_paper}
Convolution kernels over discrete sets:
\begin{displaymath}
K(x,y) = \displaystyle\sum_{\substr{x} \in R(x), \substr{y} \in R(y)} \displaystyle\prod_{k=1}^D K_k(x_k,y_k)
\end{displaymath}
have associated distributed convolution structures:
\begin{displaymath}
D(z) = \displaystyle\sum_{\substr{z} \in R(z)} \displaystyle\Mo_{k=1}^M D_k(z_k)
\end{displaymath}
where $D_k(z_k)$ are distributed convolution structures for $K_k(x_k,y_k)$ and the following property holds:\\
%\begin{displaymath}
%K(x,y) \approx D(x) \cdot D(y)   
%\end{displaymath}
%that is:\\
\begin{displaymath}
\P(K(x,y)  - \varepsilon \J(x,y)<\dotprod{D(x)}{D(y)} < K(x,y) + \varepsilon \J(x,y)) > 1-|S(x)||S(y)|\theta
\end{displaymath}
with $\J(x,y)=\displaystyle\sum_{a \in S(x), b \in S(y)} \omega_a\omega_b$
\end{theorem}
We omit the proof that at this point of the paper is trivial. It can be obtained by using Lemmas \ref{k_in_deltas}, \ref{first_formulation} and \ref{d_theorem} and by observing that $R(\cdot)$ and $G_Z$ are the same sets both for $D$ and $K$.




%\begin{proof}
%%\textbf{Basic step} Let $x$ and $y$ be terminal objects. We have that $K(x,y) = \omega_{x}\omega_{y} \delta(x,y)$.  \mysecondinsert{Using Lemma \ref{d_theorem},}
%%we can build $D(x)=\omega_{x} \basicdf(x) = \omega_{x} \svec{x} $ and $D(y)=\omega_{y} \basicdf(y) = \omega_{y} \svec{y}$. Thus:\\
%%\begin{displaymath}
%%\omega_{x} \omega_{y} (\delta(x,y)- \epsilon)<  \omega_{x} \omega_{y}  \svec{x} \cdot \svec{y}  <  \omega_{x} \omega_{y} (\delta(x,y) + \epsilon)
%%\end{displaymath}
%%as $\delta(x,y)-\epsilon<\svec{x} \cdot \svec{y} <\delta(x,y)+\epsilon$.   
%%Hence \mysecondinsert{using Lemma \ref{k_in_deltas}}:\\
%%\begin{displaymath}
%%K(x,y)   - \omega_{x} \omega_{y}\epsilon< D(x) \cdot D(y) <  K(x,y) +  \omega_{x} \omega_{y} \epsilon
%%\end{displaymath}
%%\mysecondinsert{with a probability greater than $1-\theta$.}
%%
%%
%%\textbf{Induction step}  
%$K(x,y)$ is a convolution kernel on discrete sets. Thus, from Lemma~\ref{k_in_deltas}:
%\begin{displaymath}
%K(x,y) = \displaystyle\sum_{a \in S(x), b \in S(y)} \omega_{a}\omega_{b} \delta(a,b) 
%\end{displaymath}
%%where as $S(x) = \{ a | \substr{x} \in R(x), \substr{a} \in {\mathbb S}(\substr{x}) \}$ and as $S(x) = \{ b | \substr{y} \in R(y), \substr{b} \in {\mathbb S}(\substr{y}) \}$ where ${\mathbb S}(\substr{x})$ and $S(\substr{y})$   depends on the sequence of the convolution kernels $K_k(\cdot,\cdot)$.  
%$D(\cdot)$ is the distributed convolution structure function related to $K(x,y)$. Thus, from Lemma~\ref{d_theorem}, we have  $D(x)=\displaystyle\sum_{a \in S(x)}\omega_{a}\df(a)$ and $D(y)=\displaystyle\sum_{b \in S(y)}\omega_{b}\df(b)$. 
%%with the associated  ${\mathbb S}(\substr{x})$ and ${\mathbb S}(\substr{y})$.
%% such that:\\
%%\begin{displaymath}
%%D_k(x) \cdot D_k(y_k) - \epsilon \displaystyle\sum_{a \in S(x_k), b \in S(y_k)} \omega_a\omega_b <K_k(x_k,y_k) < D(x_k) \cdot D(y_k) + \epsilon \displaystyle\sum_{a \in S(x_k), b \in S(y_k)} \omega_a\omega_b
%%\end{displaymath}
%%\mysecondremove{We can build}\mysecondinsert{Thus}:
%%\begin{displaymath}
%%D(z) = \displaystyle\sum_{\substr{z} \in R(z)} \displaystyle\Mo_{k=1}^M D_k(z_k) =\displaystyle\sum_{c \in S(z)}\omega_{c}\df(c) 
%%\end{displaymath}
%%where $S(z) = \{ c | \substr{z} \in R(z), \substr{c} \in {\mathbb S}(\substr{z})\}$. 
%%\mysecondinsert{for the bilinear properities of $\mo$}. 
%Then:\\
%\begin{displaymath}
%\dotprod{D(x)}{D(y)}= \displaystyle\sum_{a \in S(x), b \in S(y)} \omega_a\omega_b \dotprod{\df(a)}{\df(b)}
%\end{displaymath}
% Hence, having $\P(\delta(a,b) - \varepsilon< \dotprod{\df(a)}{\df(b)} <\delta(a,b) + \varepsilon)>1-\theta$ from the considerations in Section \ref{sec:compositional_definition}:\\
%\begin{displaymath}
%\displaystyle\sum_{a \in S(x), b \in S(y)} \omega_a\omega_b (\delta(a,b) -\varepsilon) <\displaystyle\sum_{a \in S(x), b \in S(y)} \omega_a\omega_b \dotprod{\df(a)}{\df(b)} <\displaystyle\sum_{a \in S(x), b \in S(y)} \omega_a\omega_b (\delta(a,b) +\varepsilon)
%\end{displaymath}
%with a probability greater than $(1-|S(x)||S(y)|\theta)$  derived using the union bound.\\
%Thus:
%\begin{displaymath}
%K(x,y)  - \varepsilon \displaystyle\sum_{a \in S(x), b \in S(y)} \omega_a\omega_b < \dotprod{D(x)}{D(y)} < K(x,y) + \varepsilon \displaystyle\sum_{a \in S(x), b \in S(y)} \omega_a\omega_b
%\end{displaymath}
%with a probability greater than $(1-|S(x)||S(y)|\theta)$.
%\end{proof}

%This is the final theorem that proves the intuition introduced in Section \ref{sec:preliminaries}. The approximating factor $\J(x,y)$ of Equation \ref{the_core_property} is:\\ 
%\begin{displaymath}
%\end{displaymath} 



%The quality of the approximation \mysecondremove{strictly} depends on the weights of the single substructures and on the number of terms in the cross product \mysecondinsert{with an high probability when $\theta$ is small}.
\finalsecondcomment{R3.12}{"The quality of the approximation strictly depends" what does "strictly" mean in this context?}

\finalcomment{Last line of section 4: It should be noted that the quality of the approximation also depends on the number of terms in the cross product.}{R1.8}


We then obtained the promised result: \emph{distributed convolution structures} represent structures in small vectors and, thus, linear learning machines can be used when learning with structured data. Therefore, \emph{distributed convolution structures} are the avenue to crack the dogma that low computational complexity and fully awareness of structural information are incompatible goals.





%%%%FINEEEEE

\section{Reformulating Existing Convolution Kernels as Distributed Convolution Structures\comment{REVISED}{}}
\label{sec:attk}

\comment{[R3,C10][IssueC][IssueE]}{Section 4 is the most important in the paper, since it introduces the actual approach taken by the authors, which is not, as was suggested in Section 3 or 2, to embed subtree structures in $R^d$, but is instead to define a mapping directly from trees to $R^d$ using a rule defined in Definition 3. Although I believe some of these ideas are useful, they read at this point like a series of hacks, most of which were described in Plate ('94) [is there a more definitive reference for this approach than a Ph.D thesis?]. What is probably the most frustrating from a Machine Learning perspective is that none of these approximations learn from data: they are all defined ad-hoc using elementary intuitions and checking that these intuitions have numerical consequences (Figure 3,4,5). While a large challenge of kernel methods in recent years has been to learn compact representations using data (Nystrom, incomplete cholesky etc...) none of this is ever mentioned here.}


\comment{[R3,C14][IssueF]}{Section 4 does not clearly improve on Section 5.1 in the ICML paper.}



%As for the convolution kernels \cite{Haussler99convolutionkernels}, the 

\emph{Distributed convolution kernels} are a general framework where specific kernels can be defined. We then apply the idea to three existing kernels: the tree kernels \cite{Collins2002}, 
%the subpath tree kernels \cite{Kimura:2011:SKR:2017863.2017871}, the route tree kernels \cite{Aiolli2009}, 
the string or sequence kernels \cite{Lodhi:2002:TCU:944790.944799}, and, finally, the partial tree kernels \cite{Kashima:2002:KSD:645531.656021,Moschitti2006b}.  For the distributed version of each kernel, we introduce the corresponding \emph{distributed convolution structure}. Each section contains: the description of the feature space of the substructures along with the definition of related distributed substructures and, finally, the definition of the distributed convolution structures that computes the distributed substructures in $\R^d$ without enumerating all the substructures (as it happens for convolution kernels). 


\subsection{Distributed Tree Kernels\comment{REVISED}{}}
\label{sec:T}


In \cite{Zanzotto2012193}, we already approached tree kernels using the theory generalized in this paper. Thus, the distributed tree kernels are a good starting point to describe the application of the distributed convolution kernels.


%This section describes the distributed parse tree kernels that emulate the Parse Tree Kernel \cite{Collins2002}. The section is organized as above described.


%\subsubsection{Distributed Tree Fragments for Parse tree kernels}

The feature space of the tree fragments used in \cite{Collins2002} can be easily described in this way. Given a context-free grammar $G=(N,\Sigma,P,S)$ where $N$ is the set of non-terminal symbols, $\Sigma$ is the set of terminal symbols, $P$ is the set of production rules, and $S$ is the start symbol, the valid tree fragments for the feature space are any tree obtained by any derivation starting from any non-terminal symbol in $N$. 


\begin{figure}[h]
\begin{center}
\begin{eqnarray*}
S_{TK}(\begin{array}{c}\parsetree{(.A. (.B. .W1.) (.C. (.D. .W2.)(.E. .W3.)))}\end{array})&=&\{
\begin{array}{c}\parsetree{(.A. . B . . C  .)}\end{array},
\begin{array}{c}\parsetree{(. B . .W1.)}\end{array},
\begin{array}{c}\parsetree{(.A. (. B . .W1.) . C .)}\end{array},
\begin{array}{c}\parsetree{(.A. .B. (.C. . D . . E .))}\end{array},
\begin{array}{c}\parsetree{(.A. .B. (.C. (. D . .W2.) . E . ))}\end{array},
\begin{array}{c}\parsetree{(.A. .B. (.C. . D . (. E . .W3.)))}\end{array},
\\&&
\begin{array}{c}\parsetree{(.A. .B. (.C. (.D. .W2.)(.E. .W3.)))}\end{array},
\begin{array}{c}\parsetree{(.A. (.B. .W1.) (.C. (.D. .W2.)(.E. .W3.)))}\end{array},
\begin{array}{c}\parsetree{(.C. (. D . .W2.) . E . )}\end{array},
\begin{array}{c}\parsetree{(.C. . D . (. E . .W3.))}\end{array},
\begin{array}{c}\parsetree{(.C. (.D. .W2.)(.E. .W3.))}\end{array},
\begin{array}{c}\parsetree{(. E . .W3.)}\end{array},
\begin{array}{c}\parsetree{(. D . .W2.)}\end{array}
\}
\end{eqnarray*}
\end{center}
\caption{Tree Fragments for the tree kernel}
\label{tk:feature_space}
\end{figure}


The above definition is impractical as it describes a possibly infinite set of features. Thus, a description of a function $S(t)$ that extracts the active subtrees from a given tree $t$ is generally preferred. Given a tree $t$, $S_{TK}(t)$ contains any subtree of $t$ which includes more than one node, with the restriction that entire (not partial) rule productions must be included. In other words, if node $n$ in the original tree has children $c_1,\ldots,c_m$, every subtree containing $n$ must include either the whole set of children $c_1,\ldots,c_m$, or none of them (i.e. having $n$ as terminal node). Figure \ref{tk:feature_space} gives an example.


For the formulation of the tree kernels described in \cite{Zanzotto2012193}, the weight $\omega_{\tau}$ for the feature representing the tree fragment $\tau \in S_{TK}(t)$ for a given tree $t$ is:
\begin{displaymath}
\omega_{\tau}= \num{\tau}{t}\sqrt{\lambda^{\nnodes{\tau}-1}}
\end{displaymath}
where $\nnodes{\tau}$ is the number of nodes of $\tau$. In the original formulation \cite{DBLP:conf/nips/CollinsD01}, the contribution 
of the tree fragment $\tau$ to the TK is $\lambda^{\nnodes{\tau}}$ giving a $\omega = \sqrt{\lambda^{\nnodes{\tau}}}$ (see also \cite{Pighin:Moschitti:conll:2010}). This difference is not relevant in the overall theory of the distributed tree kernels.



The distributed tree fragments for the reduced version of this space are the depth-first visits of the above subtrees. For example, given the tree fragments in Figure \ref{tk:feature_space}, the corresponding distributed tree fragments for the first, the second, and the third tree are: $(\svec{A}\mo(\svec{B}\mo\svec{C}))$, $(\svec{B}\mo\svec{W1})$ and $(\svec{A}\mo((\svec{B}\mo\svec{W1})\mo\svec{C}))$.


%,\\ $(\svec{A}\mo(\svec{B}\mo(\svec{C}\mo(\svec{D}\mo\svec{E}))))$, $(\svec{A}\mo(\svec{B}\mo(\svec{C}\mo(\svec{D}\mo\svec{W2})\mo\svec{E})))$, $(\svec{A}\mo\svec{B}\mo(\svec{C}\mo\svec{D}\mo(\svec{E}\mo\svec{W3})))$,\\ $(\svec{A}\mo\svec{B}\mo(\svec{C}\mo(\svec{D}\mo\svec{W2})(\svec{E}\mo\svec{W3})))$, $(\svec{A}\mo(\svec{B}\mo\svec{W1})(\svec{C}\mo(\svec{D}\mo\svec{W2})(\svec{E}\mo\svec{W3})))$, $(\svec{C}\mo(\svec{D}\mo\svec{W2})\svec{E})$, $(\svec{C}\mo\svec{D}\mo(\svec{E}\mo\svec{W3}))$, $(\svec{C}\mo(\svec{D}\mo\svec{W2})(\svec{E}\mo\svec{W3}))$, $(\svec{E}\mo\svec{W3})$, and $(\svec{D}\mo\svec{W2})$.








%=========================

%\subsubsection{Recursively computing Distributed Trees for Parse tree kernels}
%\label{sec:T}

%This section discusses how to efficiently compute DTs. We focus on the space of tree fragments implicitly defined in \cite{DBLP:conf/nips/CollinsD01}. This feature space refers to subtrees as any subgraph which includes more than one node, with the restriction that entire (not partial) rule productions must be included. We want to show that the related distributed trees can be recursively computed using a dynamic programming algorithm without enumerating the subtrees. 
%The reminder of the section is organized as follows. 
%We first define the recursive function and then we show that it exactly computes DTs. 
%Then, we introduce the $\lambda$ decaying factor in the formulation of the recursive function. 

% Finally, we analyze the worst-case time and space complexity of the algorithm.



%\subsubsection{Recursive function} 

%With the trivial weighting scheme, distributed tree forests $\svec{T}$ (in eq. \ref{}) 
%representing the subtrees of a tree $T$ can be expressed by the following function:
%\begin{equation}
%\svec{T} = \sum_{\tau \in S(T)} \svec{\tau}
%\label{initial_distributed_vectors}
%\end{equation}
%where $S(T)$ is the set of the subtrees of $T$ and $\svec{\tau}$ is the distributed vector for subtree $\tau$.

The distributed convolution structure for the computation of distributed trees $DT(t)=\svec{t}$ is the following:
\begin{equation}
\input{kernels/dt_1}
\label{distributed_vectors}
\end{equation}
where $s(n)$ represents the sum of distributed vectors for the subtrees of $t$ rooted in node $n$.
Function $s(n)$ is recursively defined as follows:
\begin{displaymath}
\input{kernels/dtk_2}
\end{displaymath}

%\item $s(n) = \svec{n} \mo (\svec{c_1} + s(c_1)) \mo \dots \mo (\svec{c_n} + s(c_n))$ if $n$ is a node with children $c_1 \dots c_n$.
The distributed convolution structures $DT$ follows Lemma \ref{d_theorem} with $S(t)=S_{TK}(t)$ and with the weight $\omega_{\tau}$ of each distributed tree fragment $\tau\in S_{TK}(t)$  .
%As for the original TK, the decay factor $\lambda$ decreases the weight of large tree fragments in the final kernel value.
With dynamic programming, the time complexity of this function is linear $O(|N(t)|)$ in terms of application of $\mo$ and the space complexity is $d$ (where $d$ is the size of the vectors in ${\mathbb R}^d$).

%\subsubsection{The recursive function computes distributed trees}
%\label{sec:sprops}

%We now demonstrate the properties of the recursive computation of DTF vectors. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%NON SERVE PERCHE' DIMOSTRATO PER I DIST CONV KERN

%The overall theorem we need is the following.
%%Focusing on the basic weighting scheme corresponding to $\lambda = 1$  ($\omega_i = 1$ if $\tau_i$ is a subtree of the original tree and $\omega_i = 0$  otherwise),  the following theorem holds:
%
%\begin{theorem}\label{overall_theorem}
%Given the ideal vector composition function $\mo$, the equivalence between 
%equation (\ref{initial_distributed_vectors}) and equation (\ref{distributed_vectors}) holds, i.e.:
%\begin{displaymath}
%\svec{T} = \sum_{n \in N(T)} s(n) = \sum_{\tau_i\in S(T)} \omega_i\df(\tau_i)
%%\label{the_equivalence}
%\end{displaymath}
%\end{theorem}
%
%%The rest of the section is devoted to demonstrate Theorem \ref{overall_theorem}. Yet, before going in-depth in the demonstration, we want to show the importance of this theorem with respect to the final distributed tree kernel function in Equation \ref{DTK_f}. Given  the theorem and the properties of the ideal function $\mo$, it is trivial to demonstrate properties \ref{near_norm} and \ref{near_orth} as: 
%%\begin{itemize}
%%\item $\df(\tau)$ is a normalized vector due to property \ref{ideal_operation}.\ref{p_modules} and the fact that it is a composition of normalized vectors;
%%\item given two different trees $\tau_1$ and $\tau_2$, $\df(\tau_1) \cdot \df(\tau_2) \approx 0$ as $\df(\tau_1)$ and $\df(\tau_2)$ are different 
%%combinations of node vectors due to properties \ref{ideal_operation}.\ref{p_comm} and \ref{ideal_operation}.\ref{p_associativity}, and their dot product is nearly 0 due to properties \ref{ideal_operation}.\ref{property_a} and \ref{ideal_operation}.\ref{property_b}.
%%\end{itemize}
%We demonstrate Theorem \ref{overall_theorem} by showing that $s(n)$ computes the weighted sum of vectors for the subtrees rooted in $n$ (see Theorem \ref{small_theorem}).
%
%\begin{definition}
%Let $n$ be a node of tree $T$. We define $R(n) = \{\tau | \tau \mbox{ is a subtree of } T \mbox{ rooted in } n\}$
%\end{definition}
%
%We need to introduce a simple lemma, whose proof is trivial.
%
%\begin{lemma}\label{lemma2}
%Let $\tau$ be a tree with root node $n$. Let $c_1,\ldots,c_m$ be the children of $n$. Then $R(n)$ is the set of all trees $\tau' = (n, \tau_1, ... ,\tau_m)$ such that $\tau_i \in R(c_i) \cup \{c_i\}$.
%%	\item $n$ is the root of $\tau$,
%%	\item $n$ has children $c_1,\ldots,c_m$,
%%	\item for each child $c_i$, the subtree $\tau_i$ rooted in $c_i$ is in the set $R(c_i) \cup \{c_i\}$.
%\end{lemma}
%
%Now we can show that function $s(n)$ computes exactly the weighted sum of the distributed tree fragments for all the subtrees rooted in $n$.
%\begin{theorem}
%\label{small_theorem}
%Let $n$ be a node of tree $T$. Then $s(n)=\displaystyle\sum\limits_{\tau\in R(n)} \sqrt{\lambda^{|\tau|-1}} \df(\tau)$.
%\end{theorem}
%\begin{proof}
%The theorem is proved by structural induction.
%
%\textbf{Basis.} Let $n$ be a terminal node. Then we have $R\left(n\right)=\emptyset$. Thus, by its definition, $s(n)=\vec{0}=\displaystyle\sum\limits_{\tau\in R(n)} \sqrt{\lambda^{|\tau|-1}} \df(\tau)$. \comment{R1, C9}{It would be useful to refer to definition 3 here to recall the definition of f hat. }
%
%\textbf{Step.} Let $n$ be a node with children $c_1,\ldots,c_m$. The inductive hypothesis is then $s(c_i)=\displaystyle\sum\limits_{\tau\in R(c_i)} \sqrt{\lambda^{|\tau|-1}}  \df(\tau)$. Applying the inductive hypothesis, the definition of $s(n)$ and the property \ref{ideal_operation}.\ref{p_dist}, we have\\
% 
%%\begin{tabular}{l}
%$s(n) 
%= 
%\svec{n} \diamond \left(\svec{c_1} + \sqrt{\lambda} s(c_1)\right) \diamond \ldots \diamond \left(\svec{c_m} + \sqrt{\lambda} s(c_m)\right)$ \\ 
%\begin{small}
%$= \svec{n} \diamond \left(\svec{c_1} + \displaystyle\sum\limits_{\tau_1\in R(c_1)} \sqrt{\lambda^{|\tau_1|}} \df(\tau_1)\right) \diamond \ldots \diamond \left(\svec{c_m} + \displaystyle\sum\limits_{\tau_m\in R(c_m)} \sqrt{\lambda^{|\tau_m|}} \df(\tau_m)\right) $\\
%\end{small}
%$= \svec{n} \diamond \displaystyle\sum\limits_{\tau_1\in \mathcal{T}_1} \sqrt{\lambda^{|\tau_1|}} \df(\tau_1) \diamond \ldots \diamond \displaystyle\sum\limits_{\tau_m\in \mathcal{T}_m} \sqrt{\lambda^{|\tau_m|}} \df(\tau_m) $\\ 
%$= \displaystyle\sum\limits_{(n,\tau_1,\ldots ,\tau_m) \in \{n\}\times \mathcal{T}_1\times \ldots \times\mathcal{T}_m} \sqrt{\lambda^{|\tau_1| + \ldots + |\tau_m|}} \svec{n} \diamond \df(\tau_1) \diamond \ldots \diamond \df(\tau_m)$\\
%%\end{tabular}\\
%
%where $\mathcal{T}_i$ is the set $R(c_i) \cup \{c_i\}$. Thus, by means of Lemma \ref{lemma2} and the definition of $\df$, we can conclude that $s(n) = \displaystyle\sum\limits_{\tau\in R(n)} \sqrt{\lambda^{|\tau|-1}} \df(\tau)$. 
%\end{proof}
%
%%In the previous definitions and theorem, we assumed that the ideal operation $\mo$ has the associative property. However, this is not a necessary condition, since the same conclusions can be drawn about a non-associative operation $\mo$, by imposing a fixed application order (i.e. left to right). In fact, most of the approximate functions we will introduce are non-associative.
%
%%=========================
%%\subsubsection{Recursively computing Distributed Trees\\Recursive function and expected properties} 
%%
%%We want to show that the related Distributed Trees (DT) can be recursevely computed using a dynamic programming algorithm without enumerating the subtrees. 
%%Initially, we focus on the trivial weighting scheme that assigns $\omega_i = 1$ if $\tau_i$ is a subtree of the original tree and $\omega_i = 0$  otherwise.
%%The reminder of the section is organized as follows. First, we define the recursive function and we analyze its desired properties. Second, we prove that these properties are satisfied. Finally, we analyze the worst time and space complexity of the algorithm.
%%
%%Focussing on the trivial weighting scheme, the distributed tree vector $\svec{T}$, representing the subtree forest of a tree $T$, can be computed through the following function:
%%\begin{equation}
%%\svec{T} = \sum_{\tau \in S(T)} \svec{\tau}
%%\label{initial_distributed_vectors}
%%\end{equation}
%%where $S(T)$ is the set of the subtrees of $T$, $\tau$ is a subtree, and $\svec{\tau}$ is its DTF vector.
%%
%%Given the definition of the subtrees in set $S(T)$, it is possible to introduce the following recursive formulation for the computation of the distributed tree $\svec{T}$:
%%\begin{equation}
%%\svec{T} = \sum_{n \in N(T)} s(n)
%%\label{distributed_vectors}
%%\end{equation}
%%where $N(T)$ is the set of nodes of the tree $T$, $n$ is a node, and $s(n)$ is the sum of the DTF vectors for the subtrees of $T$ rooted in the node $n$.
%%The function $s(n)$ is recursively defined as follows:
%%\begin{itemize}
%%\item $s(n) = \vec{0}$ if $n$ is a terminal node.
%%\item $s(n) = \svec{n} \mo (\svec{c_1} + s(c_1)) \mo \dots \mo (\svec{c_m} + s(c_m))$  if $n$ is a node with children $c_1 \dots c_m$.
%%\end{itemize}
%%
%%
%%\subsubsection{Proving properties of the recursive computation of distributed subtree vectors}
%%\label{sec:sprops}
%%
%%We now demonstrate the properties of the recursive computation of DT vectors. 
%%
%%The overall theorem we need is the following:
%%\begin{theorem}\label{overall_theorem}
%%Given the ideal vector composition function $\mo$, the following equivalence holds:
%%\begin{equation}
%%\svec{T} = \sum_{n \in N(T)} s(n) = \sum_{\tau\in S(T)} \svec{\tau}
%%\label{the_equivalence}
%%\end{equation}
%%\end{theorem}
%%
%%The rest of the section is devoted to demonstrate Theorem \ref{overall_theorem}. %Yet, before going in-depth in the demonstration, we want to show the importance of this theorem with respect to the final distributed tree kernel function in Equation \ref{DTK_f}. Given  the theorem and the properties of the ideal function $\mo$, it is trivial to demonstrate the two properties \ref{near_norm} and \ref{near_orth} as: 
%%%\begin{itemize}
%%%\item $dfv(\tau)$ is a normalized vector as it is a composition of normalized vectors and the property \ref{ideal_operation}.\ref{p_modules} holds;
%%%\item given two different trees $\tau_1$ and $\tau_2$, $dfv(\tau_1) \cdot dfv(\tau_2) \approx 0$ as $dfv(\tau_1)$ and $dfv(\tau_2)$ are different 
%%%combinations of node vectors due to property \ref{ideal_operation}.\ref{p_comm} and their dot product is nearly 0 due to property \ref{ideal_operation}.\ref{p_orth}.
%%%\end{itemize}
%%
%%We demonstrate Theorem \ref{overall_theorem} by demonstrating that $s(n)$ computes the sum of the vectors of the subtrees rooted in $n$ (see Theorem \ref{small_theorem}).
%%
%%We need to start from a definition.
%%
%%\begin{definition}
%%Let $n$ be a node of a tree $T$. We define $R(n) = \{\tau | \tau \mbox{ is a subtree of } T \mbox{ rooted in } n\}$
%%\end{definition}
%%
%%We then need to introduce a simple lemma, whose proof is trivial.
%%\begin{lemma}\label{lemma2}
%%Let $\tau$ be a tree with root node $n$. Let $c_1,\ldots,c_m$ be the children of $n$. Then $R(n)$ is the set of all trees $\tau' = (n, \tau_1, ... ,\tau_m)$ such that $\tau_i \in R(c_i) \cup \{c_i\}$.
%%\end{lemma}
%%
%%Now we can show that the function $s(n)$ computes exactly the sum of the DTFs for all the possible subtrees rooted in $n$.
%%\begin{theorem}
%%\label{small_theorem}
%%Let $n$ be a tree node of a tree $T$. Then $s(n)=\displaystyle\sum\limits_{\tau\in R(n)} \df(\tau)$.
%%\end{theorem}
%%\begin{proof}
%%The theorem is proved by structural induction.
%%
%%\textbf{Basis.} Let $n$ be a terminal node. Then we have $R\left(n\right)=\emptyset$. Thus, by its definition, $s(n)=\vec{0}=\displaystyle\sum\limits_{\tau\in R(n)} \df(\tau)$.
%%
%%\textbf{Step.} Let $n$ be a node with children $c_1,\ldots,c_m$. The inductive hypothesis is then $s(c_i)=\displaystyle\sum\limits_{\tau\in R(c_i)} \df(\tau)$. Applying the inductive hypothesis, the definition of $s(n)$ and the property \ref{ideal_operation}.\ref{p_dist}, we have
%% 
%%\begin{eqnarray*}
%%s(n) &=&
%%\svec{n} \diamond \left(\svec{c_1} + s(c_1)\right) \diamond \ldots \diamond \left(\svec{c_m} + s(c_m)\right) \\
%%&=& \svec{n} \diamond \left(\svec{c_1} + \displaystyle\sum\limits_{\tau_1\in R(c_1)}  \df(\tau_1)\right) \diamond \ldots \diamond \left(\svec{c_m} + \displaystyle\sum\limits_{\tau_m\in R(c_m)} \df(\tau_m)\right) \\
%%&=& \svec{n} \diamond \displaystyle\sum\limits_{\tau_1\in \mathcal{T}_1} \df(\tau_1) \diamond \ldots \diamond \displaystyle\sum\limits_{\tau_m\in \mathcal{T}_m} \df(\tau_m)  \\
%%&=& \displaystyle\sum\limits_{(n,\tau_1,\ldots ,\tau_m) \in \{n\}\times \mathcal{T}_1\times \ldots \times\mathcal{T}_m} \svec{n} \diamond \df(\tau_1) \diamond \ldots \diamond \df(\tau_m)
%%\end{eqnarray*}
%%where $\mathcal{T}_i$ is the set $R(c_i) \cup \{c_i\}$. Thus, by means of Lemma \ref{lemma2} and the definition of $\df$, we can conclude that $s(n) = \displaystyle\sum\limits_{\tau\in R(n)} \df(\tau)$. 
%%\end{proof}
%%
%%% NON SERVE PERCHE' DIMOSTRATO IN GENERALE PER I DISTRIBUTED TREES
%%
%%%Moreover, property \ref{ideal_operation}.\ref{p_modules} of the ideal function guarantees that the composition of versors is a versor. This is needed for the kernel $DTK$. Property \ref{ideal_operation}.\ref{p_orth}, instead, ensures that the vectors for two similar trees (i.e., trees sharing large portions) are orthogonal nonetheless.
%%
%%\subsubsection{Simulating parameter $\lambda$}
%%The classic TK can be parametrized to include a decay factor $\lambda$ that decreases the weight of large trees in the final kernel value. This parameter essentially causes subtrees to weigh $\lambda^{k-1}$ instead of $1$ in the final count, where $k$ is the number of productions (i.e. of non-terminal nodes) in the subtree. 
%%
%%Adopting a parameter $\lambda<1$ is a common practice, since in most tasks it produces better performances than the default TK. Thus, the usefulness of the DTK would be greatly limited if it was not capable of including parameter $\lambda$ in some manner. 
%%
%%In the light of the way parameter $\lambda$ affects the results of TK, we can modifiy the definition of function $s(n)$ as follows:
%%\begin{itemize}
%%\item $s_{\lambda}(n) = \vec{0}$ if $n$ is a terminal node.
%%\item $s_{\lambda}(n) = \svec{n} \mo (\svec{c_1} + \sqrt{\lambda}s_{\lambda}(c_1)) \mo \dots \mo (\svec{c_m} + \sqrt{\lambda}s_{\lambda}(c_m))$  if $n$ is a node with children $c_1 \dots c_m$.
%%\end{itemize}
%%
%%Since we assumed property \ref{p_dist}, it is easy to show that the use of function $s_{\lambda}$ produces a new version of equation \ref{the_equivalence}:
%%\begin{equation}
%%\svec{T} = \sum_{n \in N(T)} s_{\lambda}(n) = \sum_{\tau\in S(T)} \sqrt{\lambda}^{k-1}\svec{\tau}
%%\label{the_equivalence_lambda}
%%\end{equation}
%%where $k$ is the number of non-terminal nodes in subtree $\tau$. In fact, each $\svec{\tau}$ is given by the composition of the basic vectors of the nodes of $\tau$, according to a depth first visit of the tree, where each composition of a non-terminal node (apart from the root) is accompanied by a scalar multiplicative factor $\sqrt{\lambda}$.
%%
%%When applying the actual kernel, i.e. the scalar product between $\svec{T_1}$ and $\svec{T_2}$, each common subtree $\tau_i$ takes part in the result with weight $\sqrt{\lambda}^{k-1}\svec{\tau_i} \cdot \sqrt{\lambda}^{k-1}\svec{\tau_i} = \lambda^{k-1}$. 





\subsection{Distributed Sequence Kernels\comment{NEW}{}}

Given two sequences of symbols (strings), String or Sequence Kernels (SK) \cite{Lodhi:2002:TCU:944790.944799} perform a weighted count of the common subsequences of symbols, possibly including gaps. Sequence kernels are convolution kernels on countable sets. Thus, this is the fourth kernel function family to which we can apply the method described in this paper, by defining the Distributed Sequences (DS).

To describe the feature space of the subsequences and the actual distributed sequences, we need to introduce a specific set of symbols. In this context, the set of basic symbols is $C$. A sequence $s$ is a juxtaposition of symbols, that is, $s \in C^*$. By $\mychar{s}$, we denote the characters of the sequence $s$. Each symbol of a sequence $s$ will be denoted by $s_i$, with $1\leq i \leq \nchar{s}$. Given a vector $\vec{i}$ of ordered indexes, $s[\vec{i}]$ denotes the subsequence $s_{i_1}s_{i_2}...s_{i_n}$. By $l(\vec{i})$ we denote the length spanned by $s[\vec{i}]$ in $s$, i.e.: $l(\vec{i})=i_n-i_1+1$. Finally, by $s[n,m]$ we denote the subsequence $s_n s_{n+1}...s_{m-1} s_m$. Most of this notation is used only in this section.



Without loss of generality, we focus here on the $p$-bounded sequence kernel (as also defined in \cite{Lodhi:2002:TCU:944790.944799}). For this kernel, a subsequence $ss$ is an active feature in the set $S^{DS}_p(s)$ for a sequence $s$ if $ss$ is a subsequence of $s$, that is $ss=s[\vec{i}]$ for some $\vec{i}$, and $|ss|\leq p$. For example, given the sequence $s=\texttt{w}_1\texttt{w}_2\texttt{w}_3\texttt{w}_4$ where $\texttt{w}_i \in C$, the set $S^{DS}_2(\texttt{w}_1\texttt{w}_2\texttt{w}_3\texttt{w}_4)$ is:
\begin{displaymath}
S^{DS}_2(\texttt{w}_1\texttt{w}_2\texttt{w}_3\texttt{w}_4) = \{\texttt{w}_1, \texttt{w}_2,  \texttt{w}_3, \texttt{w}_4,  \texttt{w}_1\texttt{w}_2,  \texttt{w}_1\texttt{w}_3,  \texttt{w}_1\texttt{w}_4, \texttt{w}_2\texttt{w}_3, \texttt{w}_2\texttt{w}_4, \texttt{w}_3\texttt{w}_4\}
\end{displaymath}
The weight $\omega_{ss}$ of each subsequence $ss$ is:
\begin{displaymath}
\omega_{ss}=\num{ss}{s}\lambda^{l(\vec{i})}
\end{displaymath}
where $\num{ss}{s}$ counts the occurrences of the subsequence $ss$ in $s$. The weight depends on $l(\vec{i})$ that represents the span of $ss$ in $s$.  


%map sequences of symbols to a feature vector indexed by all k-tuples of symbols. A k-tuple will have a non-zero entry if it occurs as a subsequence anywhere and not necessarily contiguously in the sequence. The weight of the feature will be the sum over the occurrences of the k-tuple of a decaying factor of the number of gaps appearing in the occurrence


%\paragraph{Original formulation}
%\begin{equation}
%\input{kernels/sk_1}
%\end{equation}
%
%\begin{equation}
% K_n(s,t)=\sum_{u\in\Sigma^n}\sum_{\vec{i}:s[\vec{i}]=u}\sum_{\vec{j}:t[\vec{j}]=u}\lambda^{l(\vec{i})+l(\vec{j})}
%\end{equation}
%
%\paragraph{Efficient formulation}
%\begin{equation}
%\input{kernels/sk_3}
%\end{equation}
%
%
%\paragraph{Distributed formulation}

\finalcomment{The notation at the bottom of page 18 needs to be defined.}{R1.10}
The definition of the $p$-bounded distributed sequence $DS_p(s)$ as a distributed convolution structure follows:
\begin{displaymath}
\input{kernels/ds_1}
\end{displaymath}
with:
\begin{displaymath}
\input{kernels/ds_2}
\end{displaymath}


\finalcomment{The end of Section 5.4: This is confusing and needs to be re-worked.}{R1.11}
In this formulation, $d_n(s)$ are all \myinsert{the nearly orthonormal vectors for} the substrings $s[\vec{i}]$ of $s$ of length $n$ and starting at $s_1$, that is, $|s[\vec{i}]| = n$ and $i_1=1$. 

The \myinsert{time} computational complexity of function $DS_p$ is $O(\nchar{s}p)$ in terms of both vector sums and $\mo$ compositions. 


\subsection{Distributed Partial Tree Kernels\comment{NEW}{}}

Partial Tree Kernels (PTK) \cite{Kashima:2002:KSD:645531.656021,Moschitti2006b} is a variant of Tree Kernels \cite{Collins2002}. In this variant, a larger feature space is obtained by relaxing the constraint on the integrity of the production rules appearing in a subtree. Thus, partial production rules may be included in a subtree, i.e. a subtree may contain any subset of the original children for each one of its nodes. Moreover, even single nodes are included as subtrees. Figure \ref{ptk:feature_space} gives an example of the corresponding function $S_{PTK}(t)$.

\begin{figure}[h]
\begin{center}
\begin{eqnarray*}
S_{PTK}(\begin{array}{c}\parsetree{(.A. (.B. .W1.) (.C. (.D. .W2.)(.E. .W3.)))}\end{array})&=&
S_{TK}(\begin{array}{c}\parsetree{(.A. (.B. .W1.) (.C. (.D. .W2.)(.E. .W3.)))}\end{array}) \cup
\{
A, B, C, D, E, W1, W2, W3,
\begin{array}{c}\parsetree{(.A. . B .)}\end{array},
\begin{array}{c}\parsetree{(.A. (. B . .W1.))}\end{array},
\begin{array}{c}\parsetree{(.A. . C .)}\end{array},
\\
&&\begin{array}{c}\parsetree{(.A. (. C . .D.))}\end{array},
\begin{array}{c}\parsetree{(.A. (. C . (.D. .W2.)))}\end{array},
\begin{array}{c}\parsetree{(.A. (. C . .E.))}\end{array},
\begin{array}{c}\parsetree{(.A. (. C . (.E. .W3.)))}\end{array},
\begin{array}{c}\parsetree{(. C . .D.)}\end{array},
\begin{array}{c}\parsetree{(. C . (.D. .W2.))}\end{array},
\begin{array}{c}\parsetree{(. C . .E.)}\end{array},
\begin{array}{c}\parsetree{(. C . (.E. .W3.))}\end{array},
\begin{array}{c}\parsetree{(.A. . B . (. C . .D.))}\end{array},
\begin{array}{c}\parsetree{(.A. . B . (. C . (.D. .W2.)))}\end{array},
\\
&&\begin{array}{c}\parsetree{(.A. . B . (. C . .E.))}\end{array},
\begin{array}{c}\parsetree{(.A. . B . (. C . (.E. .W3.)))}\end{array},
\begin{array}{c}\parsetree{(.A. (.B. .W1.)(. C . .D.))}\end{array},
\begin{array}{c}\parsetree{(.A. (.B. .W1.)(. C . (.D. .W2.)))}\end{array},
\begin{array}{c}\parsetree{(.A. (.B. .W1.)(. C . .E.))}\end{array},
\begin{array}{c}\parsetree{(.A. (.B. .W1.)(. C . (.E. .W3.)))}\end{array}
\}
\end{eqnarray*}
\end{center}
\caption{Tree Fragments for the Partial Tree Kernel}
\label{ptk:feature_space}
\end{figure}

The PTK weights tree fragments according to their size and to the number of gaps in their productions, with respect to the original tree. Thus, the weight $\omega_{\tau}$ for the feature representing the tree fragment $\tau \in S_{PTK}(t)$ for a given tree $t$ is:
\begin{displaymath}
\omega_{\tau}= \mu^{\bar{\tau}}\sqrt{\lambda^{\nnodes{\tau}}}
\end{displaymath}
where $\nnodes{\tau}$ is the number of nodes of $\tau$ and $\bar{\tau}$ is the number of children missing in the productions of $\tau$, with respect to those of $t$.

The definition of the distributed partial trees (DPT) as a distributed convolution structure follows:
\begin{equation}
\input{kernels/dpt_1}
\end{equation}
where:
\begin{equation}
\input{kernels/dpt_2}
\end{equation}


The \myinsert{time} computational complexity of function $DPT$ is $0(\rho\nnodes{t})$ in terms of $\mo$ compositions and $O(\rho^2\nnodes{t})$ in terms of vector sums, where $\rho$ is the maximum branching factor of $t$. 


%\subsection{Real Basic Composition Functions}


%\section{Approximating the Ideal Basic \mysecondinsert{Binary}\mysecondremove{Vector Composition} Function\comment{REVISED}{}}
%\label{sec:approx}
%
%
%
%\comment{R3,C7}{f is heavily overloaded. First in section 2, then in p. 14 when it appears again with a completely different meaning than that used earlier in the paper.}
%
%As discussed in Sec. \ref{sec:ideal}, the basic \mysecondinsert{binary}\mysecondremove{composition} function $\mo$ has some properties, namely, the \emph{approximation properties}, that real functions can only approximate. In this section, we introduce and we discuss real functions approximating the ideal function $\mo$.
%%The computation of DTs and, consequently, the kernel function $DTK$ as defined strongly depends on the availability of a vector composition operation $\mo$ with the required ideal properties. This operation is hard to define and we can only approximate it.
%Our approach is to consider functions $\comp$ of the form:
%\begin{displaymath}
%\vec{x} \comp \vec{y} = \shuf{a}{\vec{x}} \rop \shuf{b}{\vec{y}}
%\end{displaymath}
%where functions $t_a,t_b:\R^d\rightarrow\R^d$ are two analogous but different vector transformations, used to ensure that the final operation $\comp$ is not associative (Property \ref{ideal_operation}.\ref{p_associativity})  and not commutative with a degree $k$ (Property \ref{ideal_operation}.\ref{p_comm}), and $\rop:\R^d\times\R^d\rightarrow\R^d$ is the chosen basic \mysecondinsert{binary}\mysecondremove{composition} function. The \mysecondremove{composition} function $\rop$ will be hereafter represented with different symbols that will be replicated in the middle of the circle of the final operation $\comp$ as the $\rop$ of the generic basic operation is replicated in the middle of $\comp$. \myinsert{For example, $\cconv$ is the circular convolution and the final operation is $\shufcconv$.} 
%We considered several possible combinations of vector transformation and \mysecondinsert{binary}\mysecondremove{composition} functions. 
%\finalcomment{the explanation of the symbols you form is cryptic. please provide a few simple examples, they will be easier to understand.}{R3.11}
%
%
%%In the following, vectors in $\R^d$, have indexes are in the range ${1,\ldots,d}$.
%
%\subsection{Transformation Functions\comment{REVISED}{}}\label{sec:tran_funct}
%
%
%%We will present here the different transformation functions we considered. 
%
%The transformation functions we considered preserve the properties of the original vectors (norm and statistical distribution of the value of the elements), so that \mysecondinsert{basic binary}\mysecondremove{composition} functions and transformation functions can be independently defined.
%% to the vectors. 
%
%%The transformation functions are used to introduce Property \ref{ideal_operation}.\ref{p_associativity} and Property \ref{ideal_operation}.\ref{p_comm} with different values of $k$ in the final function. 
%%Table \ref{tab:transformation_function} reports how the functions we are using behave with respect to this property.
%
%\paragraph{Reversing}\label{tran:reverse}
%The simplest transformation function is the one that reverses the order of the vector elements, i.e. $rev(\vec{x})_i = x_{d+1-i}$. In this case, we define $\shuf{b}{\cdot} = rev(\cdot)$ and $\shuf{a}{\cdot} = I(\cdot)$ (the identity function).
%For \emph{reversing}, Property \ref{ideal_operation}.\ref{p_comm} holds for a non-commutative degree $k=1$.
%
%\paragraph{Shifting}\label{tran:shift}
%Another simple transformation function is the shifting of the vector elements by j positions, i.e. $shift_j(\vec{x})_i = x_{(i-j)\mod d}$. We can use two different values $j_a$ and $j_b$ for the two transformations $\shuf{a}{\cdot}$ and $\shuf{b}{\cdot}$. For $j=0$ we have $shift_0(\cdot) = I(\cdot)$.
%For \emph{shifting} with $j\neq 0$, Property \ref{ideal_operation}.\ref{p_comm} holds for $k=d$ where $d$ is the size of the vectors.
%
%\paragraph{Shuffling}\label{tran:shuffle}
%A slightly more complex transformation is the shuffling of the elements in the vector, that is a  random permutation of the $d$ elements of the vector. The shuffling function is $shuf_{\vec{K}}(\vec{x})_i = x_{K_i}$, where $\vec{K}=(K_1,\ldots,K_{d})$ is a permutation of the vector $(1,\ldots,d)$. We can define two different permutations $\vec{K_a}$ and $\vec{K_b}$ for the two transformations $\shuf{a}{\cdot}$ and $\shuf{b}{\cdot}$. Notice that for $\vec{K_0}=(1,\ldots,d)$ we have $shuf_{\vec{K_0}}(\cdot) = I(\cdot) $. This transformation function has been also used to encode the word order in random indexing models \cite{Sahlgren:premutations:2008}.
%\myinsert{It is possible to demonstrate that} there exist shufflings such that Property \ref{ideal_operation}.\ref{p_comm} holds for $k$ that is proportional to $d!$. \comment{R1, C6}{"For shuffling, Property 4.1 holds for k that is proportional to d!" This statement seems too strong. Isn't it that "there exist shufflings such that ..."}
%\finalcomment{Page 21 paragraph on Shuffling, last line: This requires a citation}{R1.13}
%
%
%%\paragraph{Linear transformation}\label{tran:linear}
%%The most complex transformation we considered is a complete linear transformation of the form $lin_A(\vec{x}) = A \vec{x}$, where $A$ is an $n\times n$ matrix whose rows are randomly generated in the same manner as the base vectors. We can define two different matrices $A$ and $B$ for the two transformations $t_a$ and $t_b$, or take one of them to be the identity matrix. Notice that the linear transformation is much more computationally expensive than the other transformations considered.
%
%\subsection{\mysecondinsert{Basic Binary}\mysecondremove{Composition} Functions\comment{REVISED}{}}\label{sec:comp_funct}
%
%We considered two  possible \mysecondinsert{binary}\mysecondremove{composition} functions that are bilinear (Property \ref{ideal_operation}.\ref{p_dist}). As these functions are also commutative, they need to be used in conjunction with one of the above vector transformation functions.
%
%The functions are used to progressively introduce the \emph{Approximation Properties} of the ideal funtion $\mo$. We here first examine how the proposed functions can behave with respect to Property \ref{ideal_operation}.\ref{p_modules}. Section \ref{sec:approximation_properties}, instead, reports an empirical investigation on how \emph{Approximation Properties} statistically hold.
%
%%\paragraph{Element-wise product}\label{comp:product}
%
%\paragraph{$\gamma$ element-wise product}\label{comp:gammaprod}
%The $\gamma$ element-wise product $\vec{v}= \vec{x} \prodmo \vec{y}$ is based on the well known element-wise product and on a normalization parameter $\gamma$. Elements of the resulting vector $\vec{v}$ are $v_i = \gamma x_i y_i$. We introduce the parameter $\gamma$ is the element-wise product as this latter does not guarantee any relation between the norm of the original vectors and the norm of the resulting vector. Property \ref{ideal_operation}.\ref{p_modules} cannot hold. The normalization parameter $\gamma$, depending on the size of the feature space, is estimated as the reciprocal of the average norm of the element-wise product between two random unit vectors. Thus, the $\gamma$-product approximates Property \ref{ideal_operation}.\ref{p_modules}.  We need to empirically determine what is the degradation introduced by applying it in long chains of vectors.
%
%\paragraph{Circular convolution}\label{comp:circonv}
%Circular convolution has been used for purposes similar to ours by \cite{Plate1995}, in the context of the distributed representations. It is defined as $\vec{v}= \vec{a} \cconv \vec{b}$ with 
%\begin{displaymath}
%v_i = \displaystyle\sum\limits_{j=1}^{d} a_j b_{(i+1-j \mod d)}
%\end{displaymath} 
%With respect to the element-wise product, circular convolution does not require the introduction of a normalization parameter to approximate Property \ref{ideal_operation}.\ref{p_modules}. \comment{R1, C7}{It would be useful to say more here. What can you say about the norm of this convolution?} Circular convolution has a higher computational complexity, in the order of $O(d^2)$ but, this complexity can be reduced to $O(d\log{d})$ by using a Fast Fourier Transform algorithm.
%
%
%
%%\subsection{Properties of the Real Basic Composition Functions}
%
%\subsection{Empirical Analysis of the Approximation Properties\comment{REVISED}{}}
%\label{sec:approximation_properties}
%
%We performed several tests on the \mysecondinsert{binary}\mysecondremove{composition} functions we introduced, to determine if and to what degree they can satisfy the
%\emph{Approximation Properties} of the ideal \mysecondinsert{binary}\mysecondremove{composition} function $\mo$ \mysecondinsert{on vectors}. We repeated the tests using vectors of different size, to verify the impact of the dimension of the vector space on the effectiveness of the different functions.
%We considered the possible combinations of transformation functions and \mysecondinsert{binary}\mysecondremove{composition} functions.% except those considering the non-normalized element-wise product for the composition function. This latter does not even approximate the required property of norm preservation, as previously pointed out.
%
%Two categories of experiments were performed, to test for the following aspects:
%\begin{itemize}
%	\item \emph{norm preservation}, as in property \ref{ideal_operation}.\ref{p_modules};
%	\item \emph{orthogonality of compositions}, as in properties \ref{ideal_operation}.\ref{property_a} and \ref{ideal_operation}.\ref{property_b}.
%\end{itemize}
%
%\paragraph{Norm preservation}
%
%\begin{figure}[ht!]
%% \centering
%% \begin{tabular}{cc}
%% \subfloat[Dimension 1024, circular convolution]{
%% \input{Figures/modulo1024conv} 
%% \label{fig:modulo1024conv}
%% } &
%% \subfloat[Dimension 1024, $\gamma$-product]{
%% \input{Figures/modulo1024prod} 
%% \label{fig:modulo1024prod}
%% } \\
%% \subfloat[Dimension 2048, circular convolution]{
%% \input{Figures/modulo2048conv} 
%% \label{fig:modulo2048conv}
%% } &
%% \subfloat[Dimension 2048, $\gamma$-product]{
%% \input{Figures/modulo2048prod} 
%% \label{fig:modulo2048prod}
%% } \\
%% \end{tabular}
%% \caption[Norm of the vector obtained as combination of different numbers of basic random vectors]{Norm of the vector obtained as combination of different numbers of basic random vectors, for various combination functions. The values are averaged on 100 samples.}
%% \label{fig:modulo}
%% \end{figure}
%% \begin{figure}[ht!]
%% \ContinuedFloat
%\centering
%\begin{tabular}{cc}
%% \subfloat[Dimension 4096, circular convolution]{
%% \input{Figures/modulo4096conv} 
%% \label{fig:modulo4096conv}
%% } &
%% \subfloat[Dimension 4096, $\gamma$-product]{
%% \input{Figures/modulo4096prod} 
%% \label{fig:modulo4096prod}
%% } \\
%\subfloat[Circular convolution, vector size 8192, ranging on size of combination]{
%\input{Figures/modulo8192conv} 
%\label{fig:modulo8192conv}
%} &
%\subfloat[$\gamma$-product, vector size 8192, ranging on size of combination]{
%\input{Figures/modulo8192prod} 
%\label{fig:modulo8192prod}
%} \\
%\subfloat[Circular convolution, combination of 20 vectors, ranging on vector size]{
%\input{Figures/modulo20conv} 
%\label{fig:modulo20conv}
%} &
%\subfloat[$\gamma$-product, combination of 20 vectors, ranging on vector size]{
%\input{Figures/modulo20prod} 
%\label{fig:modulo20prod}
%} \\
%\end{tabular}
%\caption[]{\finalcomment{- p.22 23, figures: please put the x and y-labels. One should be able to understand the figure without having to look for explanations in the text. Overall, please make the extra-effort of writing self-contained figure captions. - done}{R3.10}
%Norm of the vector obtained as a combination of a number of basic random vectors, for various combination functions. The values are averaged on 100 samples.}
%\label{fig:modulo}
%\end{figure}
%
%
%
%
%We tried composing an increasing number of basic vectors (with unit norm) and measuring the norm of the resulting vector.
%Figures \ref{fig:modulo8192conv} and \ref{fig:modulo8192prod} show the norm of the combination of $n$ vectors with, respectively, circular convolution and $\gamma$-product, combined with the three transformation functions. The norm is the y-axis and $n$ is the x-axis. The vector size is 8192 and each point is the average of 100 samples.
%
%
%These results show that, using $\gamma$-product for the \mysecondinsert{binary}\mysecondremove{composition} function, the norm is only preserved up to a limited number of composed vectors, and then decreases rapidly. Using different transformation functions seems not to have a relevant impact.
%
%Circular convolution, instead, guarantees a very good norm preservation, as long as shuffling is used. Shifting and reversing yield worse results, but still behave better than product-based \mysecondinsert{binary}\mysecondremove{composition} functions. It should be noted that the variance measured (not reported) increases with the number of vectors in the composition, both in the case of $\gamma$-product and circular convolution.
%
%Figures \ref{fig:modulo20conv} and \ref{fig:modulo20prod} report the same results ranging on different vector sizes, for a fixed value of $n=20$. These plots show that increasing the vectors dimension generally allows for better results, though this is more evident for the circular convolution.
%
%We also repeated the tests composing sums of two and three vectors, instead of unitary vectors. The results are very similar, allowing us to postulate that the previous observations can be generalized to the composition of vectors of any norm, as in property \ref{ideal_operation}.\ref{p_modules}.
%
%\paragraph{Orthogonality of compositions}
%
%\begin{figure}[ht!]
%% \centering
%% \begin{tabular}{cc}
%% \subfloat[Dimension 1024, circular convolution]{
%% \input{Figures/simvar1024conv} 
%% \label{fig:simvar1024conv}
%% } &
%% \subfloat[Dimension 1024, $\gamma$-product]{
%% \input{Figures/simvar1024prod} 
%% \label{fig:simvar1024prod}
%% } \\
%% \subfloat[Dimension 2048, circular convolution]{
%% \input{Figures/simvar2048conv} 
%% \label{fig:simvar2048conv}
%% } &
%% \subfloat[Dimension 2048, $\gamma$-product]{
%% \input{Figures/simvar2048prod} 
%% \label{fig:simvar2048prod}
%% } \\
%% \end{tabular}
%% \caption[Variance for the values of Fig.~\ref{fig:sim}]{Variance for the values of Fig.~\ref{fig:sim}.}
%% \label{fig:simvar}
%% \end{figure}
%% \begin{figure}[ht!]
%% \ContinuedFloat
%\centering
%\begin{tabular}{cc}
%% \subfloat[Dimension 4096, circular convolution]{
%% \input{Figures/simvar4096conv} 
%% \label{fig:simvar4096conv}
%% } &
%% \subfloat[Dimension 4096, $\gamma$-product]{
%% \input{Figures/simvar4096prod} 
%% \label{fig:simvar4096prod}
%% } \\
%\subfloat[Circular convolution, vector size 8192, ranging on size of combinations]{
%\input{Figures/simvar8192conv} 
%\label{fig:simvar8192conv}
%} &
%\subfloat[$\gamma$-product, vector size 8192, ranging on size of combinations]{
%\input{Figures/simvar8192prod} 
%\label{fig:simvar8192prod}
%} \\
%\subfloat[Circular convolution, combinations of 50 vectors, ranging on vector size]{
%\input{Figures/simvar50conv} 
%\label{fig:simvar50conv}
%} &
%\subfloat[$\gamma$-product, combinations of 50 vectors, ranging on vector size]{
%\input{Figures/simvar50prod} 
%\label{fig:simvar50prod}
%} \\
%\end{tabular}
%\caption[]{Variance on 100 samples of the dot product between two combinations of basic random vectors, identical apart from one vector, for various combination functions.}
%\label{fig:simvar}
%\end{figure}
%
%\comment{R1,C8}{Compositions of only 20 random basis vectors doesn't seem like enough. In practice trees will have many more than 20 nodes.}
%Properties \ref{ideal_operation}.\ref{property_a} and \ref{ideal_operation}.\ref{property_b} can be easily shown for a single application. But the degradation introduced can become huge when the real basic composition function $\comp$ is recursively applied. Then, we want here to investigate how functions behave in their recursive application.
%We measured, by dot product, the similarity of two compositions of up to 200 random basic vectors, where all but one of the vectors in the compositions is the same, that is:
%\begin{displaymath}
%(\svec{x} \comp \svec{a_1} \comp \ldots \comp \svec{a_{n}}) \cdot (\svec{y} \comp \svec{a_1} \comp \ldots \comp \svec{a_{n}})  
%\end{displaymath}
%This is strictly related to the recursive application of properties \ref{ideal_operation}.\ref{property_a} and \ref{ideal_operation}.\ref{property_b}.
%
%
%%Similarly to what done before, plots in Figure \ref{fig:sim} represent the average on 100 samples of these dot products for a given composition function combined with the three transformation functions.
%Results are encouraging, as the absolute mean value of the similarity \myinsert{oscillates} below $0.1$ for almost every type of \mysecondinsert{binary}\mysecondremove{composition} function and number of vectors composed. In particular, shuffled circular convolution yields the best results on longer compositions.
%\finalcomment{At the top of page 24 the authors mention how they plot the variance rather than the similarity. This needs to be justified}{R1.14}
%\myremove{Instead of plotting these values, whose oscillatory outline is not very meaningful,}
%\myinsert{Plots of the absolute mean value of the similarity are not very meaningful. Instead, those of the variance of this value are more informative.}
%Figure \ref{fig:simvar} reports the variance measured on 100 samples for this experiment. Figures \ref{fig:simvar8192conv} and \ref{fig:simvar8192prod} range on the number of vectors composed, while Figures \ref{fig:simvar50conv} and \ref{fig:simvar50prod} range on the vector size for a fixed number of 50 vectors composed. These values highlight more clearly the better behavior of circular convolution with respect to $\gamma$-product, and of shuffled circular convolution with respect to the shifted and reverse variants. 
%
%We should also point out that shifted and reverse circular convolution yield exactly the same results, both in this experiment and in that of Figure \ref{fig:modulo}. This is due to the nature of circular convolution. Notice that, though sharing these properties, the vectors obtained by the two \mysecondinsert{binary}\mysecondremove{composition} operations are not the same.
%
%We also tried composing vectors in different fashions, i.e. comparing just one vector against the composition of several vectors. The results measured are analogous and thus are not reported.
%
%%The use of different transformation functions, instead, affects some special cases. In particular, we noticed that reverse $\gamma$-product produced the same result for $\vec{x} \otimes_{\gamma}^{rev} \vec{y} \otimes_{\gamma}^{rev} \vec{z}$ and $\vec{z} \otimes_{\gamma}^{rev} \vec{y} \otimes_{\gamma}^{rev} \vec{x}$. This happens because $rev(rev(\vec{x})) = \vec{x}$. Using shifting or shuffling for the transformation function is then preferable, since the probability of incurring in such special cases is much lower.
%
%\paragraph{Discussion}
%The analysis of the  properties of the \mysecondinsert{basic binary}\mysecondremove{composition} functions over vectors leads us to draw some conclusions.
%First, circular convolution seems to provide a better approximation of properties \ref{ideal_operation}.\ref{p_modules}, \ref{ideal_operation}.\ref{property_a} and \ref{ideal_operation}.\ref{property_b} with respect to $\gamma$-product. Second, when using circular convolution as the proper \mysecondinsert{binary}\mysecondremove{composition} function, shuffling is the transformation function that yields better approximation of the ideal properties. We performed the following experiments using both the shuffled circular convolution $\shufcconv$ and shuffled $\gamma$-product  $\shufprod$ as basic \mysecondinsert{binary}\mysecondremove{vector composition} functions \mysecondinsert{on vectors}.
%
%As a final note, we point out that, as expected, increasing the vector size always results in a better approximation of the desired properties.
%
%
%


\section{Computational complexity of the Distributed Convolution Kernels in Linear SVMs\comment{NEW}{}}
\label{sec:complexity}


\comment{[R4,C1][IssueG]}{The most serious issue with the paper is that the complexity claims
seem rather dubious. These claims are summarized in table 7, page 28.
The complexity of computing the parse tree kernel between two trees
with n nodes is $O(n^2)$ in the worst case and O(n) in average (for
natural language parse trees). The complexity of the approximated
kernel computation (dot product) in the lower dimension is simply d,
the dimension of that low dimension space. However, in that case, one
need also to take in account the tree preparation which is given as
O(n) time and O(d) space in this table. These complexities are
justified in section 5.1.3.  However, it seems the authors do not take
in account the complexity of the actual basic composition function
which is called several at each node. That complexity is at least O(d)
for shuffled $\gamma$-product and O(d log d) for shuffled circular
convolution. The complexity of computing the embedding of a tree with
n nodes should then at least be O(n d) or O(n d log d) and it is hard
to see how this computation can be conducted using O(d) space which
corresponds to a constant number of d-dimension vectors. The authors
really need to provide a detailed complexity analysis.}
\comment{R2,C9} {Table 7: Distributed parse tree preparation time should  be at least O(nd) rather than O(d). When using 
circular convolution , it should be at least O(n.d.log(d)), and I assume memory usage is O(d.log(d)). }
%\begin{table}[!h]
%\begin{tabular}{lccc}
%& $Training$ &  Training (with structures) & Op\\
%\hline
%SVM  & $O(n n_{SV} + n_{SV}^3)$ & $O(n n_{SV}|N|^2 + n_{SV}^3|N|^2)$  & $\Delta$ ops \\
%SVM \cite{DBLP:journals/jmlr/KeerthiCD06} & $O(n d_{max}^2)$ & $O(n d_{max}^2 |N|^2)$ & $\Delta$ ops \\
%\hline
%Linear SVM \cite{Joachims:2006:TLS:1150402.1150429} & $O(sn)$ & $O(sn +  sn |N|\log s)$ & prod  \\
%\hline
%Linear SVM\footnote{According to \cite{DBLP:journals/mp/Shalev-ShwartzSSC11}} \cite{Joachims:2006:TLS:1150402.1150429} 
%& $O(snR^2/(\lambda\epsilon^2))$  & $O(ns/(\lambda\epsilon^2) + ns|N|\log s)$  & prod\\
%Pegasos \cite{DBLP:journals/mp/Shalev-ShwartzSSC11} & $O(snR^2/(\lambda\epsilon)) $ &  $O(sn/(\lambda\epsilon) + sn|N|\log s)$ & prod \\
%\hline
%\end{tabular}
%\caption{$n$: \# examples, $n_{SV}$: \# support vectors (\cite{NIPS2003_LT01} shows that $n_{SV}$ grows linearly with $n$), $|N|$: \# nodes in the tree, $s$: \# non-zero dimensions, $\lambda$ parameter in SVM, $\epsilon$ approximation parameter in SVM, $R=1$ the max norm of a vector???, from  \cite{Joachims:2006:TLS:1150402.1150429} $\epsilon = 0.001$ and $\lambda \in [10^{-4},10^{-6}]$}
%\end{table}
We have introduced the distributed convolution kernels as a different approach to representing structured data in machine learning. In the previous sections, we have shown their properties and their computational complexities that are strictly connected to the cost of the real used basic \mysecondinsert{binary}\mysecondremove{composition} operations. This section investigates whether this approach is computationally convenient. We perform the worst case analysis of the linear SVMs \cite{Joachims:2006:TLS:1150402.1150429,DBLP:journals/mp/Shalev-ShwartzSSC11} with the DCKs against a kernelized SVM with convolution kernels. 
The kernels are analyzed in class of equivalence of kernels and distributed kernels grouped with respect to the computational complexity: (1) the tree, the subpath, and the route kernels; (2) the string or sequence kernels; (3) and, the partial tree kernels.  

The time complexity of the learning phase of the kernelized verions of SVM is $O(n n_{SV}+ n_{SV}^3)$ in terms of application of the kernel functions where $n$ is the number of training examples and $n_{SV}$ is the number of support vectors. \cite{NIPS2003_LT01} shows that $n_{SV}$ grows linearly with $n$, thus, the complexity of the learning phase is $O(n^3)$. The time complexity of a single classification step is $O(n_{SV})$, that, for the same reasons, become $O(n)$. The learning and classification time compexities when using the different classes of convolution kernels are then the following: (1) $O(n^3\nnodes{t}^2)$ and $O(n\nnodes{t}^2)$ for TK, SPK, and RK as computational complexities of these kernels is $O(\nnodes{t}^2)$ where $\nnodes{t}$ is the number of nodes of a generic tree $t$ (see \cite{Collins2002} for tree kernels); (2) $O(n^3p\nchar{s}^2)$  and $O(np\nchar{s}^2)$ for the SK as the kernel cost is $O(p\nchar{s}^2)$ where $\nchar{s}$ is the number of characters in the generic sequence $s$ 
and $p$ is the selected max length of the substrings \cite{Lodhi:2002:TCU:944790.944799}; (3) $O(n^3\rho^2\nnodes{t}^2)$ and $O(n\rho^2\nnodes{t}^2)$  for the PTK as the kernel cost is $O(\rho^2\nnodes{t}^2)$ where $\rho$ is the max branching factor of the generic tree $t$ \cite{Moschitti2006b}.


\begin{table}[!h]
\begin{center}
\begin{tabular}{ ll|c|c|}
\emph{Kernel Class} & &{SVM+CK}  &LinearSVM+DCK \\
\hline
\multirow{2}{*}{Tree} & Train & $O(n^3\nnodes{t}^2)$ &   $O(n\nnodes{t}d\log d)$ \\
 & Classif. & $O(n\nnodes{t}^2)$  & $O(\nnodes{t}d\log d)$ \\
\hline
\multirow{2}{*}{String or Sequence} & Train  & $O(n^3p\nchar{s}^2)$  &$O(np\nchar{s}d\log d)$\\
                                                            & Classif. & $O(np\nchar{s}^2)$  ì& $O(p\nchar{s}d\log d)$\\
\hline
\multirow{2}{*}{Partial Tree}             & Train         & $O(n^3\rho^2\nnodes{t}^2)$  & $O(n\rho\nnodes{t}d\log d)$\\
                                                            & Classif. & $O(n\rho^2\nnodes{t}^2)$  & $O(\rho\nnodes{t}d\log d)$ \\
\hline
\end{tabular}
\end{center}
\caption{Time computational complexities: Linear SVMs along the different distributed convolution kernels (LinearSVM+DCK) compared with  kernelized SVMs with the convolution kernels (SVM+CK). $n$ is the number of  training examples, $d$ is the dimension of the space $\R^d$ for the DCSs, $t$ is a generic tree, $s$ is a generic string, $\nnodes{t}$ is the number of nodes of $t$, $\nchar{s}$ is the number of characters in the sequence $s$, $p$ is the max length of the considered substring, and $\rho$ is the max branching factor of the trees $t$. The cost unit of SVM+CK is an access to a $\Delta$ function and a  product whereas the cost unit of LinearSVM+DCK is a product.}
\label{tab:complexities}
\end{table}


Linear versions of SVM \cite{Joachims:2006:TLS:1150402.1150429,DBLP:journals/mp/Shalev-ShwartzSSC11}  have a time complexity of $O(gn)$ for the learning phase where $n$ is still the number of training examples and $g$ is the max number of non-zero dimensions in a generic vector. The complexity is expressed in terms of products in the target space. The one-step classification complexity is $O(g)$. As the distributed convolution structures  have dense vectors, $g=d$ for the DCKs where $d$ is the dimension of the space $\R^d$ of the DCSs. The learning and classification costs are then $O(dn)$ and $O(d)$. To exploit DCKs in a linear SVM, we have to sum the previous costs with the cost of converting the structure to a vector in $\R^d$. As this conversion cost is bigger that $d$, the conversion is the dominant term in the sum of complexities. 
The time complexities of the computation of the distributed convolution structures are expressed with respect to the basic \mysecondinsert{binary}\mysecondremove{composition} function. We use here the two operations selected in the previous section: the shuffled $\gamma$-product $\shufprod$ with a complexity of $d$ and the shuffled circular convolution $\shufcconv$ with a complexity of $d \log d$. The complexity of the computation of the different distributed convolution structures is reported at the end of each corresponding sub-section in Section \ref{sec:attk}.
\finalcomment{The last sentence on page 25 repeats the contents of Table 2 and is not necessary}{R1.15}
\myremove{The learning and classification complexities for the three classes of distributed convolution kernels with $\shufprod$ are:  $O(n\nnodes{t}d)$  and  $O(\nnodes{t}d)$ for DTKs, DSPKs, and DRKs; $O(np\nchar{s}d)$ and $O(p\nchar{s}d)$ for DSKs; and, $O(n\rho\nnodes{t}d)$ and $O(\rho\nnodes{t}d)$ for DSTKs. 
In the case of  $\shufcconv$, the complexities are:  $O(n\nnodes{t}d\log d)$  and  $O(\nnodes{t}d\log d)$ for DTKs, DSPKs, and DRKs; $O(np\nchar{s}d\log d)$ and $O(p\nchar{s}d\log d)$ for DSKs; and, $O(n\rho\nnodes{t}d\log d)$ and $O(\rho\nnodes{t}d\log d)$ for DSTKs.}


The \myremove{above} analysis of the time computational complexity is summarized in Table \ref{tab:complexities}. These complexities show that linear SVMs with distributed convolution kernels are computationally much better than kernelized SVMs with convolution kernels. The dependence with respect to the number of training examples becomes the dominant factor as $n$ increases as we can obtain relevant results in the final task with $d$ below 10,000 (see Section \ref{sec:experiments}).

\finalcomment{
A second issue I pointed out in my original review regarded the
complexity claims. In this new version, the authors proposes a new
analysis of the complexity that addresses these issue. However, the
authors focused this discussion on time complexity (whereas space
complexity was also mentioned in the original paper).  I think the
argument that this approach results in a time complexity improvement
is not that convincing. Indeed, explicitly using the (sparse) feature
space corresponding to the considered kernels in conjunction with
linear SVMs, time complexities of O(gn) for training and O(g) for
classification could be achieved in theory, where n is the number of
training example and g the expected number of non-zero features. Since
it is not clear whether d is smaller than g, it is not clear whether
there is a time complexity improvement.  However, when using linear
SVMs, the space complexity required might be prohibitive (in the worst
case O(gn) or O(G) where G is actual dimension of dimension of the
explicit feature space, but a compact representation of the W vector
might still exit). This is where the authors approach could shine: the
space complexity when using linear SVMs is O(d) (since in that case g
= G = d).
}{R4.1}
\myinsert{The space complexity comparison is even more promising. The space complexity of the kernelized SVM with convolution kernels that is $O(n^2)$. Even the space complexity of a linear SVM with an explicit space for the convolution kernel is prohibitive. Using the possibly sparse vectors in learning or classification requires a space complexity of  $O(gn)$ or $O(G)$ where G is actual dimension of the explicit feature space. A linear SVMs with distributed convolution kernels has instead only a space complexity of $O(d)$ (since in that case $g = G = d$) that is extremely smaller than the other two.}

%To use the DTK along with the linear SVM, we need to add to the complexity of translating trees in distributed trees, that is $O(|N|cost(\mo))$ that, in the worst case, is the circurlar convolution computed with the application of FFT with a complexity $s \log s$. The overall complexity is then $O(sn +  n |N| s \log s) = O(n |N| s \log s)$ in term of products. 



%Thus, we can conclude that a linear SVM along with a DTK  has a much better complexity than a kernelized SVM with a TK even when using basic vector composition operations that are computationally expensive as the shuffled circular convolution.








\section{Experiments and Evaluation\comment{REVISED}{}}
\label{sec:experiments}


Distributed convolution kernels are an attractive counterpart of the traditional convolution kernels. But, as these kernels are approximations of the original ones, we want to investigate how well they approximate the original kernels. The rest of the section is organized as follows. Section \ref{sec:treecorp} describes the test-bed and Section \ref{sec:eval} then proposes a direct  and task-based evaluation on how DCKs approximate CKs. 




\subsection{Test-bed and Parameters}
\label{sec:treecorp}

We experimented with 3 linguistic tasks: question classification (QC) \cite{Li:2002:LQC:1072228.1072378}, recognizing textual entailment (RTE) \cite{RTE1} and paraphrasing \cite{}. 

These 3 tasks provided structured data, that is, sequences and trees 





\begin{figure}
% Preamble: \pgfplotsset{width=7cm,compat=1.12}
\pgfplotsset{footnotesize}
\begin{center}% note that \centering uses less vspace...
\ref{prova3}\\
\begin{tabular}{cc}
\begin{tikzpicture}
\begin{semilogxaxis}[
legend columns=-1,
legend entries={\emph{Convolution}($\varepsilon=0.01$),\emph{Direct}($\varepsilon=0.01$),\emph{Convolution}($\varepsilon=0.005$),\emph{Direct}($\varepsilon=0.005$)},
legend to name=prova3,
ymin = 0,
ymax = 1.01,
xlabel={space dimension $d$},
ylabel={probability},
title={$entailment$}]
\addplot table  [x=dim,y=eps01] {data/MainTheoremProb/entailmentDSubsetTree_lambda04_epsilon01.dat};
\addplot table  [x=dim,y=uppereps01] {data/MainTheoremProb/entailmentDSubsetTree_lambda04_epsilon01.dat};
\addplot table  [x=dim,y=eps005] {data/MainTheoremProb/entailmentDSubsetTree_lambda04_epsilon01.dat};
\addplot table  [x=dim,y=uppereps005] {data/MainTheoremProb/entailmentDSubsetTree_lambda04_epsilon01.dat};
\end{semilogxaxis}
\end{tikzpicture}
&
\begin{tikzpicture}
\begin{semilogxaxis}[
xlabel={space dimension $d$},
ymin = 0,
title={$paraphrasing$},
ymax = 1.01]
\addplot table  [x=dim,y=eps01] {data/MainTheoremProb/paraDSubsetTree_lambda04_epsilon01.dat};
\addplot table  [x=dim,y=uppereps01] {data/MainTheoremProb/paraDSubsetTree_lambda04_epsilon01.dat};
\addplot table  [x=dim,y=eps005] {data/MainTheoremProb/paraDSubsetTree_lambda04_epsilon01.dat};
\addplot table  [x=dim,y=uppereps005] {data/MainTheoremProb/paraDSubsetTree_lambda04_epsilon01.dat};
\end{semilogxaxis}
\end{tikzpicture}
\end{tabular}
\end{center}
\caption{Comparison between direct embedding of fragment spaces (\emph{Direct}) and \emph{convolution distributed structures} (\emph{Convolution}): Estimated probabilities for Lemma \ref{first_formulation} ($\varepsilon=0.01$ and $\varepsilon=0.005$) with respect to different sizes of the embedding space $\R^d$. Considered distributed structures: Distributed Trees (\emph{DT}) with $\lambda=0.4$. Datasets: entailment and paraphrasing. Estimation done over 2,000 pairs of trees.}
\label{fig:main_theorem_probability_estimation_comparison}
\end{figure}






\begin{figure}
% Preamble: \pgfplotsset{width=7cm,compat=1.12}
\pgfplotsset{footnotesize}
\begin{center}% note that \centering uses less vspace...
\ref{prova2}\\
\begin{tabular}{ccc}
\begin{tikzpicture}
\begin{semilogxaxis}[
legend columns=-1,
legend entries={\emph{question classification},$entailment$,$paraphrasing$},
legend to name=prova2,
ymin = 0,
ymax = 1.01,
xlabel={space dimension $d$},
ylabel={probability at $\varepsilon=0.01$},
title={$DPT$}]
\addplot table  [x=dim,y=eps01] {data/MainTheoremProb/QCgrctDPartialTree_lambda04_mu04_epsilon01.dat};
\addplot table  [x=dim,y=eps01] {data/MainTheoremProb/entailmentPartialTree_lambda04_mu04_epsilon01.dat};
\addplot table  [x=dim,y=eps01] {data/MainTheoremProb/paraPartialTree_lambda04_mu04_epsilon01.dat};
\end{semilogxaxis}
\end{tikzpicture}
&
\begin{tikzpicture}
\begin{semilogxaxis}[
xlabel={space dimension $d$},
ymin = 0,
title={$DS$},
ymax = 1.01]
\addplot table  [x=dim,y=eps01] {data/MainTheoremProb/QCDString_lambda04_epsilon01.dat};
\addplot table  [x=dim,y=eps01] {data/MainTheoremProb/entailmentDString_lambda04_epsilon01.dat};
\addplot table  [x=dim,y=eps01] {data/MainTheoremProb/paraphrasingfilice_SequenceKernellambda_0_4mu_0_4.dat};
\end{semilogxaxis}
\end{tikzpicture}
&
\begin{tikzpicture}
\begin{semilogxaxis}[
xlabel={space dimension $d$},
ymin = 0,
title={$DT$},
ymax = 1.01]
\addplot table [x=dim,y=eps01]  {data/MainTheoremProb/QCDSubsetTree_lambda04_epsilon01.dat};
\addplot table [x=dim,y=eps01]  {data/MainTheoremProb/entailmentDSubsetTree_lambda04_epsilon01.dat};
\addplot table [x=dim,y=eps01]  {data/MainTheoremProb/paraDSubsetTree_lambda04_epsilon01.dat};
\end{semilogxaxis}
\end{tikzpicture}
\\
\begin{tikzpicture}
\begin{semilogxaxis}[
ymin = 0,
ymax = 1.01,
xlabel={space dimension $d$},
ylabel={probability  at $\varepsilon=0.005$}]
\addplot table  [x=dim,y=eps005] {data/MainTheoremProb/QCgrctDPartialTree_lambda04_mu04_epsilon01.dat};
\addplot table  [x=dim,y=eps005] {data/MainTheoremProb/entailmentPartialTree_lambda04_mu04_epsilon01.dat};
\addplot table  [x=dim,y=eps005] {data/MainTheoremProb/paraPartialTree_lambda04_mu04_epsilon01.dat};
\end{semilogxaxis}
\end{tikzpicture}
&
\begin{tikzpicture}
\begin{semilogxaxis}[
xlabel={space dimension $d$},
ymin = 0,
ymax = 1.01]
\addplot table  [x=dim,y=eps005] {data/MainTheoremProb/QCDString_lambda04_epsilon01.dat};
\addplot table  [x=dim,y=eps005] {data/MainTheoremProb/entailmentDString_lambda04_epsilon01.dat};
\addplot table  [x=dim,y=eps005] {data/MainTheoremProb/paraphrasingfilice_SequenceKernellambda_0_4mu_0_4.dat};
\end{semilogxaxis}
\end{tikzpicture}
&
\begin{tikzpicture}
\begin{semilogxaxis}[
xlabel={space dimension $d$},
ymin = 0,
ymax = 1.01]
\addplot table [x=dim,y=eps005]  {data/MainTheoremProb/QCDSubsetTree_lambda04_epsilon01.dat};
\addplot table [x=dim,y=eps005]  {data/MainTheoremProb/entailmentDSubsetTree_lambda04_epsilon01.dat};
\addplot table [x=dim,y=eps005]  {data/MainTheoremProb/paraDSubsetTree_lambda04_epsilon01.dat};
\end{semilogxaxis}
\end{tikzpicture}
\end{tabular}
\end{center}
\caption{Estimated probabilities for Lemma \ref{first_formulation} ($\varepsilon=0.01$ and $\varepsilon=0.005$) with respect to different sizes of the embedding space $\R^d$. Considered distributed structures: Distributed Trees (\emph{DT}) with $\lambda=0.4$, Distributed Sequences (\emph{DS}) with $\lambda=0.4$ and sequence boud $=4$ and Distributed Partial Trees (\emph{DPT}) with $\lambda=0.4$ and $\mu=0.4$. Datasets: question classification, entailment and paraphrasing. Estimation done over 2,000 pairs of structures.}
\label{fig:main_theorem_probability_estimation}
\end{figure}


\begin{figure}
\pgfplotsset{footnotesize}
\begin{center}
\ref{prova}\\
\begin{tikzpicture}
\begin{semilogxaxis}[
legend columns=-1,
legend entries={$DPT$,$DS$,$DT$},
legend to name=prova,
ymin = 0.85,
ymax = 1.005,
xlabel={space dimension $d$},
ylabel={spearman's correlation},
title={QC}]
\addplot table  [x=dim,y=correlation] {data/correlation/QCgrctDPartialTree_lambda04_mu04_epsilon01.dat};
\addplot table  [x=dim,y=correlation] {data/correlation/QCDString_lambda04_epsilon01.dat};
\addplot table [x=dim,y=correlation]  {data/correlation/QCDSubsetTree_lambda04_epsilon01.dat};
\end{semilogxaxis}
\end{tikzpicture}
\begin{tikzpicture}
\begin{semilogxaxis}[
ymin = 0.85,
ymax = 1.005,
xlabel={space dimension $d$},
title={entailment}]
\addplot table  [x=dim,y=correlation] {data/correlation/entailmentfilice_PartialTreeKernel_lex_true_all_falselambda_0_4mu_0_4.dat};
\addplot table  [x=dim,y=correlation] {data/correlation/entailmentfilice_SequenceKernel_lex_false_all_falselambda_0_4mu_0_4.dat};
\addplot table  [x=dim,y=correlation] {data/correlation/entailmentfilice_SubsetTreeKernel_lex_true_all_falselambda_0_4mu_0_4.dat};
\end{semilogxaxis}
\end{tikzpicture}
\begin{tikzpicture}
\begin{semilogxaxis}[
xlabel={space dimension $d$},
ymin = 0.85,
ymax = 1.005,
title={paraphrasing}]
\addplot table  [x=dim,y=correlation] {data/correlation/paraphrasingfilice_PartialTreeKernellambda_0_4mu_0_4.dat};
\addplot table  [x=dim,y=correlation] {data/correlation/paraphrasingfilice_SequenceKernellambda_0_4mu_0_4.dat};
\addplot table [x=dim,y=correlation]  {data/correlation/paraphrasingfilice_SubsetTreeKernellambda_0_4mu_0_4.dat};
\end{semilogxaxis}
\end{tikzpicture}
\end{center}
\caption{Spearman's correlations between distributed convolution kernels and convolution kernels in the configurations applied for the 3 tasks: question classification, entailment and paraphrasing. Compared kernels: PTK vs. DPTK, SK vs. DSK, TK vs. DTK. }
\label{fig:correlation}
\end{figure}




\begin{table}
\begin{tabular}{l|ccc|ccc|ccc}
 & \multicolumn{3}{|c|}{\emph{entailment}} & \multicolumn{3}{|c|}{\emph{paraphrasing}} & \multicolumn{3}{|c}{\emph{question classification}} \\
 & \emph{DC-ll} & \emph{DC-pa} & \emph{CK-SVM} & \emph{DC-ll} & \emph{DC-pa} & \emph{CK-SVM} & \emph{DC-ll} & \emph{DC-pa} & \emph{CK-SVM}\\
\hline
\input{data/accuracies.tex}
\hline
\end{tabular}
\caption{Accuracies of SVM with convolution kernels and linear machines with distributed convolution structures in $\R^d$ with $d=16384$. Used linear machines: LIBLINEAR ($ll$) and Passive-Aggressive ($pa$). Used distributed convolution structures: $DS$, $DT$ and $DPT$. Used datasets: question classification, entailment and paraphrasing.
Dimension of Distributed Representation is $d=16384$}
\label{tab:accuracies}
\end{table}



%
\begin{figure}
\pgfplotsset{footnotesize}
\begin{center}
\ref{exec_time_legend}\\
\begin{tikzpicture}
\begin{axis}[
    ybar stacked,
%    enlargelimits=0.15,
    legend columns=-1,
	bar width=5pt,
  ymin=0, ymax=10000000*3,
    legend to name=exec_time_legend,
    legend entries={Training, Testing },
     title={entailment},
    ylabel={Execution Time (in $ms$)},
    symbolic x coords={\emph{DS-ll}, \emph{DS-pa}, \emph{SK-SVM}, , \emph{DT-ll}, \emph{DT-pa}, \emph{TK-SVM}, , \emph{DPT-ll}, \emph{DPT-pa}, \emph{PTK-SVM}},
    xtick=data,
    x tick label style={rotate=90,anchor=east},
    ]

\addplot table  [x=set,y=train] {data/exec/exec_entailment.dat};
\addplot table  [x=set,y=test] {data/exec/exec_entailment.dat};
\end{axis}
\end{tikzpicture}
\begin{tikzpicture}
\begin{axis}[
    ybar stacked,
%    enlargelimits=0.15,
	bar width=5pt,
  ymin=0, ymax=10000000*3,
    symbolic x coords={\emph{DS-ll}, \emph{DS-pa}, \emph{SK-SVM}, , \emph{DT-ll}, \emph{DT-pa}, \emph{TK-SVM}, , \emph{DPT-ll}, \emph{DPT-pa}, \emph{PTK-SVM}},
title={paraphrasing},
    xtick=data,
    x tick label style={rotate=90,anchor=east},
    ]
\addplot table  [x=set,y=train] {data/exec/exec_paraphrasing.dat};
\addplot table  [x=set,y=test] {data/exec/exec_paraphrasing.dat};
\end{axis}
\end{tikzpicture}
\begin{tikzpicture}
\begin{axis}[
  ymin=0,
    ybar stacked,
%    enlargelimits=0.15,
title={QC},
	bar width=5pt,
  ymin=0, ymax=10000000*3,
    symbolic x coords={\emph{DS-ll}, \emph{DS-pa}, \emph{SK-SVM}, , \emph{DT-ll}, \emph{DT-pa}, \emph{TK-SVM}, , \emph{DPT-ll}, \emph{DPT-pa}, \emph{PTK-SVM}},
    xtick=data,
    x tick label style={rotate=90,anchor=east},
    ]
\addplot table  [x=set,y=train] {data/exec/exec_QC.dat};
\addplot table  [x=set,y=test] {data/exec/exec_QC.dat};
\end{axis}
\end{tikzpicture}
\caption{Execution times in $ms$ for SVM with convolution kernels and linear machines with distributed convolution structures in $\R^d$ with $d=16384$. Used linear machines: LIBLINEAR ($ll$) and Passive-Aggressive ($pa$). Used distributed convolution structures: $DS$, $DT$ and $DPT$. Used datasets: question classification, entailment and paraphrasing.}
\label{fig:execution_times}
\end{center}
\end{figure}



%\subsubsection{Linguistic Tasks}


For the experiments, we used standard datasets for the two NLP tasks of QC and RTE. \comment{R3, C4}{please provide more info on the datasets, such as labels and tree distribution (length etc)}

For QC, we used a standard question classification training and test set\footnote{The QC set is available at \url{http://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC/}} \cite{Li:2002:LQC:1072228.1072378}, where the test set is the 500 TREC 2001 test questions. To measure the task performance, we used a question multi-classifier by combining $n$ binary SVMs according to the ONE-vs-ALL scheme, where the final output class is the one associated with the most probable prediction. We used the classification task based on the coarse-grain labels. The trees in the set contain 9817 node labels, including 67 non-terminal labels (POS-tags) and 9750 terminal labels (words). The trees have an average number of 30 nodes, with up to 115 nodes for the largest ones. The maximum branching factor in the trees is 18, and the maximum tree depth is 20. Sentences in the set are 10 tokens long on average, with a maximum of 37.

For RTE we considered the corpora ranging from the first challenge to the fifth \cite{RTE1}, except for the fourth, which has no training set. These sets are referred to as RTE1-5. The dev/test distribution for RTE1-3, and RTE5 is respectively 567/800, 800/800, 800/800, and 600/600 T-H pairs. We used these sets for the traditional task of pair-based entailment recognition, where a pair of text-hypothesis $p=(t,h)$ is assigned a positive or negative entailment class. The trees in the set contain 30965 node labels, including 74 non-terminal labels and 31039 terminal labels. The trees are composed of 79 nodes on average, with up to 736 nodes for the largest ones. The maximum branching factor in the trees is 47, and the maximum tree depth is 60. Sentences in the set are 28 tokens long on average, with a maximum of 260.

As final specification of the experiment setting, we give the list of the external resources and tools used. For the syntactic interpretation we used the Charniak's parser \cite{Charniak00}. 









\subsection{Experimental Evaluation}
\label{sec:eval}

This section reports the results. The first set of experiments is the direct evaluation: we compared the similarity produced with the DCKs with those produced with the CKs. The second set of experiments is a task-based evaluation where we investigated the behavior of the DCKs in the two NLP tasks.


\section{Discussion and Conclusions\comment{REVISED}{}}
\label{sec:conclusions}

\comment{[R2,C4][IssueD]}{The bottom line is that, when reaching the (non-existing) conclusion, the reader is confused and do not 
know when and how to use DTK. Are they a good approximation of TK? Even when they are not such a 
good approximation, are there better than other approaches? Shall we only use small lambdas or not? 
It is essential to remove/explain the contradictions, connect the theory and the experiments, and offer a 
"take-home" message to the reader for when to use DTK. I am convinced the paper contains nearly all 
the needed material to achieve that, and this is mostly a question of proper write-up.
}
Distributed convolution kernels on countable sets are an innovative way of representing structured data in machine learning as these kernels well approximate convolution kernels on countable sets. The major computational benefit is obtained when using these kernels in combination with linearized versions of kernel machines such as linear SVM \cite{Joachims:2006:TLS:1150402.1150429,DBLP:journals/mp/Shalev-ShwartzSSC11}. We empirically prove that DCKs well approximate CKs with the $\lambda$ penalizing factor in the range $\lambda \leq 0.4$. These values of $\lambda$ are those generally used when exploiting convolution kernels in tasks.


We presented the distributed convolution kernels only from a point of view, that is a way to approximate the original convolution kernel in order to reduce the computational complexity of learning and classification. As we will see later in this section, this is not the only important characteristic of DCKs but, as such, DCKs are in competition with two other families of approaches working on reducing the computational complexity. A \emph{first family} of approaches is the one that aims to reduce or to control the computational complexity of kernel computation. For example in the case of the tree kernels \cite{Collins2002}, there is a line of research that aims to build more efficient methods by exploiting characteristics of trees in specific applications \cite{Moschitti2006} or by performing a feature selection (in the kernel computation \cite{Rieck:2010:ATK:1756006.1756022} or directly on the trees \cite{Pighin:Moschitti:conll:2010}). But, these approaches do not reduce the $n^3$ factor in the overall 
complexities (where $n$ is the number of training examples). Thus, for all these cases, DCKs with linear SVMs are still better than the others. 
\comment{[R3,C10][IssueC][IssueE]}{Section 4 is the most important in the paper, since it introduces the actual approach taken by the authors, which is not, as was suggested in Section 3 or 2, to embed subtree structures in $R^d$, but is instead to define a mapping directly from trees to $R^d$ using a rule defined in Definition 3. Although I believe some of these ideas are useful, they read at this point like a series of hacks, most of which were described in Plate ('94) [is there a more definitive reference for this approach than a Ph.D thesis?]. What is probably the most frustrating from a Machine Learning perspective is that none of these approximations learn from data: they are all defined ad-hoc using elementary intuitions and checking that these intuitions have numerical consequences (Figure 3,4,5). While a large challenge of kernel methods in recent years has been to learn compact representations using data (Nystrom, incomplete cholesky etc...) none of this is ever mentioned here.}
\comment{[R4,C1][IssueG]}{The most serious issue with the paper is that the complexity claims
seem rather dubious. These claims are summarized in table 7, page 28.
The complexity of computing the parse tree kernel between two trees
with n nodes is $O(n^2)$ in the worst case and O(n) in average (for
natural language parse trees). The complexity of the approximated
kernel computation (dot product) in the lower dimension is simply d,
the dimension of that low dimension space. However, in that case, one
need also to take in account the tree preparation which is given as
O(n) time and O(d) space in this table. These complexities are
justified in section 5.1.3.  However, it seems the authors do not take
in account the complexity of the actual basic composition function
which is called several at each node. That complexity is at least O(d)
for shuffled $\gamma$-product and O(d log d) for shuffled circular
convolution. The complexity of computing the embedding of a tree with
n nodes should then at least be O(n d) or O(n d log d) and it is hard
to see how this computation can be conducted using O(d) space which
corresponds to a constant number of d-dimension vectors. The authors
really need to provide a detailed complexity analysis.}
A \emph{second family} instead works on the approximation of the Gram matrix by using methods like the Nystr\"{o}m method \cite{DBLP:conf/nips/WilliamsS00} or the incomplete Cholesky decomposition \cite{Fine:2002:EST:944790.944812,Bach:2003:KIC:944919.944920} and those proposed in this line of research.  By working on reduced-rank approximations of the Gram matrix, these models bound the learning complexity to $O(nm^2)$ where $m$ is the rank of the approximation. These methods are competitive with DCKs. For example, in the case of the tree kernels, the two complexities of SVM with TKs and the approximation of thee Gram matrix vs. the linear SVM with DTKs are comparable, that is, respectively, $O(n m^2 \nnodes{t}^2)$  vs. $O(n \nnodes{t} d \log d)$. With respect to this second family, DCKs seem to be only another, although different, way of approximating kernel functions for the purpose of reducing the computational complexity.


\comment{[R2,C5][IssueC]}{Introduction: the position of the paper is extremely narrow, and the authors only connect to other work 
on tree kernels. It is thus likely to be ignored by many reader interested in ML for NLP. I believe this 
paper is part of a trend: using recursive structures to handle problems when one needs a proper 
representation for long term dependencies. The most vocal representative of this new trend is Richard 
Socher. It would also have been interesting to compare DTK to Recursive NN on some problems that are 
much more recent and critical that ones used for experiments in this paper, such as sentiment analysis.}
Distributed convolution kernels also have another important characteristics that is not treated in this paper that makes them totally different from approaches approximating the Gram matrix \cite{DBLP:conf/nips/WilliamsS00,Fine:2002:EST:944790.944812,Bach:2003:KIC:944919.944920}. DCKs with their distributed convolution structures give also the possibility of encoding structures in small vectors. This is extremely relevant in an emerging field in natural language processing, namely, the compositional distributional semantics \cite{mitchell-lapata:2008:ACLMain,baroni-zamparelli:2010:EMNLP,fabio2010-2} and the strictly related work on recursive autoencoders \cite{SocherEtAl2011:PoolRAE,SocherEtAl2012:MVRNN}. In this area, distributional (and not distributed) vectors representing the meaning of words are composed to obtain the meaning of word sequences or full sentences. Except for those methods encoding distributional semantics in tensors \cite{ClarkCoeckeSadrzadeh2008,Grefenstette:2011:ESC:2145432.2145580},
 compositional distributional semantics methods use structural information to derive the meaning of word sequences but the structural information is lost in the final vector. Distributed convolution structures represent a way to fill this gap as showing a way to encode the lost structural information. How to encode structures along with distributional meaning is still matter for research. Simple attempts fail (see \cite{zanzotto-dellarciprete:2011:DiSCo}). The real research issue is how to scale from \emph{distributed convolution kernels on countable sets} to \emph{distributed convolution kernels} by removing the restriction \emph{on countable sets}. 


As a final remark, DCKs on countable sets are not bounded to support vector machines and not even to kernel machines. DCKs could be used in other kernel machines such as on-line learning models by helping in using unbounded on-line learning models \cite{Cavallanti:2007:TBH:1296038.1296052} even when dealing with structured data without any need to go for bounded on-line learning models that select and discard vectors for memory constraint or time complexity \cite{Cavallanti:2007:TBH:1296038.1296052,DBLP:conf/nips/DekelSS05,Orabona:2008:PBK:1390156.1390247}.  
DCKs with the associated distributed convolution structures also give the possibility to fully use structured data in non kernel machines, for example, probabilistic classifiers such as naive bayes or maximun entropy classifiers as well as decision tree learners \cite{Quinlan:1993}. \comment{R3, C1}{do you mean "(Quinlan '93) can NOW use tree structured data."?} Results on pilot experiments show that this is a viable possibility.

\newpage


\appendix 
\section{A Map of the Symbols\comment{NEW}{}}
\label{section:symbols}

\comment{[R4,C3][IssueA,IssueB]}{Presentation is a major issue with this paper. The authors introduce a
lot of different notations but they are not consistent in their usage
making their statements less precise and less clear than they should
be. For instance $\mo$ is used to refer for the ideal basic
composition function in section 4. This "ideal" function does not
exists and the authors used instead a "real" or "actual" (no
consistency again) basic composition function denoted by $\Box$ will be
used (sec. 4.4.3). 
But it section 5, the $\mo$ notation is used for
the "real" or "actual" basic composition function instead of $\Box$
which is quite confusing. Similarly, the authors do not seem to be
completely consistent in their use of $f$, $\bar{f}$ and $\bar{f}$. Finally at
the beginning of subsection 4.4, $\vec{x}$ and $R set^n$ are used instead
of the squig-vector x and $ R set^d$ as in the rest of subsection 4.4
(or even the rest of section 4). [IssueA]These impreciseness issues carry over
to the text which is extremely verbose and lacks clarity. The text is
also encumbered and made painful to read by the presence of extremely
trivial and useless lemmas such as lemma 9.}
\comment{[R3,C7][IssueB]}{I believe the paper in its current form is very challenging to read. There are many grammatical errors and minor typos, but this is could be solved through extensive proofreading. What is really difficult to follow at times is the wealth of notations, and the fact that these notations are loosely defined. Look for instance at Section 2.1 "The Notation and The Idea" (maybe better formulated as "Notations and Ideas"):
\begin{itemize}
\item the set of trees $\mathbb{T}$ is not defined. Are these rooted trees? labeled trees? tree forests?
\item $\mathcal{I}$ is introduced for trees butc$ I(.)$ is introduced on *subtrees*. Yet aren't these objects both in $\mathbb{T}$? Depending on the situation, wouldn't it be possible that a tree T is both a tree and a subtree? in that case, what would $\mathcal{I}(T)$ and $I(T)$ have in common?
\item Is there a way the authors could do without the hats, the curvy arrows and other distractions which prevent the reader from understanding more clearly the basic linear algebra which is described?
\item f is heavily overloaded. First in section 2, then in p. 14 when it appears again with a completely different meaning than that used earlier in the paper.
\item p.5 between equations 2\&3: is f linear? if yes, please use this fact explicitly earlier in your description.
\end{itemize}}
\comment{[R3,C11][IssueA][IssueB]}{Notations are poor, and I encourage the authors to start from scratch and redefine them completely, now that they have a more clear idea of what their target result is.}
\comment{[R1,C11][IssueB]}{There is a lot of notation whose definitions are spread throughout the paper. It might be useful to provide a table of notation at the end of the paper. There is also some re-use of symbols.}
\begin{small}
\begin{tabular}{lp{10cm}}
\emph{Symbol type} & \emph{Description of its use}\\
\hline
$X$, $Y$, $A$, $B$, $S$, $\ldots$ & sets containing structures \\
$x$, $y$, $a$, $b$, $\ldots$ & a complete structure (eventually a substructure of a bigger structure)\\
$x_i \in X_i$  & the $i$-th part of the structure $x$ and $X_i$ is the related set of structures \\
$\substr{x} = (x_1,...,x_M) \in X_1 \times \ldots \times X_M$ & a decomposition into parts of a structure $x$\\
$|\substr{x}| = M$ & it is the number of parts the structure $x$ is decomposed into\\
$R(x)$  & the set of all the possible decomposition into parts of $x$\\
$R_i(x)$  & as above, but it denotes a specific way for extracting the possible decomposition into parts \\
$S(x)$  & the set of all the possible substructures of $x$\\
$S_i(x)$  & as above, but it denotes a specific way for extracting the possible substructures of $x$\\
${\mathbb S}(\substr{x})$ & $S_1(x_1) \times  \ldots \times S_M(x_M)$\\
$S(x) = \{ a | \substr{x} \in R(x), \substr{a} \in {\mathbb S}(\substr{x})\}$\\
\hline
$K(x,y)$ &  a kernel function between $x$ and $y$\\ 
$K_i(x,y)$ &  a specific kernel function between $x$ and $y$\\ 
\hline
$D(x)$ &  a distributed convolution function on a structure $x$\\ 
$D_i(x)$ &  a specific distributed convolution function on a structure $x$\\ 
\hline
$\OBV{x}$ & the base vector in $\R^m$ representing the substructure $x$\\
$\delta(x,y)$ &  the Kroneker delta between $x$ and $y$\\
%$\omega_{x,y}$ &  the weight assigned to $\delta(x,y)$\\
$\omega_{x}$ &  the weight assigned to a structure $x$ \mysecondinsert{or the value assigned to the feature $x$}\\
\hline
%%\vspace{.2em}
%$\vec{x} \in {\mathbb R}^m$ &  the original vector representing the structure $x$ in the feature space ${\mathbb R}^m$\\
$\svec{x} \in {\mathbb R}^d$ &  the vector representing its distributed representation of a structure $x$ in the reduced space ${\mathbb R}^d$\\
$\smallvectors{S},\smallvectors{S'}, \ldots$ & discrete subsets of ${\mathbb R}^d$\\
$MOV(\varepsilon,\theta)$ & a sets of nearly orthorormal vectors with the Properties \ref{near_norm} and \ref{near_orth} defined in Section \ref{sec:compositional_definition}
\\
$\df(x)$ &  the function that maps a structure $x$ in the vector $\svec{x}$ representing its distributed representation\\
$\basicdf(x)$ & the basic random indexing function that maps a final object in its random vector\\
\hline
$\mynodes{t}$ & the set of nodes of a given tree $t$\\
$\mychar{s}$ & the set of characters of a given sequence $s$\\
\hline
$\num{s}{t}$ & the frequency of the substructure $s$ in the structure $t$\\
\hline
$\svec{x}\mo\svec{y}$     & Ideal basic \mysecondinsert{binary}\mysecondremove{vector composition} operator \mysecondinsert{on vectors}\\
$\shuf{a}{\svec{x}}$ & a transformation function that permutes the vector components where $a$ is a parameter of the permutation\\
$\svec{x}\rop\svec{y}$    & Real (but generic) basic \mysecondinsert{binary}\mysecondremove{vector composition} operator without permutation \\
$\svec{x}\comp\svec{y}=\shuf{a}{x}\rop\shuf{b}{y}$ &  Real (but generic) basic \mysecondinsert{binary}\mysecondremove{vector composition} operator  with permutation \\
$\svec{x}\prodmo\svec{y}$ & $\gamma$-element-wise Shur product\\
$\svec{x}\shufprod\svec{y}=\shuf{a}{x}\prodmo\shuf{b}{y}$ & transformed  $\gamma$-element-wise Shur product (the selected transformation is the random shuffling after Section \ref{sec:approx}) \\
$\svec{x}\cconv\svec{y}$ & circular convolution\\
$\svec{x}\shufcconv\svec{y}=\shuf{a}{x}\cconv\shuf{b}{y}$ & transformed circular convolution (the selected transformation is the random shuffling after Section \ref{sec:approx})\\
\end{tabular}
\end{small}



%%%%%%%%%%%%%%%%%%%%%%%

\section{}
\label{_demonstration_}

\begin{lemma}
Given the subset $V_1 = \{v| v \in \{0,1\}^m \text{ and }  ||v||=1 \}$ such that $|V_1|=m$, there exists a function $f:\R^m \rightarrow \R^d$ such that, for any $v_a$ and $v_b$ in $V_1$, these two properties hold:
\begin{eqnarray*}
P(1-\epsilon<||f(v_a)||^2<1-\epsilon)> 1 - \theta\\
P(|f(v_a) \cdot f(v_b)| < 1.5\epsilon) > (1 - \theta)^3
\end{eqnarray*}
and the lower-bound of the dimension $d$ of the space $\R^d$ is $\Omega(\epsilon^{-2}\log m \log 1/\theta)$.
\end{lemma}
\begin{proof}
Theorem 4.3 in \cite{Jayram:2011:OBJ:2133036.2133037} proves that there exists a function $f:\R^m \rightarrow \R^d$  such that, for any vectors $v'$ and $v''$ in $\{0,1\}^m$, these two properties hold:
\begin{displaymath}
P(||v'||^2-\epsilon<||f(v')||^2<||v'||^2+\epsilon)>1-\theta
\end{displaymath}
\begin{displaymath}
P(||v'-v''||^2-\epsilon<||f(v') - f(v'')||^2<||v'-v''||^2+\epsilon)>1-\theta
\end{displaymath}
and the lower-bound of the dimension $d$ of the space $\R^d$ is $\Omega(\epsilon^{-2}\log m \log 1/\theta)$.
Thus, for vectors $v_a,v_b \in V_1 \subset \{0,1\}^m$, the above properties hold. $v_a$ and $v_b$ are unit vectors and orthogonal. Thus:
\begin{displaymath}
P(1-\epsilon<||f(v_a)||^2<1+\epsilon)> 1 - \theta
\end{displaymath}
\begin{displaymath}
P(1-\epsilon<||f(v_b)||^2<1+\epsilon)> 1 - \theta
\end{displaymath}
Observing that $||v_a-v_b||^2 = ||v_a||^2 + ||v_b||^2 - 2 v_a \cdot v_b$ = 2 and that $||f(v_a)-f(v_b)||^2 = ||f(v_a)||^2 + ||f(v_b)||^2 - 2 f(v_a)\cdot f(v_b)$, we have that:
\begin{displaymath}
P(||v_a-v_b||^2-\epsilon<||f(v_a) - f(v_b)||^2<||v_a-v_b||^2+\epsilon)>1-\theta
\end{displaymath}
becomes:
\begin{displaymath}
P(2-\epsilon<||f(v_a)||^2 + ||f(v_b)||^2 - 2 f(v_a)\cdot f(v_b)<2+\epsilon)>1-\theta
\end{displaymath}
By combining the three events in a joint event and by considering these events as independent events, we have:
\begin{displaymath}
P(|f(v_a) \cdot f(v_b)| < 3/2\epsilon) > (1 - \theta)^3
\end{displaymath}
\end{proof}


$$\Omega(\overline{\epsilon}^{-2}\log m \log 1/\overline{\theta})$$
where:
$\varepsilon\mysecondinsert{=1.5\overline{\epsilon}}$ and $\theta=\overline{\theta}^3 - 3 \overline{\theta}^2 + 3 \overline{\theta}$. 
that is:
$$
\overline{\theta} = 1 + \sqrt[3]{\theta - 1}
$$

%%% EQUAZIONE DEL POLINOMIO DI GRADO 3 : http://utenti.quipo.it/base5/numeri/equasolutore.htm




\section{Theorems 4.2 and 4.3 of \cite{Jayram:2011:OBJ:2133036.2133037}}



\begin{theorem}[Theorem 4.2.] The \textbf{one-way communication complexity}
of the problem of approximating the $\|\cdot\|_p$ difference
of two vectors of length n to within a factor $1 + \varepsilon$ with
failure probability at most $\delta$ is $\Omega(\varepsilon^{-2}\log n \log (1/\delta))$
\end{theorem}


The \textbf{one-way communication complexity} of Q is the minimum \textbf{communication
cost} of a protocol for Q with failure probability at
most $\delta$.

\textbf{Communication cost of the protocol P}: The maximum length
of Alice's message (in bits) over all inputs.


\section{Counting nearly-orthogonal unit vectors in $\R^d$}
\label{sec:_demonstration_}
\begin{theorem}[Johnson-Lindenstrauss Lemma]
\label{th:JJL} For any $0 < \epsilon < 1$ and any integer $m$. Let $d$ be a positive integer such that
%\begin{center}
%$d \geq 4 (\epsilon^2/2 - \epsilon^3/3)^{-1} \ln m$.
%\end{center}
$$d=O(\epsilon^{-2}\log{m})$$
Then for any set $V$ of $m$ points in ${\mathbb R}^k$, there is a map $f : {\mathbb R}^k \rightarrow {\mathbb R}^d$ such that for all $\vec{u}, \vec{v} \in V$, 
\begin{center}
$(1- \epsilon) \|\vec{u}-\vec{v}\|^2_2 \leq \|f(\vec{u})-f(\vec{v})\|^2_2 \leq  (1 + \epsilon)\|\vec{u}-\vec{v}\|^2_2 $.
\end{center}
\end{theorem}

The theorem can be derived using the following lemma:
\begin{lemma}
\label{th:lemma_1}
For any $\epsilon>0$, $\tau < 1/2$ and positive integer $d$, there exists a distribution $\mathcal D$ over $\R^{d\times k}$ for $d=O(\epsilon^{-2}\log{1/\tau})$ such that, for any $\vec{x} \in \R^k$ with $||\vec{x}||_2=1$,
$$\P(| \|A\vec{x}\|_2^2 -1  | > \epsilon) < \tau$$
\end{lemma}
by choosing $\tau = 1/m^2$ and by applying the union bound on the vectors $(\vec{u} - \vec{v})/\|\vec{u}-\vec{v}\|_2$ for all vectors $\vec{u}$ and $\vec{v}$ in $V$. It is possible to demonstrate that there is a probability strictly greater than $0$ that a function $f$ exists.

Now we can demonstrate that the following lemma holds:
\begin{corollary}
\label{existence_of_f}
For any $0 < \epsilon < 1$ and any integer $m$. Let $d$ be a positive integer such that
%\begin{center}
%$d \geq 4 (\epsilon^2/2 - \epsilon^3/3)^{-1} \ln m$.
%\end{center}
$$d=O(\epsilon^{-2}\log{m})$$
Then given the standard basis $E$ of ${\mathbb R}^m$, there is a map $f : {\mathbb R}^m \rightarrow {\mathbb R}^d$ such that for all $\vec{e}_i, \vec{e}_j \in E$, 
\begin{equation}
\P(1 - \epsilon<\|f(\vec{e}_i)\|_2^2 <1 + \epsilon) > 1 - \tau = 1 - 1/m^2
\label{eq:p1}
\end{equation}
and
\begin{equation}
\P(|f(\vec{e}_i)f(\vec{e}_j)|<2\epsilon)> (1 - \tau)^2 = (1 - 1/m^2)^2
\label{eq:p2}
\end{equation}
\end{corollary}

\begin{proof}
Equation (\ref{eq:p1}) derives from lemma \ref{th:lemma_1} as $\vec{e}_i \in E$ are unitary, that is,  $\|\vec{e}_i\|_2=1$ as $\tau = 1/m^2$.

To prove Equation (\ref{eq:p2}), first, we can observe that $||\vec{e}_i-\vec{e}_j||^2 = ||\vec{e}_i||^2 + ||\vec{e}_j||^2 - 2 \vec{e}_i\vec{e}_j$ = 2 as $\vec{e}_i$ and $\vec{e}_j$ are unitary and orthogonal. Then, we can see that $||f(\vec{e}_i)-f(\vec{e}_j)||^2 = ||f(\vec{e}_i)||^2 + ||f(\vec{e}_j)||^2 - 2 f(\vec{e}_i)f(\vec{e}_j)$.
With Theorem \ref{th:JJL}, the following holds:
$$
2(1-\epsilon)\leq||f(\vec{e}_i)||^2 + ||f(\vec{e}_j)||^2 - 2 f(\vec{e}_i)f(\vec{e}_j)\leq2(1+\epsilon)
$$
Hence:
$$
||f(\vec{e}_i)||^2 + ||f(\vec{e}_j)||^2 -2-2\epsilon\leq 2 f(\vec{e}_i)f(\vec{e}_j)\leq||f(\vec{e}_i)||^2 + ||f(\vec{e}_j)||^2  -2+2\epsilon
$$
Thus, using Equation (\ref{eq:p1}) on the two independent events $f(\vec{e}_i)$ and $f(\vec{e}_j)$:  
$$
\P(2 - 2\epsilon -2 -2\epsilon\leq 2 f(\vec{e}_i)f(\vec{e}_j)\leq2+2\epsilon  -2+2\epsilon) = \P(|f(\vec{e}_i)f(\vec{e}_j|<2\epsilon) > (1 - \tau)^2
$$

\end{proof}

Putting together Equation (\ref{eq:p1}) and Equation (\ref{eq:p2}), it is possible to derive a set $NOV(\varepsilon,\theta)$ of $m$ nearly-orthogonal unit vectors such that for each $\vec{a},\vec{b}\in NOV(\varepsilon,\theta)$:
$$\P(\delta(\vec{a},\vec{b}) - \varepsilon \leq \dotprod{\vec{a}}{\vec{b}} \leq \delta(\vec{a},\vec{b}) + \varepsilon) > 1 -\theta$$
by choosing $\varepsilon = 2\epsilon$, a space $\R^d$ with $d=O(\varepsilon^{-2}\log{m})$ and $\theta = 2/m^2 - 1/m^4$.



\section{Properties of d-dimensional Gaussian Vectors $\mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$}
\label{properties_of_normal_vectors}


In the rest of the paper, the property for vectors to belong to $NOV(\varepsilon,\theta)$ (Equation (\ref{eq:nov})) is also referred as the following two properties:
\finalcomment{$|| g || = 1$: in which norm?}{R3.5}
\begin{property}\label{near_norm}
\emph{(Nearly Unit Vectors)}
A vector $\svec{a} \in NOV(\varepsilon,\theta)$ is a nearly unit vector, that is:
$$\P(1 - \epsilon \leq ||\svec{a}||^2_2 \leq 1 + \epsilon) \geq 1-\theta$$
where $||\svec{a}||_2$ is the ${\ell}^2$ norm (Euclidean norm)
\end{property}
\begin{property}\label{near_orth}\emph{(Nearly Orthogonal Vectors)}
Two different vectors $\vec{a},\vec{b}\in NOV(\varepsilon,\theta)$ are nearly orthogonal if: 
$$\P(| \dotprod{\svec a}{\svec b}| \leq \epsilon) \geq 1-\theta$$ 
\end{property}








Taking into account the potentially infinite dimensional spaces $\mathcal H$ of substructures, our method uses directly $\mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ as $NOV(\varepsilon,\theta)$. In this way, we can extract potentially infinite vectors that pairwise respect Equation \ref{eq:nov}. We showed (see Appendix \ref{properties_of_normal_vectors}) that vectors $\vec a, \vec b \sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ have the following distributions for Properties \ref{near_norm} and \ref{near_orth} splitting Equation~\ref{eq:nov}:
\begin{itemize}
\item[]  For Property \ref{near_norm},
%$$\mathbb{P}(1-\varepsilon<\|\vec{a}\|^{2}<1+\varepsilon) = F\left(d(1+\varepsilon)\right)-F\left(d(1-\varepsilon\right))$$
$\mathbb{P}(1-\varepsilon<\|\vec{a}\|^{2}<1+\varepsilon) = \frac{1}{\Gamma\left(\frac{d}{2}\right)}\gamma\left(\frac{d}{2},\frac{d(1+\varepsilon)}{2}\right) - \frac{1}{\Gamma\left(\frac{d}{2}\right)}\gamma\left(\frac{d}{2},\frac{d(1-\varepsilon)}{2}\right)$
where $\Gamma(\cdot)$ is the gamma function and $\gamma(\cdot,\cdot)$ is the lower incomplete Gamma function \cite{gammafunction}
\item[] For Property \ref{near_orth},
$\mathbb{P}(\vert \dotprod{\vec a}{\vec b}\vert\leq\varepsilon)=\int_{-\varepsilon}^{\varepsilon}p_{Z}\left(x;d,\sigma\right)\,\text{d}x$
with:
$
p_Z(t;d)=\frac{2^{\frac{1}{2}-\frac{d}{2}} d^{\frac{d+1}{4}} \left| x\right| ^{\frac{d-1}{2}} K_{\frac{d-1}{2}}\left(\sqrt{d} \left| x\right| \right)}{\sqrt{\pi } \Gamma \left(\frac{d}{2}\right)}
$
where $K_{\alpha}(x)$ is the modified Bessel function of the second kind.  
\end{itemize}
These distributions show that vectors drawn from $\mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ with $d=4096$ and $d=8192$ have a very high probability of respecting the two properties with $\varepsilon=0.1$, that is, $\theta \propto 10^{-xxx}$ and $\theta \propto 10^{-yyy}$, respectively. With these $\theta$, it is possible to approximate convolution kernels with dot products of distributed convolution structures (cf. Equation \ref{the_core_property}). 



\section{Code}

The code of the distributed convolution kernels is available at:
\begin{center}
http://code.google.com/p/distributed-tree-kernels/
\end{center}



\bibliography{refined_bibliography,math}
\bibliographystyle{theapa}

\end{document}



\section{\comment{Da buttare recuperando poco!}}

\subsection{Notations and Ideas}


Figure \ref{the_big_picture} shows an example to 
%fix the notation. 
fix notations.
The tree $T$ is on the left side and two of its possible tree fragments $\tau_i$ and $\tau_j$ are on the right side (see Figure \ref{the_big_picture}.(a)). The two tree fragments are two dimensions of the underlying vector space.
In these vector spaces, trees $T$ are then represented as vectors $\vec{T} = {\mathcal I}(T) \in {\mathbb R}^m$ 
where each \emph{dimension} $\vec{\tau}_i$ corresponds to a tree fragment $\tau_i$ (see Figure \ref{the_big_picture}.(b)). 
The function  ${\mathcal I}(\cdot)$ is the mapping function between the space ${\mathbb T}$ and ${\mathbb R}^m$. The trivial weighting scheme assigns $\omega_i = 1$ to dimension $\vec{\tau}_i$ if the tree fragment $\tau_i$ is in the original tree $T$ and $\omega_i = 0$ otherwise. Different weighting schemes are possible and are used. 
The counting of the common tree fragments done by the tree kernels $TK(T_1,T_2)$ is, by construction, the dot product of the two vectors $\vec{T_1}\cdot\vec{T_2}$ representing the trees in the feature spaces ${\mathbb R}^m$  of the tree fragments, that is:
\begin{displaymath}
TK(T_1,T_2) = \vec{T_1}\cdot\vec{T_2}
\end{displaymath}
As these  feature spaces of the tree fragments ${\mathbb R}^m$ are huge, kernel functions $TK(T_1,T_2)$ are used to implicitly compute the similarity $\vec{T_1}\cdot\vec{T_2}$ without explicitly representing vectors $\vec{T_1}$ and $\vec{T_2}$. But, these kernel functions are generally computationally expensive. 


\begin{figure}
\begin{center}
\begin{tabular}{cc|rcccclr}

%%% TREES
\multirow{7}{*}{(a)} & $x$ & \hspace{2mm} \ldots & $x_i$ & \ldots & $x_j$ & \ldots & & \hspace{5pt}\multirow{22}{*}{\ntnode{CC}{$\df(\tau) = f(I(\tau))$}} \\

& \begin{parsetree}( .S. (.NP.  `we') ( .VP. (.V.    `looked' ) ( .NP.   `them') ) )\end{parsetree} & \ldots & \begin{parsetree} (.S. .NP. .VP.) \end{parsetree} & \ldots & \begin{parsetree}(.VP.  .V.  ( .NP. .them.)  ) \end{parsetree} & \ldots &  
\multirow{1}{*}{ $\Bigg\} {\mathbb T}$} \ntnode{T}{} & \\

\cline{1-8}7
%%% OLD SPACE
\multirow{10}{*}{(b)} & $\vec{x} = {\mathcal I}(x)$ & \ldots & $\vec{x}_i = I(x_i)$ & \ldots & $\vec{x}_j = I(x_j)$ & \ldots & \\ 

& \begin{tiny} $\left(\begin{array}{c}0\\ \vdots \\0\\1\\0\\ \vdots \\0\\1\\0\\ \vdots \\0\end{array} \right)$ \end{tiny} & \ldots & \begin{tiny}$ \left(\begin{array}{c} 0\\ \vdots \\0\\1\\0\\ \vdots \\0\\0\\0\\ \vdots \\0 \end{array}\right)$ \end{tiny} &\ldots & \begin{tiny}$ \left(\begin{array}{c} 0\\ \vdots \\0\\0\\0\\ \vdots \\0\\1\\0\\ \vdots \\0\end{array}\right)$\end{tiny} & \ldots & \multirow{1}{*}{$\Bigg\} {\mathbb R}^m$} & \\
\cline{1-8}

%%% NEW SPACE
\multirow{4}{*}{(c)} & $\svec{x}= f(\vec{x})$ & \ldots & $f(\vec{x}_i) = \svec{x}_i$ & \ldots & $f(\vec{x}_j) = \svec{x}_j$ & \ldots & \\

& \begin{tiny}$\left(\begin{array}{c} 0.0002435\\ 0.00232\\\vdots \\-0.007325\end{array}\right)$
\end{tiny} & \ldots & \begin{tiny} $\left(\begin{array}{c} -0.0017245\\ 0.0743869\\\vdots \\0.0538474 \end{array} \right)$\end{tiny} & \ldots & \begin{tiny}$\left(\begin{array}{c}
0.0032531\\ -0.0034478\\\vdots \\-0.0345224\end{array}\right)$\end{tiny}&\ldots&\multirow{1}{*}{$\Big\} {\mathbb R}^d$} \ntnode{R}{}\nodecurve[r]{T}[t]{CC}{20pt} \anodecurve[b]{CC}[r]{R}{20pt}& \\
\cline{1-8}
\end{tabular}
%\includegraphics[width=15cm]{Figures/Figurona_statica}
\end{center}
\caption{Map of the used spaces and functions: the tree fragments ${\mathbb T}$, the full feature space of tree fragments ${\mathbb R}^m$ and the reduced space ${\mathbb R}^d$; the function ${\mathcal I}(\cdot)$ that maps trees $T$ in vectors of ${\mathbb R}^m$, the function $I: {\mathbb T}\rightarrow E$ where $E$ is the standard orthonormal basis of ${\mathbb R}^m$, the space reduction function $f: {\mathbb R}^m\rightarrow{\mathbb R}^d$, and the direct function $\df: {\mathbb T}\rightarrow{\mathbb R}^d$. Examples are given for trees, tree fragments, and vectors in the two different spaces.}
\label{the_big_picture}
\end{figure}

\comment{Il discorso dovrebbe essere fatto in riferimento a generici structured object, magari separandolo in una sezione a parte e lasciando una sezione sull'esempio del tree kernel (inclusa l'ultima parte della sezione precedente)}
Our idea with the distributed tree kernels is to map vectors $\vec{T} \in {\mathbb R}^m$ into a lower dimension space ${\mathbb R}^d$, with $d \ll m$  (see Fig. \ref{the_big_picture}.(c)), to allow for an approximated but faster and explicit computation of these kernel functions. In these lower dimension spaces the kernel computation, being a simple dot product, is extremely efficient. 
%  in the explicit space $$ 

A direct embedding $f: {\mathbb R}^m \rightarrow {\mathbb R}^d$ is, in principle, possible with techniques like singular value decomposition or random indexing \citep[see][]{sahlgren05} but it is impractical due to the huge dimension of ${\mathbb R}^m$. 
Then, to map vectors $\vec{T}$  in the explicit space ${\mathbb R}^m$ into a lower dimension space ${\mathbb R}^d$, we need a function $\df$ that directly maps trees $T$ into vectors $\svec{T}$ much smaller than the implicit vector $\vec{T}$ used by classic tree kernels. This function acts from the set of trees to the space ${\mathbb R}^d$, i.e., $\df: {\mathbb T} \rightarrow {\mathbb R}^d$ (see Fig. \ref{the_big_picture}).
For an assonance \comment{assonance?} with Distributed Representations (\cite{Plate1995}), we call $\svec{\tau}_i$ a Distributed Tree Fragment (DTF), whereas $\svec{T}$ is a Distributed Tree (DT).  We then define the Distributed Tree Kernel (DTK) between two trees as the dot product between the two Distributed Trees, that is:
\begin{displaymath}
DTK(T_1,T_2)  \triangleq \svec{T_1}\cdot\svec{T_2} = \df(T_1) \cdot \df(T_2)
\end{displaymath}

%The rest of the section is organized as follows. First, we set more formally the challenges of our idea (Sec. \ref{sec:method}). Second, we attack these challenges by showing that a function $f$ with the desired properties actually exists (Sec. \ref{sec:f}), by describing the direct mapping function $\df$ and proving its properties (Sec. \ref{sec:df}), and, finally, by introducing a method to efficiently compute the mapping from a tree $T$ to its distributed representation $\svec{T}$ for a specific tree fragment space without explicitly enumerating the tree fragments (Sec. \ref{sec:T}). 


We expect that distributed tree kernels approximate the original tree kernels, that is:
\begin{equation}\label{basic_dtk_tk_equivalence}
DTK(T_1,T_2) = \svec{T_1}\cdot\svec{T_2} \approx \vec{T_1}\cdot\vec{T_2} = TK(T_1,T_2)
\end{equation}




\subsection{The Challenges}
\label{sec:method}

The function $\df : {\mathbb T} \rightarrow {\mathbb R}^d$  linearizes trees in low dimension vectors and, then, has a crucial role in the overall picture. We need then to clearly define which properties we expect for this function. 
%In this section, we want to show how we can obtain distributed tree fragments $\svec{\tau}$ and distributed trees $\svec{T}$ and what are the challenges we need to face. 

To derive the properties of function $\df$, we need to examine the relation between the traditional tree kernel mapping ${\mathcal I}:{\mathbb T} \rightarrow {\mathbb R}^m$ that maps trees into the feature spaces of the tree fragments, $I:{\mathbb T} \rightarrow {\mathbb R}^m$ that maps tree fragments into the standard orthogonal basis of ${\mathbb R}^m$, the additional function $f: {\mathbb R}^m \rightarrow {\mathbb R}^d$, that maps $\vec{T}$ into a smaller vector $\svec{T}=f(\vec{T})$, and our newly defined function $\df$.


The first function to examine is the embedding function $f$. It works as a sort of \emph{approximated linear transformation} of the basis of ${\mathbb R}^m$ in ${\mathbb R}^d$ by embedding the first space in the second. This is the embedding function that could be obtained using techniques like singular value decomposition or random indexing. But, as already observed, this is impractical due to the huge dimension of the feature spaces of the tree fragments ${\mathbb R}^m$. This function is useful to introduce and to formally justify our objective of building a function $\df$ (see always Figure \ref{the_big_picture} as a reference).

We can start by observing that each vector $\vec{T}\in {\mathbb R}^m$ can be trivially represented as:\\
\begin{displaymath}
\vec{T} = \sum_{i} \omega_i\vec{\tau}_i
\end{displaymath}
where each $\vec{\tau}_i$ represents the unitary vector corresponding to the tree fragment $\tau_i$, i.e., $\vec{\tau}_i = I(\tau_i)$ where $I(\cdot)$ is the mapping function from tree fragments to vectors of the standard basis. In other words, the set $\{ \vec{\tau}_1 \ldots \vec{\tau}_m\}$ corresponds to the \emph{standard basis} $E = \{ \vec{e}_1 \ldots \vec{e}_m\}$ of ${\mathbb R}^m$, whose vectors $e_i$ have elements $e_{ii} = 1$ and $e_{ij} = 0$ for $i\neq j$.
Then, the dot product between two vectors $\vec{T_1}$ and $\vec{T_2}$ is:
\begin{equation}\label{dtk_tk_equivalence}
\vec{T_1}\cdot\vec{T_2} = \sum_{i,j} \omega_i^{(1)}\omega_j^{(2)}\vec{\tau}_i \vec{\tau}_j = \sum_{i} \omega_i^{(1)}\omega_i^{(2)}
\end{equation}
given the properties of the orthonormal basis  $\{ \vec{\tau}_1 \ldots \vec{\tau}_m\}$ and where $\omega_i^{(k)}$ is the weight of the $i$-th dimension of the vector $\vec{T_k}$. This reading is trivial but it is useful for better explaining the other functions.

We can now examine the expected properties of $f$ with respect to the final aim of approximating tree kernels with distributed tree kernels (see equation \ref{basic_dtk_tk_equivalence}). The approximated $\svec{T} \in {\mathbb R}^d$ can be rewritten as:
\begin{displaymath}
\svec{T} = f(\vec{T}) = f(\sum_{i} \omega_i\vec{\tau}_i) = \sum_{i} \omega_if(\vec{\tau}_i) = \sum_{i} \omega_i\svec{\tau}_i
\end{displaymath}
where each $\svec{\tau}_i$ represents tree fragment $\tau_i$ in the new space. The embedding function $f$ \comment{Bisogna scrivere in qualche modo che ha la proprietà $f(\alpha x + \beta y) = \alpha f(x) + \beta f(y)$} then maps vectors $\vec{\tau}$ of the standard basis $E$ into some corresponding vectors $\svec{\tau}\in {\mathbb R}^d$.
To approximately preserve the dot product (see equation \ref{basic_dtk_tk_equivalence}),
% of two vectors $\vec{T_1}$ and $\vec{T_2}$  with the dot product of the two approximated vectors $\svec{T_1}$ and $\svec{T_2}$, 
that is:\\
\begin{equation}
\label{eq:equivalence}
\svec{T_1}\cdot\svec{T_2} = \sum_{i,j} \omega_i^{(1)}\omega_j^{(2)}\svec{\tau}_i \svec{\tau}_j \approx \sum_{i} \omega_i^{(1)}\omega_i^{(2)} = \vec{T_1}\cdot\vec{T_2} 
\end{equation}
%To preserve, to some extent, the properties of vectors $\vec{\tau}$, t
the set of vectors $\widetilde{E} = \{ \svec{\tau}_1 \ldots \svec{\tau}_m\}$ should then be a sort of \emph{approximated basis} of ${\mathbb R}^d$. 
%As $\widetilde{E}$ should be an approximated basis and the transformation should preserve the dot product,
Then, these two properties should hold for the vectors\comment{R1, C1}{These properties seem sufficient but not necessary} $f(\vec{\tau})$ in $\widetilde{E}$:
\begin{property}\label{near_norm}
\emph{(Nearly Unit Vectors)}
A distributed tree fragment $\svec{\tau}$ representing a tree fragment $\tau$ is a nearly unit vector: $ 1 - \epsilon < ||\svec{\tau}|| < 1 + \epsilon$
\end{property}
\begin{property}\label{near_orth}\emph{(Nearly Orthogonal Vectors)}
Given two different tree fragments $\tau_1$ and $\tau_2$, their distributed vectors are nearly orthogonal:  if $\tau_1 \neq \tau_2$, then $|\svec{\tau_1} \cdot \svec{\tau_2}| < \epsilon$
\end{property}


We discussed the embedding function $f$ and its expected properties but we also know that a direct realization of this function is impractical although possible. We are now ready to introduce the role of function $\df$. As vectors $\svec{\tau} \in \widetilde{E}$ represent the basic tree fragments $\tau$, the idea is that $\svec{\tau}$ can be obtained directly from tree fragments $\tau$ using a function $\df(\tau)= f(I(\tau))$ that composes $f$ and $I$. Using this function to obtain distributed tree fragments $\svec{\tau}$, distributed trees $\svec{T}$ can be obtained as follows: 
\begin{displaymath}
\svec{T} = \df(T) =  \sum_{i} \omega_i\df(\tau_i) = \sum_{i}\omega_i\svec{\tau}_i
\end{displaymath}


In the rest of the paper, we need to address the following issues:
\begin{itemize}
\item First, we need to show that an embedding function $f: {\mathbb R}^m \rightarrow {\mathbb R}^d$ exists. This function $f$ should have the property of keeping the vectors $\svec{\tau}_i$ nearly orthogonal and should preserve the dot product between distributed trees $\svec{T}$.
%for each pair $\vec{\tau}_i, \vec{\tau}_j \in E$, $|f(\vec{\tau}_i) \cdot f(\vec{\tau}_j)| < \epsilon$ with a high probability for each $i \neq j$ where $0<\epsilon<1$. 
%The expected value $E(\svec{\tau}_i \cdot \svec{\tau}_j)$ should be 0. 
Using the Johnson-Lindenstrauss Lemma (\cite{JLL}), it is possible to show that such a function exists. We can also find the relation between $\epsilon$, $d$, and $m$. This is useful to estimate how many nearly orthogonal distributed tree fragments $\tau$ can be encoded in the space ${\mathbb R}^d$, given the expected error $\epsilon$. 

\item Second, we want to build the function $\df$ that directly computes $\svec{\tau}_i$ using the structure of the tree fragment $\tau_i$. The function $\df$ merges two mapping functions $f$ and $I$ as $\svec{\tau}_i = f(\vec{\tau}_i) = f(I(\tau_i)) = \df(\tau_i)$. The function $\df(\tau_i)$ is based on the use of a set of nearly orthogonal vectors for the nodes of the tree fragments and a basic vector composition function $\mo$. We need to show that, given specific properties of function $\mo$ and given two trees $\tau_i$ and $\tau_j$, $\df(\tau_i)$ and $\df(\tau_j)$ are reasonably nearly orthogonal, i.e., $\df(\tau_i) \cdot \df(\tau_j) < \epsilon$ with a high probability.

\item Finally, we need to show that vectors $\svec{T}$ can be efficiently computed using dynamic programming for the spaces of tree fragments underlying the selected tree kernels: \cite{Collins2002}'s tree kernels, the subpath tree kernels \cite{Kimura:2011:SKR:2017863.2017871}, and the route tree kernels \cite{Aiolli2009}. 
\end{itemize}

Given the three above issues solved (see, respectively, Sections \ref{sec:f}, \ref{sec:compositional_definition}, and \ref{sec:attk}), we need to demonstrate that the related distributed versions of the kernels approximate the kernels in the original space. Equation (\ref{dtk_tk_equivalence}) should hold for the distributed parse tree kernels (DPTK), the distributed subpath tree kernels (DSTK), and the distributed route tree kernels (DRTK) with a high probability. We need also to demonstrate that its computation is more efficient. Section \ref{sec:experiments} discusses this last issue.
% whereas the above three points are addressed in the following sections.





\subsection{Distributed Subpath Tree Kernel}

In this section, we show that the Distributed Tree framework can be applied to tree kernels other than the classic TK by \cite{Collins2002}. We consider the Subpath Tree Kernel (STK) for unordered trees \cite{Kimura:2011:SKR:2017863.2017871}. STK considers tree subpaths as features. This section follows the same organization of the previous one.


\subsubsection{Distributed Tree Fragments for the Subpath Tree Kernel}

A subpath of a tree is formally defined as a substring of a path from the root to one of the leaves of the tree.

\begin{table}[h]
\begin{center}
\begin{eqnarray*}
&&P(\begin{array}{c}\parsetree{(.A. (.B. .W1.) (.C. (.D. .W2.)(.E. .W3.)))}\end{array})=\{
\begin{array}{c}\parsetree{(.A. . B .)}\end{array},
\begin{array}{c}\parsetree{(.A. (. B . .W1.))}\end{array},
\begin{array}{c}\parsetree{(. B . .W1.)}\end{array},
\begin{array}{c}\parsetree{(.A. (. C . (.D. .W2.)))}\end{array},
\begin{array}{c}\parsetree{(. C . (.D. .W2.))}\end{array},
\begin{array}{c}\parsetree{(.A. (. C . .D.))}\end{array},
\begin{array}{c}\parsetree{(.A. . C .)}\end{array},\\&&
\begin{array}{c}\parsetree{(. C . .D.)}\end{array},
\begin{array}{c}\parsetree{(. D . .W2.)}\end{array},
\begin{array}{c}\parsetree{(.A. (. C . (.E. .W3.)))}\end{array},
\begin{array}{c}\parsetree{(. C . (.E. .W3.))}\end{array},
\begin{array}{c}\parsetree{(.A. (. C . .E.))}\end{array},
\begin{array}{c}\parsetree{(. C . .E.)}\end{array},
\begin{array}{c}\parsetree{(. E . .W3.)}\end{array},
A, B, W1, C, D, E, W2, W1
\}
\end{eqnarray*}
\end{center}
\caption{Tree Fragments for the subpath tree kernel}
\label{stk:feature_space}
\end{table}


The subpath tree kernel uses very simple tree fragments: chains of nodes. Given a context-free grammar $G=(N,\Sigma,P,S)$, any sequence of non-terminal symbols $N$, possibly closed by one terminal symbol in $\Sigma$, is a valid tree fragment.
Function $S(T)$ can be defined accordingly and will be referred as $P(T)$. Given a tree $T$, $P(T)$ contains any sequence of symbols $a_0 ... a_n$ where $a_i$ is a direct descendent of $a_{i-1}$ for any $i>0$. The distributed tree fragments are then: $\svec{a_0}\mo(\svec{a_1} \mo ( ... \svec{a_n})...))$.

Figure \ref{stk:feature_space} proposes an example of the above function. 
In \cite{Kimura:2011:SKR:2017863.2017871}, the weight $w$ for the subpath feature $p$ for a given tree $T$ is:
\begin{displaymath}
w = \text{num}(T_p)\sqrt{\lambda^{|p|}}
\end{displaymath}
where $|p|$ is the length of the subpath $p_i$ and $\text{num}(T_p)$ is the number of times a subpath $p$ appears in the tree $T$. The related distributed tree fragments can be easily derived.

\subsubsection{Subpath Tree Kernel}

The kernel function between two trees $T_1$ and $T_2$ is then defined as:
\begin{equation}
 STK(T_1,T_2) = \sum_{p \in P(T_1) \cap P(T_2)} \lambda^{|p|}\text{num}({T_1}_p) \text{num}({T_2}_p)
\end{equation}
where $P$ is the set of all subpaths in $T_1$ and $T_2$ and $num(T_p)$ is the number of times a subpath $p$ appears in tree $T$. $\lambda$ is a parameter, similar to that of the classic TK, assigning an exponentially decaying weight to a subpath $p$ according to its length $|p|$.

A simple algorithm is the recursive definition of STK that follows:\\
\begin{equation}
\input{kernels/stk_1}
\end{equation}
The function $\Delta(n_1,n_2)$ is defined as:
\begin{displaymath}
\input{kernels/stk_2}
\end{displaymath}
where, as usual, $ch(n,i)$ is the i-th son of the node $n$ in the tree $T$.
More efficient algorithms are provided in \cite{Kimura2012,Kimura:2011:SKR:2017863.2017871}.
\comment{[R2,C7]}{Section 5.2.2: I do not understand the recursive computation of Delta(n1,n2).
\begin{itemize}
\item "n1 or  n2 is a terminal node and n1 = n2": do you mean  "n1 and n2 are terminal nodes and n1 = n2" ?
Makes no sense, as a terminal cannot be equal to a non-terminal
\item "if n1 and n2 are 2 non-terminal nodes": do you mean "if n1 and n2 head the same production" ?
With your definition, Delta would always be at least lambda for any pair of non-terminal!
\end{itemize}}

%For example, subpath \texttt{(A C D W2)}, appearing in the tree of Fig. \ref{sampletree}, is represented by vector $\svec{A} \mo (\svec{C} \mo (\svec{D} \mo \svec{W2}))$.

\begin{equation}
\input{kernels/stk_3}
\end{equation}



\subsubsection{Recursively computing Distributed Trees for the Subpath Tree Kernel}


As in the case of the classic TK, we can define a Distributed Tree representation $\svec{T}$ for tree $T$ such that the kernel function can be approximated by the explicit dot product, i.e. $STK(T_1,T_2) \approx \svec{T_1}\cdot\svec{T_2}$.
In this case, each standard unit vector $\vec{p_i}$ of the implicit feature space $\mathbb{R}^m$ corresponds to a possible subpath $p_i$. Thus, the Distributed Tree is:
\begin{equation}
 \svec{T} = \sum_{p \in P(T)} \lambda^{|p|}\svec{p}
\end{equation}
where $P(T)$ is the set of subpaths of $T$ and $\svec{p}$ is the Distributed Tree Fragment vector for subpath $p$. Subpaths can be seen as trees where each node has at most one child, thus their DTF representation is the same. 


Again, an explicit enumeration of the subpaths of a tree $T$ is impractical, so an efficient way to compute $\svec{T}$ is needed. The formulation in Eq. \ref{distributed_vectors} is still valid, as long as we define recursive function $s(n)$ as follows:
\begin{eqnarray}
&&\input{kernels/dst_1}\\
&&\input{kernels/dst_2}
\end{eqnarray}
where $C(n)$ is the set of children of node $n$.

%\subsubsection{Validity of the recursive computation}

%A theorem like Theorem \ref{overall_theorem} must be proved:
%
%\begin{theorem}\label{subpath_theorem}
%Given the ideal vector composition function $\mo$, the following equivalence holds:
%\begin{equation}
%\svec{T} = \sum_{n \in N(T)} s(n) = \sum_{p\in P(T)} \sqrt{\lambda^{|p|}}\svec{p}
%\label{the_equivalence}
%\end{equation}
%\end{theorem}


%%%% NON SERVE PERCHE' PROVATO PER DCK
%
%To prove Theorem \ref{subpath_theorem}, we introduce a definition and two simple lemmas, whose proof is trivial. In the following, we will denote by $(n|p)$ the concatenation of a node $n$ with a path $p$.
%
%\begin{definition}
%Let $n$ be a node of a tree $T$. We define $P(n) = \{p | p \mbox{ is a subpath of } T \mbox{ starting with } n\}$
%\end{definition}
%
%\begin{lemma}\label{subpath_lemma1}
%Let $n$ be a tree node and $C(n)$ the (possibly empty) set of its children. Then $P(n) = n \cup \displaystyle\bigcup_{c\in C(n)}{\displaystyle\bigcup_{p'\in P(c)}{(n|p')}}$.
%\end{lemma}
%
%\begin{lemma}\label{subpath_lemma2}
%Let $p= (n|p')$ be the path given by the concatenation of node $n$ and path $p'$. Then $\svec{p} = \svec{n} \mo \svec{p'}$.
%\end{lemma}
%
%
%Now we can show that function $s(n)$ computes exactly the sum of the DTFs for all the possible subpaths starting with $n$.
%\begin{theorem}
%\label{subpath_small_theorem}
%Let $n$ be a node of tree $T$. Then $s(n)=\displaystyle\sum_{p\in P(n)} \sqrt{\lambda^{|p|}}\svec{p}$.
%\end{theorem}
%\begin{proof}
%The theorem is proved by structural induction.
%
%\textbf{Basis.} Let $n$ be a terminal node. Then we have $P(n)=n$. Thus, by its definition, $s(n)=\sqrt{\lambda}\svec{n}=\displaystyle\sum_{p\in P(n)} \sqrt{\lambda^{|p|}}\svec{p}$.
%
%\textbf{Step.} Let $n$ be a node with children set $C(n)$. The inductive hypothesis is then $\forall c\in C(n). s(c)=\displaystyle\sum_{p\in P(c)} \sqrt{\lambda^{|p|}}\svec{p}$. Applying the inductive hypothesis, the definition of $s(n)$ and property \ref{ideal_operation}.\ref{p_dist}, we have
% 
%\begin{eqnarray*}
%s(n) &=&
%\sqrt{\lambda}\left(\svec{n} + \svec{n}\mo\sum_{c\in C(n)} s(c)\right) \\
%&=& \sqrt{\lambda}\left(\svec{n} + \svec{n}\mo\sum_{c\in C(n)} \sum_{p\in P(c)} \sqrt{\lambda^{|p|}}\svec{p}\right) \\
%&=& \sqrt{\lambda}\svec{n} + \sum_{c\in C(n)} \sum_{p\in P(c)} \sqrt{\lambda^{|p|+1}}\svec{n}\mo\svec{p} 
%\end{eqnarray*}
%Thus, by means of Lemmas \ref{subpath_lemma1} and \ref{subpath_lemma2}, we can conclude that $s(n) = \displaystyle\sum_{p\in P(n)} \sqrt{\lambda^{|p|}}\svec{p}$. 
%\end{proof}



\subsection{Evaluating the approximation of the dot product}
\label{sec:properties_of_the_vector_space}


\comment{[R2,C2]}{Section 3.2: the material in this section seems to offer an explanation of the fact that the performance 
of the DSTK and DRTK performance drops more than the DTK when lambda increases, and it is 
incomprehensible that the authors do not build the link (actually, this section would be better at the end 
of the paper). Why not add the omega weights in equation (5). Then instead of |S(T)| one would use the 
sum of the omegas, which drops much faster when lambda is small. Is there something I am missing 
here?}

\begin{table}
\begin{center}
\begin{tiny}
\begin{tabular}{r||r@{.}l|r@{.}l||r@{.}l|r@{.}l||r@{.}l|r@{.}l||r@{.}l|r@{.}l||r@{.}l|r@{.}l}
&\multicolumn{4}{c||}{\emph{h=0}}	&\multicolumn{4}{c||}{\emph{h=1}}	&\multicolumn{4}{c||}{\emph{h=2}}	&\multicolumn{4}{c||}{\emph{h=5}}	&\multicolumn{4}{c}{\emph{h=10}}\\
\hline
Dim. 512	&\multicolumn{2}{c|}{Avg}	&\multicolumn{2}{c||}{Var}	&\multicolumn{2}{c|}{Avg}	&\multicolumn{2}{c||}{Var}	&\multicolumn{2}{c|}{Avg}	&\multicolumn{2}{c||}{Var}	&\multicolumn{2}{c|}{Avg}	&\multicolumn{2}{c||}{Var}	&\multicolumn{2}{c|}{Avg}	&\multicolumn{2}{c}{Var}\\
\hline
\emph{k} = 20	&-0&0446	&0&7183	&0&9739	&1&0211	&2&0138	&0&5777	&4&9507	&0&6272	&9&8645	&1&0127\\
\emph{k} = 50	&-0&0545	&5&4629	&1&1306	&3&4711	&2&2941	&4&4938	&5&0171	&4&824	&9&6618	&4&2196\\
\emph{k} = 100	&-0&6819	&21&09	&0&9822	&16&6965	&1&8942	&20&1004	&5&2458	&16&3342	&10&2801	&20&3961\\
\emph{k} = 200	&-0&4052	&77&8553	&1&9928	&53&8576	&2&1421	&76&4839	&5&0398	&78&0001	&9&0137	&78&9879\\
\emph{k} = 500	&-1&5108	&417&1121	&2&6724	&491&0844	&5&0566	&489&0258	&7&3068	&481&6177	&10&6926	&360&5955\\
\hline
Dim. 1024	&\multicolumn{4}{c||}{}	&\multicolumn{4}{c||}{}	&\multicolumn{4}{c||}{}	&\multicolumn{4}{c||}{}	&\multicolumn{4}{c}{}\\
\hline
\emph{k} = 20	&0&0002	&0&49	&0&9851	&0&4446	&1&9378	&0&3432	&4&8732	&0&3592	&9&8996	&0&4976\\
\emph{k} = 50	&-0&2511	&2&3229	&0&9788	&2&4572	&1&9985	&1&8425	&5&1292	&2&2691	&10&0537	&2&1102\\
\emph{k} = 100	&-0&0327	&10&0111	&1&37	&8&2847	&2&5441	&8&9007	&5&5761	&7&9167	&9&8143	&8&529\\
\emph{k} = 200	&0&4292	&40&0213	&1&6357	&36&9718	&2&6673	&38&6285	&4&8888	&36&5791	&10&5477	&32&6902\\
\emph{k} = 500	&0&961	&240&6677	&-0&2899	&197&0764	&2&1809	&238&7662	&6&1283	&247&0507	&10&5271	&243&4888\\
\hline
Dim. 2048	&\multicolumn{4}{c||}{}	&\multicolumn{4}{c||}{}	&\multicolumn{4}{c||}{}	&\multicolumn{4}{c||}{}	&\multicolumn{4}{c}{}\\
\hline
\emph{k} = 20	&-0&0528	&0&2225	&0&9848	&0&1895	&1&943	&0&1712	&4&995	&0&2328	&9&9534	&0&2106\\
\emph{k} = 50	&-0&0289	&1&0154	&0&9423	&1&2872	&2&0609	&1&1389	&5&2488	&1&5417	&10&0115	&1&3548\\
\emph{k} = 100	&0&145	&4&5321	&1&1818	&4&25	&2&2062	&5&3105	&5&074	&4&1688	&10&1047	&4&7287\\
\emph{k} = 200	&0&3227	&20&7004	&1&7306	&21&3282	&1&0251	&20&681	&4&4617	&21&263	&10&3172	&25&5613\\
\emph{k} = 500	&0&8879	&107&0174	&2&5353	&111&5448	&0&3597	&136&8164	&5&7405	&145&3144	&9&8646	&133&716\\
\hline
Dim. 4096	&\multicolumn{4}{c||}{}	&\multicolumn{4}{c||}{}	&\multicolumn{4}{c||}{}	&\multicolumn{4}{c||}{}	&\multicolumn{4}{c}{}\\
\hline
\emph{k} = 20	&-0&0079	&0&0959	&1&0025	&0&0916	&2&0257	&0&068	&4&9981	&0&0884	&9&9922	&0&1027\\
\emph{k} = 50	&0&0441	&0&5899	&1&0628	&0&5569	&2&0566	&0&6462	&4&9818	&0&6404	&9&9896	&0&6424\\
\emph{k} = 100	&0&172	&2&2318	&1&0608	&1&6847	&2&0725	&2&5823	&5&0671	&2&506	&9&9597	&2&244\\
\emph{k} = 200	&-0&2724	&10&2017	&1&1402	&8&715	&2&2495	&10&1631	&5&3923	&7&8346	&10&4583	&9&428\\
\emph{k} = 500	&-0&4444	&66&6689	&1&3839	&83&4934	&1&8321	&73&682	&5&3868	&46&6318	&10&2459	&71&5771\\
\hline
Dim. 8192	&\multicolumn{4}{c||}{}	&\multicolumn{4}{c||}{}	&\multicolumn{4}{c||}{}	&\multicolumn{4}{c||}{}	&\multicolumn{4}{c}{}\\
\hline
\emph{k} = 20	&0&0181	&0&0559	&1&0237	&0&0411	&1&9857	&0&0441	&5&0078	&0&0524	&9&9892	&0&0529\\
\emph{k} = 50	&0&0165	&0&3067	&0&969	&0&304	&1&9878	&0&3611	&5&0606	&0&3586	&10&0249	&0&3336\\
\emph{k} = 100	&-0&0316	&1&2879	&0&8976	&1&074	&1&9485	&1&0715	&4&9804	&1&3136	&9&899	&1&0581\\
\emph{k} = 200	&0&0654	&3&4304	&1&1079	&4&4238	&1&6926	&4&9392	&4&7375	&4&1595	&10&299	&4&7273\\
\emph{k} = 500	&0&6881	&31&5398	&0&7341	&29&1581	&2&467	&23&417	&5&4105	&34&0299	&9&6346	&31&7414\\
\hline
\end{tabular}
\end{tiny}
\end{center}
\caption{Average and variance over 100 samples of the dot product between two sums of $k$ random vectors, with $h$ vectors in common, for several vector sizes.}
\label{prop_vettori_table}
\end{table}


The previous result shows that the embedding function $f$ can transform the orthonormal basis $\{\vec{\tau_1} ... \vec{\tau_m}\}$ of ${\mathbb R}^m$ in a nearly orthonormal basis $\{\svec{\tau_1} ... \svec{\tau_m}\}$ in ${\mathbb R}^d$  where $|\svec{\tau_i} \cdot \svec{\tau_j}|<\epsilon$. \comment{Actually, it shows that such an f exists. This sentence should be re-worded.}
We here want to empirically investigate whether this is sufficient to guarantee that the dot products in the reduced space approximate those in the original space, that is $\svec{T_1}\cdot\svec{T_2} \approx \vec{T_1}\cdot\vec{T_2}$. To better explain what we want to investigate, we assume the trivial weighting scheme that assigns $\omega_i=1$ if $\tau_i$ is a subtree of $T$ and $\omega_i=0$ otherwise. Distributed trees can be rewritten as:
\begin{equation}
\svec{T} = \sum_{\tau \in S(T)} \svec{\tau}
\label{initial_distributed_vectors}
\end{equation}
where $S(T)$ is the set of the subtrees of $T$, $\tau$ is a subtree, and $\svec{\tau}$ is its DTF vector.
Given Equation (\ref{eq:equivalence}) and properties \ref{near_norm} and \ref{near_orth} expected for the vectors $\svec{\tau}$, we can derive the following:
\begin{equation}
\label{eq:distorsion_of_the_dot_product}
\vec{T_1} \cdot \vec{T_2} - |S(T_1)||S(T_2)|\epsilon <\svec{T_1} \cdot \svec{T_2} < \vec{T_1} \cdot \vec{T_2} + |S(T_1)||S(T_2)|\epsilon
\end{equation}
where $|S(T)|$ is the cardinality of the set $S(T)$ and $|S(T_1)||S(T_2)|\epsilon$ is the introduced distortion.
We want to empirically determine whether this variability can be a real problem when applying this approach to real cases.  


%With this experiments, we tried to test the limits of the generic vector space we are considering, with respect to the objectives we are pursuing. 
To experiment with the above issue, we produced two sums of $k$ basic random vectors $\svec{v}$, with $h<k$ vectors in common. 
To ensure that these basic vectors are nearly orthonormal, their elements $\svec{v}_i$ are randomly drawn from a normal distribution $\mathcal{N}(0,1)$ and they are normalized so that $||\svec{v}|| = 1$ (as done in the demonstration of the Johnson-Lindenstrauss Lemma by \cite{JLLsimple_demonstration}).
The sums were then compared by means of their dot product. Ideally, the result is expected to be as close as possible to $h$. We repeated the experiment for several values of $k$ and $h$, and for different vector space sizes. The results in terms of average value and variance are shown in Tab. \ref{prop_vettori_table}.

Some observations follow:
\begin{itemize}
	\item for small values of $k$, the actual results are close, on average, to the expected ones, independently from the value of $h$ considered;
	\item for larger values of $k$, the noise grows and we register a sensible degradation of the results, especially with respect to their variance;
	\item using larger vector spaces scarcely affects the average values (that are already close to the expected ones) and it has a large positive impact on the variances.
\end{itemize}
These results suggest that the adopted approach has some structural limits in relation to its ability to scale up to highly complex structures, i.e. structures that can be decomposed into many substructures. To overcome the limits, we need to establish the correct dimension of the target vector space in relation with the expected number of active substructures and not only with respect to the size of the initial space of tree fragments.



%\begin{table}
%\begin{center}
%\begin{tabular}{l|cc|cc|cc|}
%Algorithm & \multicolumn{2}{|c|}{\emph{Tree preparation}} & \multicolumn{2}{|c|}{Learning} & \multicolumn{2}{|c|}{Classification} \\ 
% & \emph{Time} & \emph{Space} & \emph{Time} & \emph{Space}  &\emph{Time} & \emph{Space}  \\
%\hline
%
%TK &  - &  - & $O(n^2)$ & $O(n^2)$  & $O(n^2)$  & $O(n^2)$ \\
%FTK & - & - & $A(n)$  & $O(n^2)$  & $A(n)$  & $O(n^2)$  \\
%FTK-with-FS &  - & - & $A(n)$  & $O(n^2)$  & k & k \\
%ATK & - & - & $O(\frac{n^2}{q_\omega})$ & $O(n^2)$ & $O(\frac{n^2}{q_\omega})$ & $O(n^2)$ \\
%DTK & $O(n)$ & d & d & d & d & d \\
%\hline
%\end{tabular}
%\end{center}
%\caption{Computational time and space complexities for several tree kernel techniques: $n$ is the tree dimension, $q_\omega$ is a speed-up factor,  $k$ is the size of the selected feature set, $d$ is the dimension of space $R^d$, $O(\cdot)$ is the worst-case complexity, and $A(\cdot)$ is the average case complexity.}
%\label{tab_complex}
%\end{table}





The initial formulation of the parse tree kernel (TK) \cite{Collins2002} has worst-case complexity that is quadratic with respect to the nodes both in time and in space complexity. As worst-case complexity of TK is hard to improve, the biggest effort has been devoted to controlling the average execution time of TK algorithms. Three directions have mainly been explored. The first direction is the exploitation of some specific characteristics of trees. With the Fast Tree Kernels (FTK), \cite{Moschitti2006} demonstrated that the execution time of the original algorithm becomes linear on average for parse trees of natural language sentences. Yet, the tree kernel has still to be computed over the full underlying feature space and the space occupation is still quadratic.  The second explored direction is the reduction of the underlying feature space of tree fragments to control the execution time by approximating the kernel function. The feature selection is done in the learning phase. Then, for the 
classification, 
either the selection is directly encoded in the kernel computation by selecting subtrees headed by specific node labels, in the Approximate Tree Kernels (ATK) \cite{Rieck:2010:ATK:1756006.1756022}, or the smaller selected space is made explicit, in FTK with feature selection (FTK-with-FS) \cite{Pighin:Moschitti:conll:2010}. In these cases, the beneficial effect is only during the classification, and learning is overloaded with feature selection.
This discussion is summarized in Table \ref{tab_complex}.
A third direction, not reported in the table, exploits dynamic programming on the whole training and application sets of instances \cite{ICML2011Shin_503}. Kernel functions are reformulated to be computed using partial kernel computations done for other pairs of trees. As with any dynamic programming technique, this approach converts time complexity into space complexity.



\subsection{Theoretical Limits for Distributed Tree Kernels/should become a paragraph based on JLL-Transforms}
\label{sec:f}

\comment{[R3,C12]}{I would also suggest re-arranging the order of the sections and motivate better Section 3}

\comment{[R3,C13][IssueE][IssueF]}{Section 3 is unclear and incomplete. I am not even sure why it's needed, since the embedding proposed by the authors (diamond) is heuristic, and not the result of any random projection (apart from the choice of embeddings for labels, but their number is small in their application)}

\cite{HechtNielsen94} introduced the conjecture that there is a strict relation between the dimensions of the space and the number of nearly-orthonormal vectors that it can host. This is exactly what we here need to demonstrate the existence of embedding function $f$ and to describe the limits of the approach. But, the proof should have appeared in a later paper that, to the best of our knowledge, has never been published. A previous result \cite{Kainen_Kurkova_1993} gives some theoretical lower-bounds on the number of nearly orthonormal vectors but these lower-bounds are not satisfactory as big vector spaces are needed  to be sure to  cover the nearly orthonormal vectors needed for the distributed tree kernels.

This section wants to investigate the theoretical and practical limits of the approach of using low dimension spaces to embed larger spaces. Section \ref{sec:limits_f} reports a novel result based on the \cite{JLL} lemma that is useful to demonstrate the existence of the embedding function $f$  applied to orthonormal bases and allows to determine its theoretical limits. Section \ref{sec:properties_of_the_vector_space} instead empirically investigates whether this funtion $f$ can guarantee that the dot product between distributed trees approximately preserve the original dot products (see equation \ref{basic_dtk_tk_equivalence}).



\comment{[R3,C8][IssueE][IssueF]}{In section 3, the authors basically try to provide a Johnson-Lindenstrauss type of result for m points in a Euclidean space. However, unlike the general setting of the JL-lemma where $n$ arbitrary points in a $d$ dimensional space are embedded in a lower dimensional place, the m points are known in this section. Basically these points are the vectors of the canonical basis of $R^m$. Lemma 2 is ambiguous. The statement that $\delta_{ij}$ is approximately 0 with high-probability is incomplete since the actual bounds are not provided, and would at least require an extra union bound. If I understand correctly, the authors assume in their numerical explanation that $\delta_ij$ *is* actually 0, wherease the interplay between the constant delta, d, m and epsilon is actually important. I am not sure what to do with Table 1 \& 2 given that important gap in the theoretical explanation.}

\comment{[R3,C9][IssueA][IssueE][IssueF]}{I am not sure the Johnson-Lindenstrauss part of the paper should come first. For the authors, this might be the best order, but for a reader introduced to these ideas for the first time, this discussion (which is mostly about intuitions and not really about any formal result) might be more interesting after Section 4.}

The function $f: {\mathbb R}^m \rightarrow {\mathbb R}^d$ applied to the vectors of the orthonormal basis $\vec{\tau}$ should produce vectors $\svec{\tau}$ satisfying Property \ref{near_norm} and Property \ref{near_orth}.
%for each pair $\vec{\tau}_i, \vec{\tau}_j \in E$, $|f(\vec{\tau}_i) \cdot f(\vec{\tau}_j)| < \epsilon$ with a high probability for each $i \neq j$, where $0<\epsilon<1$.
This function should guarantee that vectors $\svec{\tau}$ of the transformed basis are nearly orthogonal. It is also useful to know the relation between the approximation $\epsilon$, the dimension $d$ of the target low dimension space, and the dimension $m$ of the original high dimension space of the tree fragments.

The existence of such an $f$ and the description of the relations between $\epsilon$, $d$, and $m$ are formalized in Lemma \ref{existence_of_f}. To prove the lemma, we use the Johnson-Lindenstrauss Lemma \cite{JLL}. We report here the theorem as described in \cite{JLLsimple_demonstration} and we adapt it to our notation:

\begin{theorem} (Johnson-Lindenstrauss Lemma) For any $0 < \epsilon < 1$ and any integer $m$. Let $d$ be a positive integer such that
\begin{center}
$d \geq 4 (\epsilon^2/2 - \epsilon^3/3)^{-1} \ln m$.
\end{center}
Then for any set $V$ of $m$ points in ${\mathbb R}^k$, there is a map $\overline{f} : {\mathbb R}^k \rightarrow {\mathbb R}^d$ such that for all $\vec{u}, \vec{v} \in V$, 
\begin{center}
$(1- \epsilon) ||\vec{u}-\vec{v}||^2 \leq ||\overline{f}(\vec{u})-\overline{f}(\vec{v})||^2 \leq  (1 + \epsilon) ||\vec{u}-\vec{v}||^2 $.
\end{center}
\end{theorem}

We can use this theorem to show that it is possible to project the orthonormal basis $E$ of the space ${\mathbb R}^m$ in the reduced space ${\mathbb R}^d$, and obtain properties \ref{near_norm} and \ref{near_orth} with high probability. To do this, we start by observing that mapped vectors $\overline{f}(\vec{\tau})$ can be assumed to be approximately unitary, i.e. $||\overline{f}(\vec{\tau})||^2\approx 1$, with high probability. This is because \cite{JLLsimple_demonstration} showed that the mapping can take the form of $f(\vec{\tau})=\sqrt{\frac{m}{d}}\vec{\tau'}$, where $\vec{\tau'}$ is the projection of $\vec{\tau}$ on a random $d$-dimensional subspace of ${\mathbb R}^m$. Moreover, they showed that the expected norm of $\vec{\tau'}$, other than having expected length $\mu = d/m$, is also fairly tightly concentrated around $\mu$. 
Specifically:
\begin{equation*}
\Pr\left[\frac{d}{m}-\Delta_-\leq ||\vec{\tau'}||^2 \leq \frac{d}{m}+\Delta_+\right] \geq 1 - e^{\frac{d}{2}(\ln(1-\Delta_-)+\Delta_-)} - e^{\frac{d}{2}(\ln(1-\Delta_+)-\Delta_+)}
\end{equation*}
thus:
\begin{equation*}
\Pr[1-\delta_-\leq ||\overline{f}(\vec{\tau})||^2 \leq 1+\delta_+] \geq 1 - e^{\frac{d}{2}(\ln(1-\frac{d}{m}\delta_-)+\frac{d}{m}\delta_-)} - e^{\frac{d}{2}(\ln(1-\frac{d}{m}\delta_+)-\frac{d}{m}\delta_+)}
\end{equation*}
where $\delta_-=\frac{m}{d}\Delta_-$ and $\delta_+=\frac{m}{d}\Delta_+$. So, if we express $||\overline{f}(\vec{\tau})||^2$ as $1+\delta$, we obtain:
\begin{equation}
\label{jll_delta}
\Pr[-\delta_-\leq \delta \leq \delta_+] \geq 1 - e^{\frac{d}{2}(\ln(1-\frac{d}{m}\delta_-)+\frac{d}{m}\delta_-)} - e^{\frac{d}{2}(\ln(1-\frac{d}{m}\delta_+)-\frac{d}{m}\delta_+)}
\end{equation}
i.e. the difference between the length of a mapped vector $\overline{f}(\vec{\tau})$ and $1$ is statistically very small.
Now we can demonstrate that the following lemma holds:\comment{R1,C2}{The statement of Lemma 2 is not very clear perhaps it could be re-written in the form $Pr[f(t_i)f(t_j) < 2e]>1-d$. That would be clearer.}\comment{[R2,C6]}{Section 3.1: Equation (\ref{jll_delta}) is still very far from establishing property 1. It only suggest that the difference 
between the form of f(tau) and 1 is small, without giving any quantitative insight. Actually, unless there 
is a typo in the second exponential term (I did not check the derivation), this difference may actually be 
quite large. Please correct and explain better!
The problematic term on Eq.(\ref{jll_delta}) is: $ln(1-d/m delta_+) - d/m delta_+$
For this to converge to zero when $d/m delta_+$ tends to zero, one would need:  $ln(1-d/m delta_+) + d/m 
delta_+$}
\begin{lemma}
\label{existence_of_f}
For any $0 < \epsilon < 1$ and any integer $m$. Let $d$ be a positive integer such that
\begin{center}
$d \geq 4 (\epsilon^2/2 - \epsilon^3/3)^{-1} \ln m$.
\end{center}
Then given the standard basis $E$ of ${\mathbb R}^m$, there is a map $f : {\mathbb R}^m \rightarrow {\mathbb R}^d$ such that for all $\vec{\tau}_i, \vec{\tau}_j \in E$, 
\begin{center}
$|f(\vec{\tau}_i)f(\vec{\tau}_j) - \delta_{ij}|<\epsilon$
\end{center}
\end{lemma}
where $\delta_{ij} \approx 0$ with high probability.

\begin{proof}
First, we can observe that $||\vec{\tau}_i-\vec{\tau}_j||^2 = ||\vec{\tau}_i||^2 + ||\vec{\tau}_j||^2 - 2 \vec{\tau}_i\vec{\tau}_j$ = 2 as $\vec{\tau}_i$ and $\vec{\tau}_j$ are unitary and orthogonal. Then, we can see that $||f(\vec{\tau}_i)-f(\vec{\tau}_j)||^2 = ||f(\vec{\tau}_i)||^2 + ||f(\vec{\tau}_j)||^2 - 2 f(\vec{\tau}_i)f(\vec{\tau}_j)$ = $2 (1 + \delta - f(\vec{\tau}_i)f(\vec{\tau}_j))$, assuming $||f(\vec{\tau}_i)||^2 = 1 +\delta_i$, $||f(\vec{\tau}_j)||^2 = 1 +\delta_j$ and $\delta_{ij} = \frac{\delta_i+\delta_j}{2}$. 

Then, being $V$ as the set of the orthonormal basis $E$, $k=d$, and $\overline{f}=f$, the inequality of the Johnson-Lindenstrauss Lemma becomes:
$(1- \epsilon) \leq (1 +\delta_{ij} - f(\vec{\tau}_i)f(\vec{\tau}_j)) \leq  (1 + \epsilon)$
This can be reduced to:
$-\epsilon < f(\vec{\tau}_i)f(\vec{\tau}_j) - \delta_{ij} < \epsilon$. By Eq. \ref{jll_delta} and the definition of $\delta_{ij}$, we can conclude that $\delta_{ij} \approx 0$ with a high probability. 
\end{proof}


\begin{table}
\begin{center}

\begin{tabular}{ccc}
\begin{tabular}{clr}
$\epsilon$& \emph{d}	& \emph{m}\\
\hline
0.05 
%& 1000	&	$125$\\
& 1500	&	$1408$\\
& 2000	&	$15782$\\
& 2500	&	$176898$\\
& 3000	&	$1982759$\\
& 3500	&	$22223629$\\
& 4000	&	$249092116$\\
& 4500	&	$2791932932$\\
& 5000	&	$31293200256$\\
& 5500	&	$3.50748\cdot 10^{11}$\\
& 6000	&	$3.93133\cdot 10^{12}$\\
%& 6500	&	$4.40641\cdot 10^{13}$\\
\hline
\end{tabular}
&
\hspace{5mm}
&
\begin{tabular}{clr}
\emph{d} & $\epsilon$	& \emph{m}\\
\hline
2500	&	0.01	&	$1$\\
	&	0.02	&	$7$\\
	&	0.03	&	$82$\\
	&	0.04	&	$2408$\\
	&	0.05	&	$176898$\\
	&	0.06	&	$31960138$\\
	&	0.07	&	$13921032552$\\
	&	0.08	&	$1.43294\cdot 10^{13}$\\
	&	0.09	&	$3.41657\cdot 10^{16}$\\
	&	0.1	&	$1.84959\cdot 10^{20}$\\
\hline
\end{tabular}
\\
(a) & & (b)\\
\end{tabular}
\end{center}
\caption{Relation between $d$ and $n$, and $\epsilon=0.05$ with respect to the result of the lemma. Table (a) has a fixed $\epsilon = 0.05$. Table (b) has a fixed $d=2500$.}
\label{d_k_e_table}
\end{table}


This result is also important as it gives a relation, though approximated, between $\epsilon$, $d$, and $m$, that is, to encode the orthonormal basis of ${\mathbb R}^m$ with an approximation $\epsilon$, we need a space ${\mathbb R}^d$ such that\comment{R1, C3}{"we need a space ... such that" The Lemma seems to say that this is sufficient not that it is necessary.}:
\begin{displaymath}
d \geq 4 (\epsilon^2/2 - \epsilon^3/3)^{-1} \ln m
\end{displaymath}
Table \ref{d_k_e_table}.(a) shows the relation between the $d$ and $m$ with a fixed $\epsilon = 0.05$. Table \ref{d_k_e_table}.(b) shows the relation between $\epsilon$ and $m$ with a fixed $d=2500$. The tables show that the space ${\mathbb R}^m$ that can be encoded in a smaller space ${\mathbb R}^d$ rapidly grows with $d$ (for a fixed $\epsilon$). Whereas, if we want to fix $d=2500$, the dimension of ${\mathbb R}^m$ can rapidly grow reducing the expectations on $\epsilon$. This result shows that the strategy of encoding large tree fragment feature spaces in much smaller spaces is possible.






\section{Kernels and Distributed Kernels}


Specific notations:\\
\begin{itemize}
\item $ch(n,j)$ is the subtree of $t$ rooted in the $j$-th child of node~$n$. $ch(n,j)=\emptyset$ if $n$ has less than $j$ children.
\item $C(n)$ is the set of the direct children of $n$.  
\item $N(t)$ is the set of nodes of a tree $t$.
\item $R_N(t) =\{ \substr{c} = (c_0,c_1,\ldots,c_k)| c_0 \in N(t) \text{ and, with }i>0, c_i=ch(c_0,i)\}$. Note that the numbering of subparts starts from $0$ as $c_0$ represents the root of the subtree $c$ of $t$.
$\substr{c}$ starts 
\item $r(t)$ is the root of a tree $t$. Note that $\delta(r(\emptyset),x)=0$ for any $x$.
\end{itemize}

\begin{tabular}{l}
\hline
Tree Kernels\\
%$\input{kernels/tk_1}$ \\
%$\input{kernels/tk_2}$  \\
%$\input{kernels/tk_3}$  \\
%\hline
$\input{kernels/dt_1}$  \\
$\input{kernels/dtk_2}$  \\
\hline
\hline
Subpath Tree Kernels\\
%$\input{kernels/stk_1}$\\
%$\input{kernels/stk_2}$\\
%$\input{kernels/stk_3}$\\
%\hline
$\input{kernels/dst_1}$\\
$\input{kernels/dst_2}$\\
\hline
\hline
Route Tree Kernels\\
%& $\input{kernels/rtk_2}$\\
%& $\input{kernels/rtk_3}$\\
\hline
$\input{kernels/drt_1}$\\
$\input{kernels/drt_2}$\\
\hline
String Kernels\\
%$\input{kernels/sk_1}$\\
%%$\input{kernels/sk_2}$\\
%\hline
%$\input{kernels/sk_3}$\\
%\hline
$\input{kernels/ds_1}$\\
$\input{kernels/ds_2}$\\
\hline
\hline
Partial Tree Kernels\\
%$\input{kernels/ptk_1}$\\
%$\input{kernels/ptk_2}$\\
%$\input{kernels/ptk_3}$\\
%\hline
$\input{kernels/dpt_1}$\\
$\input{kernels/dpt_2}$\\
\hline
\end{tabular}






%What about SVM in the primal space \cite{DBLP:journals/jmlr/KeerthiCD06}? As both DTK+SVM and PSVM are both approximations of TK-SVM; as the two approximations depends on $s$ and $d_{max}$, it is difficult to determine whether a method is better than another in term of complexities: $O(n d_{max}^2 |N|^2)$  vs. $O(n |N| s \log s)$. This depends on the relation between $s \log s$ and $d_{max}^2|N|$. If $d_{max}^2$ is comparable with $s$ (and this is the case in the experiments in  \cite{DBLP:journals/jmlr/KeerthiCD06}), then $|N|$ is in generally bigger that $\log s$.  




\section{Tree Kernels seen as Convolution Kernels on Countable Sets}
\label{sec:CDTK}


Parse tree kernels \cite{DBLP:conf/nips/CollinsD01,Collins2002} can be defined with a recursive function that computes the similarity between two trees $x$ and $y$ as follows:\\
\begin{equation}
TK(x,y) = \sum_{n_x \in N(x), n_y \in N(y)} \Delta(n_x,n_y)
\label{equation:tk}
\end{equation}
where $N(t)$ is the set of nodes of the tree $t$. The recursive function $\Delta(n_x,n_y)$ is the core of the kernel function and of the computation algorithm:
\begin{displaymath}
\input{kernels/tk_2}
%\Delta(n_x,n_y) =
%\begin{cases} 
%		\lambda \mbox{     if } n_x \mbox{ and } n_y \mbox{ are }  \mbox{     two terminal nodes and } n_x = n_y \\ 
%		\lambda\prod_j (1 + \Delta(ch(n_x,j),ch(n_y,j)) \mbox{     if } n_x \mbox{ and } n_y \mbox{ head the same production} \\
%		0 \mbox{     otherwise}
%\end{cases}
\end{displaymath}
where $ch(n_t,j)$ is, as already defined, the subtree of $t$ rooted in the $j$-th child of the node~$n_t$. 


%\section*{\comment{TDa spostare}}
%\textbf{Example on the Tree Kernel \comment{Questo dovrebbe essere messo nella sezione del tree kernel}}
%
%
%The classical formulation \cite{Collins2002}:
%\begin{displaymath}
%TK(T_1,T_2) = \sum_{n_1 \in N(T_1), n_2 \in N(T_2)} \Delta(n_1,n_2)
%\end{displaymath}
%where 
%\begin{displaymath}
%\Delta(n_1,n_2) =
%\begin{cases} 
%		\lambda \mbox{     if } n_1 \mbox{ and } n_2 \mbox{ are }  \mbox{     two terminal nodes and } n_1 = n_2 \\ 
%		\lambda\prod_j (1 + \Delta(ch(n_1,j),ch(n_2,j)) \mbox{     if } n_1 \mbox{ and } n_2 \mbox{ head the same production} \\
%		0 \mbox{     otherwise}
%\end{cases}
%\end{displaymath}

Equation (\ref{equation:tk}), along with the $\Delta(n_x,n_y)$ function, is a realization of the convolution kernel~(Definition \ref{convolution_kernel_def}). $\Delta(n_x,n_y)$ can be rewritten as:
\begin{displaymath}
\input{kernels/tk_3}
\end{displaymath}
where $r(t)$ is the root of a tree $t$,  $k=max(|C(n_1)|,|C(n_2)|)$. $C(n)$ is the set of the direct children of $n$. Note that $ch(n,j)=\emptyset$ if $n$ has less than $j$ children. This is the prelude for writing $K_1(\substr{a},\substr{b})$ as \comment{Perch\'e k+1??}:
\begin{displaymath}
K_1(\substr{a},\substr{b}) =\lambda \delta(a_1,b_1) \displaystyle\prod_{j=2}^{k+1}K_2(a_j,b_j)( 1 + K_1(a_j,b_j)) \end{displaymath}
where $\substr{a}$ and $\substr{b}$ are the decompositions into parts of two trees $a$ and $b$ and $K_2(\substr{a},\substr{b}) = \delta(a_1,b_1)$.
Then:\\
\begin{displaymath}
TK(x,y) = \sum_{\substr{a} \in R(x),\substr{b} \in R(y)} \lambda \delta(a_1,b_1) \displaystyle\prod_{j=2}^{k+1}K_2(a_j,b_j)( 1 + K_1(a_j,b_j))
\end{displaymath}
by defining $R(t)$ for a tree $t$ as:
\begin{displaymath}
R(t)= \{(n,ch(n,1), \ldots, ch(n,k))|n\in N(t) \text{ and }k=|C(n)|\}
\end{displaymath}

%Let us define the following. Given a tree $T$ where $N(T)$ is the set of its nodes, a decomposition into parts is $(n,sub(ch(n,1)), \ldots sub(ch(n,D_n)))$ where $n \in N(T)$ is a node of the tree, $ch(n,j)$ is the $j$-th child of the node $n$, and $sub(x)$ is the complete subtree of $T$ headed by the node $x$. We can define:
%and
%NON SERVE\begin{displaymath}
%R_1(T) = \{n, (n,sub(ch(n,1)), \ldots sub(ch(n,D_n)))| n \text{ is the head of } T\}
%\end{displaymath}
%
%%\begin{figure}
%%\begin{center}
%%\parsetree{(.A. (.B. .W1.) (.C. (.D. .W2.)(.E. .W3.)))}
%%\end{center}
%%\caption{A sample tree T}
%%\label{sampletree}
%%\end{figure}
%For example, given the tree $T$ in Fig. \ref{sampletree}, we have:
%\begin{displaymath}
%R(T) = \{ (A,sub(B),sub(C)), (B,sub(W1)), (C,sub(D),sub(E)), (D,sub(W2)), (E,sub(W3)) \}  
%\end{displaymath}
%\begin{displaymath}
%R_1(T) = \{A, (A,sub(B),sub(C))\}  
%\end{displaymath}
%Now, we can observe the tree kernel with respect to the general formulation of the convolution kernel by defining:
%\begin{displaymath}
%K_1(n_1,n_2) = \lambda \delta(n_1,n_2)
%\end{displaymath}
%and:
%\begin{displaymath}
%K_2(t_1,t_2) = K_1(n_1,n_2)( 1 + K_3(t_1,t_2))
%\end{displaymath}
%where $t_1 = (n_1,sub(ch(n_1,1)),\ldots sub(ch(n_1,M)))$ and $t_2 = (n_2,sub(ch(n_2,1)),\ldots sub(ch(n_2,M)))$.
%\begin{displaymath}
%K_3(t_1,t_2) = K_1(n_1,n_2) \prod_{i=1}^M K_2(sub(ch(n_1,i)),sub(ch(n_2,i)))
%\end{displaymath}







\newpage
\section{The idea in two pages or less}
\label{sec:preliminaries}

%IO METTEREI QUI UNA SEZIONE SULLA NOTAZIONE E DEFINIZIONI BASE
% STRUCTURE
% SUBSTRUCTURE
% RELEVANT SUBSTRUCTURE
% COUNTABLE SET

%COMMENT: TENTATIVO DI DEFINIZIONI, MOLTO FRAMMENTATO, POI SI VEDE DOVE METTERLO
%
%An object $x \in X$ (for example a tree) can be seen in two different ways: as a single object or as an object itself composed of subobjects (of the same kind): when we consider $x$ as an atomic object we will think of $X$ as a space endowed with the discrete metric (also called the kronecker's delta) that is:
%$$
%\delta(x,y) = 
%\begin{cases}
%1 &\text{if } x = y \\
%0 &\text{if } x \neq y
%\end{cases}
%$$
%in other words, we are only interested if two objects are equal or not. 
%On the other hand, when we want to compute a kernel between two objects we are interested in their substructures and so the kernel will define a different, more refined metric on $X$ which is allowed to take values other than zero or one.
%
%IDEA: in the first case we will refer to elements of $X$ simply as \emph{objects} or \emph{structures} while in the second case we will use the terms \emph{convolutional objects} or \emph{convolutional structures}.
%
%---
%
%We will assume throughout that $X$ is a discrete space, we will not consider the case where on the atomic object themselves is defined a metric that is not the discrete metric.
%
%---

%It is important to notice that a structured object can be interpreted in several different ways, that is, we are free to decide what are the substructures that we are interested in. Defining a kernel $K$ amounts then to decide which substructures are \emph{relevant} for $K$. [METTERE UN PAIO DI ESEMPI, PURE CON IMMAGINI CASOMAI]
%
%---
%
%Every kernel $K$ on a discrete space can be then written as a weighted summation of Kronecker's delta between relevant structures for $K$:
%\begin{equation}
%K(x,y) = \dotprod{\vec{x}}{\vec{y}} = \sum_{\substack{a\in S(x) \\ b \in S(y)}} \omega_{a}\omega_{b} \delta(a,b) 
%\label{K_as_deltas}
%\end{equation}
%
%---

%Before going into definitions, lemmas and theorems, this section introduces the main notation, the main concepts and the main result of the paper.  

%How structured data are defined? How these are used in learning machines based on kernel functions? How our distributed representations change the previous perspective?


%To explain the results of this paper, this section introduces the idea of embedding trees into small vectors, clarifies the used notation, and, finally, poses the challenges that need to be solved to demonstrate the theoretical soundness and the applicability of the approach. 





%\subsection{Structured Objects and Convolution Kernels}



%\meta{Traditional Approach: Convolution kernels and Kernel Machines}




\LD{A spark to accomplish both goals
% in a single model 
is hidden in one of current family of strategies for learning with structures: convolution kernels \cite{Haussler99convolutionkernels} coupled with kernel machines (see \cite{cristianiniSVM}).} The widely-accepted power of convolution kernels  $K(x,y)$ is that they algorithmically compute similarity between two structures $x$ and $y$ without an explicit description of feature spaces. However, it is known that for any convolution kernel $K\colon X \times X\rightarrow \R$ there exist a corresponding Hilbert space $\mathcal H$ and function $f\colon X\rightarrow \mathcal{H}$ such that
%:
%$$
$K(x,y) = \dotprod{f(x)}{f(y)}$  \cite{Haussler99convolutionkernels}.
%$$
%the kernel function 
If $X$ is a countable set, each basis vector $\vec{a}$ of $\mathcal H$ represents a structure $a$ that is relevant for the specific kernel, for example, subtrees in tree kernels \cite{Collins2002}.

%\arco{a}{b}
\LD{We showed that convolution kernels $K(x,y)$ restricted on discrete sets\footnote{Sets endowed with the Kronecker's delta function} (Definition \ref{convolution_kernel_def} in Section \ref{sec:convolution_kernel_def}) can be rewritten as weighted sums of Kroneker's delta functions $\delta(a,b)$ over substructures.}
%applied over substructures.
More formally, we proved that
(see Lemma \ref{k_in_deltas} in Section \ref{sec:convolution_kernel_def}):
\begin{equation}
K(x,y) = \dotprod{\vec{x}}{\vec{y}} = \displaystyle\sum_{a\in S(x), b \in S(y)} \omega_{a}\omega_{b} \delta(a,b) 
\label{K_as_deltas}
\end{equation}
\finalsecondcomment{R3.4}{"$w_a$ and $w_b$ are the weights of substructures a and b." what is a weight mathematically speaking? if $w_a$ is a weight, what part of a total mass does $w_a$ represent?}
where $S(x)$ and $S(y)$  are discrete sets of all substructures of $x$ and $y$ that are relevant for the given kernel, $\omega_{a}=\dotprod{\vec{a}}{\vec{x}}$ and $\omega_{b}=\dotprod{\vec{b}}{\vec{y}}$ are the values (or  weights) of structures $a$ and $b$ (again, the definition of such weights depend on the kernel in use) and
$\delta(a,b)=\dotprod{\vec{a}}{\vec{b}}$ is the Kroneker's delta function which is one if $a = b$ and zero otherwise.
%The $\delta(a,b)$ function is defined on the discrete, possibly infinite, set $S = \bigcup_{z} S(z)$ that is the union of sets $S(z)$ for all the considered structured objects $z$.




%A relevant substructure $a$ is then a feature for structured objects $x$. The value of the feature $a$ is $\omega_{a} = \vec{a}\cdot\vec{x}$, that is the projection of the vector $\vec{x}$ on the basis vector $\vec{a}$ representing the substructure.

%The main result of \cite{Haussler99convolutionkernels} is that convolution kernels are valid kernels and, thus,  \mysecondinsert{}.\finalsecondcomment{R3.3}{"Each basis vector in H represents a relevant substructure for the convolution kernel." I do not understand this statement. Substructures are discrete objects, RKHS are (possibly infinite) vector spaces with a dot-product. Where is the map that clearly and unambiguously associates a substructure to a basis in the RKHS?}
\finalcomment{At the bottom of p.5, the authors mention that "for any convolution Kernel K(x,y) on countable sets, there is a corresponding feature space $R^m$ where the kernel function is expressed [...]". Since countable also includes countable + infinite, I think it's false to state that there exists such a thing. For instance, consider the set of integers and define K(a,b)= delta(a=b). There's no finite feature space that can embed this kernel exactly. So I think the whole argument at the bottom of p.5, as well as the top of p.6, is false ("the function $\delta(a,b)$ realizes the dot product"). Unless I have misunderstood the intention of the authors, I think this is something of a blow to the authors' claims.}{R3.3}


%\meta{Novel approach: Distributed Convolution Structures and Linear Machines}

\finalcomment{At the top of p.5, $\delta(x,y)$ is "Kronecker's delta function". At the top of p.6, "The function $\delta(a, b)$ realizes the dot product between the two vectors of the orthonormal basis of Rm representing substructures a and b in the feature space of the substructures." Which one is it? a dot-product or a binary valued function?}{R3.4}

 \comment{[R2,C2]}{Section 3.2: the material in this section seems to offer an explanation of the fact that the performance 
of the DSTK and DRTK performance drops more than the DTK when lambda increases, and it is 
incomprehensible that the authors do not build the link (actually, this section would be better at the end 
of the paper). Why not add the omega weights in equation (5). Then instead of |S(T)| one would use the 
sum of the omegas, which drops much faster when lambda is small. Is there something I am missing 
here?}

\LD{The above result enlightened our idea to approximate first the Kronener's delta function  $\delta(a, b)$ and then the full convolution kernels $K(x,y)$ by mapping substructures and structures in low dimensional vectors.}
We defined two families of  mapping functions -- the \textbf{distributed substructures} and the \textbf{distributed convolution structures} --  for the two approximations, respectively. These two families of functions stem from the recently revitalized tradition of distributed representations \cite{Hinton:1986,McClelland:Rumelhart:1986,Plate1995}, the possibility of embedding large spaces in smaller spaces \cite{JLL} and our previous results on distributed tree kernels \cite{Zanzotto2012193}.

%The dot product between distributed substructures and between distributed convolution structures approximate the Kronener's delta function and the selected convolution kernel, respectively.

\LD{Thus, the Kroneker's delta function $\delta(a, b)$  between substructures was approximated by the dot product between \emph{distributed substructures}.} Each \emph{distributed substructure} $\df$ represents a single structure and was defined as recursive functions mapping structures in small vectors: 
\begin{displaymath}
\df\colon X \rightarrow \R^d
\end{displaymath}
where $d$ is extremely smaller than the size of the potentially infinite-dimensional Hilbert space $\mathcal H$ of structures. We showed that the dot product between distributed substructures approximates the Kroneker's delta function between the two related structures with a high probability (see Sections \ref{sec:nov} and \ref{sec:ideal} and Lemma \ref{simpler_lemma} in Section \ref{sec:compositional_definition}), that is:\\
\begin{equation}
%\df(a) \cdot \df(b) - \epsilon <\delta(a,b)<\df(a) \cdot \df(b) + \epsilon
\P(\delta(a,b) - \varepsilon \leq \dotprod{\df(a)}{\df(b)} \leq \delta(a,b) + \varepsilon) \geq 1- \theta
\label{eq:approx_delta_1}
\end{equation}
where $\varepsilon<1$ and $\theta<<1$ are two parameters for the approximation.

\LD{Consequently, it was easy to show that convolution kernels $K(x,y)$ between structures are approximated with dot products between sums of the distributed substructures related to $x$ and $y$.} The approximation has the following probability (see Lemma \ref{first_formulation} in Section \ref{sec:first_formulation}):
\begin{displaymath}
\P(K(x,y) - \varepsilon \J(x,y) <\dotprod{\displaystyle\sum_{a \in S(x)} \omega_a\df(a)}{\displaystyle\sum_{b \in S(y)} \omega_b\df(b)} <K(x,y) + \varepsilon \J(x,y)) > 1-|S(x)||S(y)|\theta
\end{displaymath}
where
%\begin{displaymath}
$\J(x,y)=\displaystyle\sum_{a \in S(x), b \in S(y)} \omega_a\omega_b$.
%\end{displaymath}
Unfortunately, this first solution is impractical. It is not trivial to directly build the sums %$$\sum_{a \in S(z)} \omega_a\df(a)}$$ 
as $S(x)$ and $S(y)$ may be extremely large. 


\LD{The last step before our main result is that, for each convolution kernel $K(x,y)$, it is possible to define a related \emph{distributed convolution structure} function that is a recursive function $D\colon X \rightarrow \R^d$ mapping structures into small vectors (see Definition \ref{d_definition} in Section \ref{sec:the_core_section}).}
%\begin{displaymath}
%D\colon X \rightarrow \R^d
%\end{displaymath}
We showed that these distributed convolution structures $D(x)$ compute the sum of the distributed substructures of structures $x$ (see Lemma \ref{d_theorem} in Section \ref{sec:the_core_section}):
\begin{equation}
D(z)= \displaystyle\sum_{c\in S(z)} \omega_{c} \df(c)
\label{D_z_result}
\end{equation}
%that relates distributed convolution structures with distributed substructures $\df$. Therefore, $D(z)$ is a small vector representing $z$.

\LD{Hence, our main finding is that, using the three above results (Equations \ref{K_as_deltas}, \ref{eq:approx_delta_1} and \ref{D_z_result}), we demonstrated that convolution kernels on discrete sets can be approximated with dot products between distributed convolution structures}. Theorem \ref{The_theorem_of_the_paper} states that:\\
\begin{equation}
%D(x) \cdot D(y)  - \epsilon J < K(x,y) < D(x) \cdot D(y)  + \epsilon J
\P\left( K(x,y) - \varepsilon \J(x,y) < \dotprod{D(x)}{D(y)} < K(x,y)  + \varepsilon \J(x,y)\right)> 1- |S(x)||S(y)|\theta
\label{the_core_property}
\end{equation}
%where: 
%$\varepsilon<1$ and $\theta_2=|S(x)||S(y)|\theta_1$ are two parameters for the approximation, and 
%two parameters $\epsilon < 1$ is the approximation constant of Equation \ref{eq:approx_delta_1} and 
%$\J$ is a value depending on $x$ and $y$:
\myinsert{Section \ref{sec:experiments} empirically investigates the usefulness of this approximation that depends on $\varepsilon$, $\J$ and $\theta$.}
\finalcomment{Equation 3: it would be appropriate to note that the usefulness of the approximation depends on epsilon. Epsilon will be measured experimentally in Section 8.}{R1.3}


\OCAR{Distributed convolution structures $D(x)$ are thus the key to merge the two incompatible goals.} {Low computational complexity and fully awareness of structural information can coexist as $D(x)$ are small vectors, approximating original convolution kernels,}{ that allow using linear versions of learning machines.} {This drastically reduces the complexity of learning and classification when applied to real kernels (see Section \ref{sec:complexity}).}


\textbf{approximates the reproducing function $K(x,\cdot)$ \cite{aronszajn50reproducing} underlying the convolution kernel $K$.}


%As discussed in the introduction, being able to approximate convolution kernels through dot products between small vectors in $\R^d$ allows us to use linear versions of kernel machines. This reduces the complexity of learning and classification (see Section \ref{sec:complexity}).
%The properties that we will establish on the distributed convolution structures to respect Equation \ref{eq:approx_delta} bound the approach to approximate convolution kernels on countable sets.




%Dot products  of distributed substructures approximate the Kroneker's delta functions $\delta(a,b)$ applied over substructures whre(2) a given convolution kernel $K(x,y)$. 
% 
%With the previous definitions, we can introduce the core of the idea derived from the distributed tree kernels \cite{Zanzotto2012193}. We define the \textbf{distributed convolution structures} as functions $D$ mapping structured objects into small vectors:
%\begin{displaymath}
%D: X \rightarrow \R^d
%\end{displaymath}
%where $d$\myremove{$<<m$} \myinsert{is extremely smaller than the size of the potentially infinite-dimensional Hilbert space $\mathcal H$ of the substructures}. 
%The function $D$, that is defined in line with the definition of convolution kernels (see Definition \ref{d_definition} in Section \ref{sec:the_core_section}), ends up having the following property (see Lemma \ref{d_theorem} in Section \ref{sec:the_core_section}), that is:
%\begin{displaymath}
%D(z)= \displaystyle\sum_{c\in S(z)} \omega_{c} \df(c)
%\end{displaymath}
%where $\df$ is a function that maps substructures \mysecondinsert{in $S$} into $\R^d$.  If we can demonstrate that:\\
%\begin{equation}
%%\df(a) \cdot \df(b) - \epsilon <\delta(a,b)<\df(a) \cdot \df(b) + \epsilon
%\delta(a,b) - \epsilon <\df(a) \cdot \df(b)<\delta(a,b) + \epsilon
%\label{eq:approx_delta}
%\end{equation}
%we can use the \textbf{distributed convolution structures} to approximate convolution kernels (see Theorem \ref{The_theorem_of_the_paper}):\\
%\begin{equation}
%%D(x) \cdot D(y)  - \epsilon J < K(x,y) < D(x) \cdot D(y)  + \epsilon J
% K(x,y) - \epsilon J < D(x) \cdot D(y) < K(x,y)  + \epsilon J
%\label{the_core_property}
%\end{equation}
%where $\epsilon < 1$ is an approximation constant and the factor $J$ is a value depending on $x$ and $y$:
%\begin{displaymath}
%J=\displaystyle\sum_{a \in S(x), b \in S(y)} \omega_a\omega_b
%\end{displaymath}
%\myinsert{Section \ref{sec:experiments} empirically investigates the usefulness of this approximation that depends on $\epsilon$ and $J$.}
%\finalcomment{Equation 3: it would be appropriate to note that the usefulness of the approximation depends on epsilon. Epsilon will be measured experimentally in Section 8.}{R1.3}

%The next two sections address the two following issues:  (1) is it possible to derive the vector in $\R^d$ representing $x$ using a \mysecondremove{compositional}\mysecondinsert{recursive}  definition of $\df(x)$ \mysecondinsert{such that Equation \ref{eq:approx_delta} holds}? (2) how can distributed convolution structures be defined so that the property expressed in Equation \ref{the_core_property} holds?
%\myinsert{The reader interested in actual distributed convolution kernels and in the experimental investigation can skip these two sections and directly go to Section \ref{sec:attk}.}





%\subsection{The Notation and the Idea}



\subsubsection{Nearly-Orthogonal Unit Vectors to Approximate Kronecker's Delta}
\label{sec:nov}

%\arco{a}{b}
\LD{The second step for our model was showing that Kronecker's delta $\delta(a,b)$ between substructures in $S$ can be approximated with dot products between \emph{distributed substructures} $\df_{\Sigma}(a),\df_{\Sigma}(b)$  that are nearly-orthogonal unit vectors in a low dimensional space  $\R^d$.}  This is a big question. Formally, the property required to \emph{distributed substructures} is:
\begin{equation}
%\df(a) \cdot \df(b) - \epsilon <\delta(a,b)<\df(a) \cdot \df(b) + \epsilon
\P(\delta(a,b) - \varepsilon \leq \dotprod{\df_{\Sigma}(a)}{\df_{\Sigma}(b)} \leq \delta(a,b) + \varepsilon) \geq 1- \theta
\label{eq:approx_delta_1}
\end{equation}
where $\varepsilon<1$ and $\theta<<1$ are two parameters for the approximation.
%Equation (\ref{eq:approx_delta_1}) holds and, consequently, Equation (\ref{the_core_property}) can be shown. 
This section shows that it is possible to find these these \emph{distributed substructures}.
% and, thus, as discussed in the next section, convolution kernels are approximated with dot products between \emph{distributed structures}. 
%Finally, it analyzes the limits of directly defining a bijective mapping function $\df_{\Sigma}$.   
As a direct result, Section \ref{sec:first_formulation} shows that convolution kernels $K(x,y)$ can be approximated with dot products of \emph{distributed structures} $D_{\Sigma}(x)$ and $D_{\Sigma}(y)$, which are sums of \emph{distributed substructures}.


\LD{Our answer to the big question stems from research in embedding high-dimensional spaces in low-dimensional spaces \cite{JLL}.} In fact, $\delta(a,b)$ between two substructures $a$ and $b$ in $S$ is the dot product $\dotprod{\vec{a}}{\vec{b}}$ between the releated basis vectors $\vec{a}$ and $\vec{b}$ of Hilbert spaces $\mathcal H$ underlying kernels. It is known that for any convolution kernel $K\colon X \times X\rightarrow \R$ there exist a corresponding Hilbert space $\mathcal H$ and function $f\colon X\rightarrow \mathcal{H}$ such that
%:
%$$
$K(x,y) = \dotprod{f(x)}{f(y)}$  \cite{Haussler99convolutionkernels}.
%$$
%the kernel function 
If $X$ is a countable set, each basis vector $\vec{a}$ of $\mathcal H$ represents a substructure $a$ that is relevant for the specific kernel, for example, subtrees in tree kernels \cite{Collins2002}.
Then, showing that $\delta(a,b)$ can be approximated means to demonstrate that it is possible to embed the Hilbert space $\mathcal H$ in a low-dimensional space $\R^d$ by mapping the orthonormal basis of $\mathcal H$ into a discrete set of nearly orthonormal vectors in $\R^d$. 
%Using basic results from research in embedding, this section answers to the question. 

%An answer to this question prove that 
%that the Kronecker's delta on the discrete set $S$ of the \mysecondremove{structured objects}\mysecondinsert{substructures} can be approximated (cf. equation (\ref{eq:approx_delta})) means to sho that  \mysecondinsert{the orthonormal basis of} 


\LD{Thus, the core part of the answer is showing the existence of sets $NOV(\varepsilon,\theta)$ of nearly-orthogonal unit vectors in $\R^d$ with a cardinality $m=|NOV(\varepsilon,\theta)|>>d$.} %Consequently, the function $\df$ maps substructures in $S$ to vectors in $NOV(\varepsilon,\theta)$. 
These sets should contain low-dimensional vectors such that, for each $\vec{a},\vec{b} \in {NOV(\varepsilon,\theta)}$, the dot product $\dotprod{\vec{a}}{\vec{b}}$ has the following property: 
\begin{equation}
\P(\delta(\vec{a},\vec{b}) - \varepsilon \leq \dotprod{\vec{a}}{\vec{b}} \leq \delta(\vec{a},\vec{b}) + \varepsilon) \geq 1- \theta
\label{eq:nov}
\end{equation}
These sets along with the definition of a bijective mapping function $\df_{\Sigma}: S \rightarrow NOV(\varepsilon,\theta)$ guarantee that Equation \ref{eq:approx_delta_1} holds.
% The orthonormal basis of a Hilbert space $\mathcal H$ of substructures is embedded in $\R^d$ 
%to vectors in $NOV(\varepsilon,\theta)$  
%since basis vectors in $\mathcal H$ represent substructures in $S$. 
%The ideal situation is that $NOV(\varepsilon,\theta)$ is a discrete set of vectors
 %$\smallvectors{S}$ of the same cardinality of $S$ 
%and $\df$ is a bijective mapping between the two sets. 



 

%, \mysecondremove{each substructure needs a nearly orthonormal vector in $\R^d$}\mysecondinsert{the mapping function is $\df: S \rightarrow \smallvectors{S}$}. \mysecondremove{This latter is true}\mysecondinsert{The set $\smallvectors{S}$ is a set of nearly orthonormal vectors and the function $\df$ embeds $\mathcal H$ in $\R^d$ } if these two properties hold for all the substructures \mysecondinsert{$a,b \in S$}\mysecondremove{and all the pairs of substructures $a$ and $b$}: \comment{R1, C1}{These properties seem sufficient but not necessary} 

\finalsecondcomment{R3.6}{the length of Footnote 2 is problematic. If it's that long, it should not be a footnote. Footnote 4 is not meaningful.}\finalcomment{Footnote 2: Should this issue be addressed in the body of the text or an appendix rather than a footnote.}{R1.5}
\LD{Knowing the potential cardinality $m$ of  $NOV(\varepsilon,\theta)$ in $\R^d$ is definitely crucial as sets $S$ are possibly infinite.} How large should be $d$ if we want to map at least $m$ structures in $S$ to different vectors in $NOV(\varepsilon,\theta)$? 
%affects the quality of the approximation of  $\delta(a,b)$ between substructures. 
%Thus, it is crucial to determine h
%How large $d$ should be such that $\R^d$ can host $m$ nearly orthogonal unit vectors?
There is a long-lasting conjecture that postulates a relation  between $d$ and $m$ for any given $\theta$ and $\varepsilon$ \cite{HechtNielsen94} but, to the best of our knowledge, a definitive demonstration does not still exist. By using the Johnson\&Lindestrauss Lemma \cite{JLL}, we derived an upper-bound for $d$. 
Sets $NOV(\varepsilon,\theta)$ can potentially host\footnote{The expression \emph{The set $NOV(\varepsilon,\theta)$ can potentially host ...} stands for the more formal \emph{There is a probability strictly greater than 0 that $NOV(\varepsilon,\theta)$ contains ...}} $m$ vectors with $\theta = 2/m^2 - 1/m^4$  if $d=O(\varepsilon^{-2}\log{m})$ (see Appendix \ref{sec:_demonstration_} for a proof). 
Thus, there is a exponential relation between $d$ and $m$. This is a positive result as spaces $\R^d$ can host large sets of $NOV(\varepsilon,\theta)$.
%Fortunately, sets $NOV(\varepsilon,\theta)$ are extremely large even for small $\theta$ and $d$. 
Thus, definitely many substructures in $S$ in real datasets can be represented with vectors in $NOV(\varepsilon,\theta)$.


%To the best of our knowledge, the only proof is an a previous result \cite{Kainen_Kurkova_1993} but the given theoretical lower-bounds on the number of nearly orthonormal vectors are not satisfactory in our case as high values of $d$ are needed  to be sure to partially cover the cardinality of $S$.

%%Clearly, sets $\mathcal S$ 
%%The space $\R^d$ is finite-dimensional space. Thus, the properties \ref{near_norm} and \ref{near_orth} cannot both hold for a possibly infinite discrete set $\smallvectors{S}$. Yet, these properties can hold for subsets $\smallvectors{S}'$ of $m$ vectors of $\smallvectors{S}\subset\R^d$.
%%, that is, for all $\svec{a}$ and $\svec{b}$ in $\smallvectors{S}'$, $P(1 - \epsilon < ||\svec{a}|| < 1 + \epsilon) > 1 - \theta$ and $P(|\svec{a} \cdot \svec{b}| < \epsilon) > 1 - \theta$. 
%An important issue is determining \mysecondinsert{}\mysecondremove{what is the dimension $d$ of $\R^d$ that guarantees the embedding of $m$ vectors of the orthonormal basis of the Hilbert space $\mathcal H$  with a approximation $\epsilon$ and with probability higher than 1-$\theta$}. This issue has been also referred to as the problem of determining \cite{HechtNielsen94} introduced the conjecture that there is a \mysecondremove{strict} relation between the dimensions of the space $\R^d$ and \mysecondinsert{$m$, that is,} the number of nearly-orthonormal vectors. But, the proof, \mysecondinsert{that} should have appeared in a later paper\mysecondremove{that}, to the best of our knowledge, has never been published. A previous result \cite{Kainen_Kurkova_1993} gives some theoretical lower-bounds on the number of nearly orthonormal vectors but these lower-bounds are not satisfactory as big vector spaces are needed  to be sure to  cover the nearly orthonormal vectors needed for the distributed convolution kernels. \mysecondremove{An answer to this question}\mysecondinsert{A proof of the \cite{HechtNielsen94}'s conjecture} can be derived from 

%We found the relation between $d$ and $m$ for any given $\theta$ and $\varepsilon$  by using a recent result \cite{Jayram:2011:OBJ:2133036.2133037} proposed in the context of the \cite{JLL} Transform. Our result is that for a given $\varepsilon$, a given $m$ and a given $\theta$, the lower bound of $d$ such that xxx is preserved is:
%$$\Omega(\varepsilon^{-2}\log m \log 1/(1 + \sqrt[3]{\theta - 1}))$$
%%%where: $\varepsilon\mysecondinsert{=1.5\overline{\epsilon}}$ and $\theta=\overline{\theta}^3 - 3 \overline{\theta}^2 + 3 \overline{\theta}$. 
%The lemma and the proof are in Appendix \ref{sec:_demonstration_}.

%by choosing $\varepsilon = 2\epsilon$, a space $\R^d$ with $d=O(\varepsilon^{-2}\log{m})$ and $\theta = 2/m^2 - 1/m^4$.



% Theorem 4.3 in \cite{Jayram:2011:OBJ:2133036.2133037} shows that it is possible to find a transformation $f$ from $\R^m$ to $\R^d$ such that vectors $v'$ and $v''$ in $\{0,1\}^m$\mysecondinsert{: (1)} preserve their norm with an approximation $\overline{\epsilon}$ and a probability $1-\overline{\theta}$\mysecondinsert{, that is $P(||v'||^2-\overline{\epsilon}<||f(v')||^2<||v'||^2+\overline{\epsilon})>1-\overline{\theta}$;} \mysecondremove{and}\mysecondinsert{(2)} preserve their ${l}_2$-distances with the same approximation and probability \mysecondinsert{$P(||v'-v''||^2-\overline{\epsilon}<||f(v') - f(v'')||^2<||v'-v''||^2+\overline{\epsilon})>1-\overline{\theta}$}. \mysecondinsert{As this is valid for any $v'$ and $v''$ in $\{0,1\}^m$, for all vectors $v_a$ and $v_b$ in $\{0,1\}^m$ containing a single $1$, $P(1 - \overline{\epsilon} < ||f(v_a)||^2 < 1 + \overline{\epsilon}) > 1 - \overline{\theta}$ and $P(|f(v_a) \cdot f(v_b)| < 1.5\overline{\epsilon}) > (1 - \overline{\theta})^3$ by using the properties of the dot product and the three independent events on $f(v_a)$, $f(v_b)$, and $f(v_a)-f(v_b)$}, that are the two properties required to nearly orthonormal vectors \mysecondinsert{(the proof is in Appendix \ref{_demonstration_})}. The lower-bound of the dimension $d$ the space $\R^d$ such that both the norm and the distances are approximatively preserved is $\Omega(\overline{\epsilon}^{-2}\log m \log 1/\overline{\theta})$. From this, it is easy to derive the relation between the dimension $d$ of $\R^d$ and the number of nearly orthonormal vectors the space can host with an approximation $\epsilon\mysecondinsert{=1.5\overline{\epsilon}}$ with a probability higher than $1-\theta$ \mysecondinsert{with $\theta=\overline{\theta}^3 - 3 \overline{\theta}^2 + 3 \overline{\theta}$ }.


%In the following, we first introduce the mapping function $\df(x)$. We introduce the properties of an ideal \mysecondinsert{basic binary} \mysecondremove{vector composition} function $\mo$ \mysecondinsert{on vectors}. We then show that, given the properties of the ideal \mysecondremove{vector composition}  function $\mo$, the proposed function $\df(x)$ satisfies properties~\ref{near_norm}~and~\ref{near_orth}. 

%Finally, we analyze whether it is possible to realize the properties of the ideal function $\mo$ in real functions.


%As the previous section shows, the embedding function $f$ exists. But, its direct application is impractical. This section describes the function $\df$ that directly computes nearly-orthonormal vectors $\svec{\tau}_i$ in the reduced space ${\mathbb R}^d$ using the original trees $\tau_i$. 
%As already described, these vectors represent dimensions of the original space ${\mathbb R}^m$ and, thus, tree fragments $\tau_i$. 
%The mapping function $\df$ composes $f$ and $I$, that is: 
%\begin{displaymath}
%\df(\tau_i) = f(I(\tau_i)) = f(\vec{\tau}_i)  = \svec{\tau}_i
%\end{displaymath}
%The key idea is to build the mapping function $\df$ on top of for tree node labels and 


%%%%%%%%%%%%%%%%%%%%%%% Definition of structure 
\begin{figure}
\begin{center}
\parsetree{(.A. (.B. .W1.) (.C. (.D. .W2.)(.E. .W3.)))}
\end{center}
\caption{A sample tree}
\label{sampletree}
\end{figure}

\meta{DIRE COME I VETTORI SI OTTENGONO}


\OCAR{To really build a bijective function $\df_{\Sigma}$ for distributed substructures,}{ knowing that sets $NOV(\varepsilon,\theta)$ in $\R^d$ are large is important but it is more important to know how to find these vectors.} {A strategy can be derived from practices in embedding with Johnson-Lindestrauss Transform (JLT) \cite{JLL}. In JLT, vectors $\vec{a}$, which we showed belong to $NOV(\varepsilon,\theta)$, are used as columns for linear transformation matrices that embeds high dimensional spaces $\R^m$ in low dimensional spaces $\R^d$. The general idea is that these vectors can be drawn out of a d-dimensional Gaussian distribution $\vec v\sim \mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ \cite{Indyk:1998:ANN:276698.276876,JLLsimple_demonstration}. Then, we have two options. The first option is to build sets $NOV(\varepsilon,\theta)$ with $\theta=0$ in a polynomial time \cite{JLLsimple_demonstration} by adding only those vectors that are nearly orthogonal with respect to existing ones in a growing set $NOV(\varepsilon,\theta)$. The second option is to use  $\mathcal{N}(0,\frac{1}{d}\mathbb{I}_d)$ as an virtually infinite $NOV(\varepsilon,\theta)$ drawing out vectors when needed.
In this latter case, we showed that it is possible to build $NOV(\varepsilon,\theta)$ with $\varepsilon = 0.1$ and $\theta= \propto 10^{-xxx}$ (see Appendix \ref{properties_of_normal_vectors}) that is sufficient to approximate convolution kernels with dot products of distributed convolution structures (cf. Equation \ref{the_core_property}).}{ Hence, there are two valid ways to build vectors for $NOV(\varepsilon,\theta)$. }

\LD{Thus, we showed that functions $\df_{\Sigma}$ to produce \emph{distributed substructures} can be defined as sets $NOV(\varepsilon,\theta)$ are large and can be build.}


\subsubsection{Convolution Kernels with Sums of Nearly-orthogonal Unit Vectors: Limits of the Direct Formulation}
\label{sec:first_formulation}

\LD{Given sets of nearly orthonormal vectors $NOV(\varepsilon,\theta)$, it is possible to derive a first conclusion: convolution kernels can be approximated with dot product of \emph{distributed structures} that are vectors in $\R^d$.} This section shows this first conclusion and analyzes its limits that Section \ref{sec:fra} resolves.

\LD{The first conclusion is based on: (1) a direct definition of $\df_{\Sigma}$ as a bijective mapping between substructures and vectors in $NOV(\varepsilon,\theta)$; and (2) a function \emph{distributed structure} $D_{\Sigma}$ that maps structures in sums of distributed substructures.} The definition of \emph{distributed structure} $D_{\Sigma}$ is the following:
$$
D_{\Sigma}(z)= \displaystyle\sum_{c\in S(z)} \omega_{c} \df_{\Sigma}(c)
$$
Hence, it is easy to prove the following lemma using Lemma \ref{k_in_deltas} and the union bound on the properties of vectors in $NOV(\varepsilon,\theta)$.
\begin{lemma} 
\label{first_formulation}
For each convolution kernel on discrete sets $K(x,y)$ it exists a bijective function $\df: X \rightarrow NOV(\varepsilon,\theta)$ and a function $D_{\Sigma}$ previously defined such that:
$$
\P(K(x,y) - \varepsilon\J(x,y)<D_{\Sigma}(x,y)<K(x,y) + \varepsilon \J(x,y) > 1 - 3\theta
$$
where 
$\J(x,y) = \|\Omega_x\|^2 + \|\Omega_y\|^2 +  K(x,y)$.
\end{lemma}

\begin{proof}
Let $\df$ be the matrix $\df=(\df(v_1)\ldots\df(v_k)\df(a_1)\ldots\df(a_h)\df(b_1)\ldots\df(b_g))$ where $v_i \in S(x) \cap S(y)$,  $a_i \in S(x) - S(x) \cap S(y)$ and $b_i \in S(y) - S(x) \cap S(y)$.
Let $\Omega_x$ and $\Omega_y$ represent weigths $\omega$ as vectors.

Vectors $\df\Omega_x$, $\df\Omega_y$ and $\df(\Omega_x + \Omega_y)$ behave as random vectors drawn from $\mathcal{N}(0,\frac{1}{d}\mathbb{I}_d\|\Omega_x\|^2)$, $\mathcal{N}(0,\frac{1}{d}\mathbb{I}_d\|\Omega_y\|^2)$ and $\mathcal{N}(0,\frac{1}{d}\mathbb{I}_d\|\Omega_x+\Omega_y\|^2)$

Hence, the following probabilities hold: 
\begin{equation}
\P(\|\df\Omega_x\|^2 > (1 + \varepsilon) \|\Omega_x\|^2, \|\df\Omega_x\|^2 < (1 - \varepsilon) \|\Omega_x\|^2  ) < \theta
\label{eq:1}
\end{equation}
\begin{equation}
\P(\|\df\Omega_y\|^2 > (1 + \varepsilon) \|\Omega_y\|^2, \|\df\Omega_y\|^2 < (1 - \varepsilon) \|\Omega_y\|^2  ) < \theta
\label{eq:2}
\end{equation}
$$
\P(\|\df(\Omega_x+\Omega_y)\|^2 > (1 + \varepsilon) \|(\Omega_x+\Omega_y)\|^2, \|\df(\Omega_x+\Omega_y)\|^2 < (1 - \varepsilon) \|(\Omega_x+\Omega_y)\|^2  ) < \theta
$$
The union of the above events has a probability $p<3\theta$.


As 
$$
\|\df(\Omega_x+\Omega_y)\|^2 = \|\df\Omega_x\|^2 + \|\df\Omega_y\|^2 + 2 \dotprod{\df\Omega_x}{\df\Omega_y}
$$
and
$$
\|\Omega_x+\Omega_y\|^2  = \|\Omega_x\|^2 + \|\Omega_y\|^2 + 2 \dotprod{\Omega_x}{\Omega_y}
$$
we rewrite:
$$
\|\df(\Omega_x+\Omega_y)\|^2 > (1 + \varepsilon) \|(\Omega_x+\Omega_y)\|^2
$$
as:
$$
\|\df\Omega_x\|^2 + \|\df\Omega_y\|^2 + 2 \dotprod{\df\Omega_x}{\df\Omega_y)} > (1 + \varepsilon) (\|\Omega_x\|^2 + \|\Omega_y\|^2 + 2 \dotprod{\Omega_x}{\Omega_y})
$$
From equations (\ref{eq:1}) and (\ref{eq:2}) , we can derive the following:
$$
- \|\df\Omega_x\|^2 - \|\df\Omega_y\|^2 > -\|\Omega_x\|^2 -\|\Omega_x\|^2 +  \varepsilon\|\Omega_x\|^2 + \varepsilon\|\Omega_y\|^2 
$$
Hence, the event:
$$
\dotprod{\df\Omega_x}{\df\Omega_y} > \dotprod{\Omega_x}{\Omega_y} + \varepsilon (\|\Omega_x\|^2 + \|\Omega_y\|^2 +  \dotprod{\Omega_x}{\Omega_y})
$$
holds with a probability $3\theta$.
In the same way, it is possible to derive that:
$$
\dotprod{\df\Omega_x}{\df\Omega_y} < \dotprod{\Omega_x}{\Omega_y)} - \varepsilon (\|\Omega_x\|^2 + \|\Omega_y\|^2 +  \dotprod{\Omega_x}{\Omega_y)})
$$
Finally, since the two equations hold with a probability $\theta^3$, the following probability holds:
$$
\P(K(x,y) - \varepsilon\J(x,y)<D_{\Sigma}(x,y)<K(x,y) + \varepsilon \J(x,y) > 1 - 3\theta
$$
where $K(x,y) = \dotprod{\Omega_x}{\Omega_y}$, 
$D_{\Sigma}(x,y) = \dotprod{\df\Omega_x}{\df\Omega_y}$ and 
$\J(x,y) = \|\Omega_x\|^2 + \|\Omega_y\|^2 +  K(x,y)$.
\end{proof}


\meta{NON SI PUO' FARE IL MAPPING DIRETTO SU QUEI VETTORI}

\LD{Unfortunately, this first solution is impractical as it is not trivial to directly build bijective mapping functions $\df_{\Sigma}$ and, thus, the function $D_{\Sigma}$.} Hilbert spaces $\mathcal H$ of substructures are potentially infinite dimensional and, thus, we would need large sets $NOV(\varepsilon,\theta)$  of vectors. Moreover, structures $x$ have exponentially large sets of substructures $S(x)$. Thus, even if we solve the problem of directly defining function $\df$, the direct computation of distributed structures $D_{\Sigma}(x)$ is implausible. 

The next section describes our model on how to efficiently build both a bijective function $\df$ and a function $D$ with a basic vector composition operation $\mo$ and two recursive definition: one for the distributed substructures and one for the distributed substructures that will be named distributed convolution structures. 

%This section explorers possible sets $\mathcal S$ whereas the function $\df$ is discussed in Section \ref{sec:compositional_definition} as a simple direct mapping from a set $S$ to a set $\mathcal S$ is unfeasible. Techniques like singular value decomposition or random indexing (see \cite{sahlgren05}) are impractical in this case as $S$ is huge, although possible. 


%\comment{R3, C3}{you use versors, can you provide a definition?}
%\mysecondinsert{Let $G_S$ be the ground set relevant for substructures in $S$. Substructures in $S$ are again structured objects and, thus, follow the definition of structured objects. As elements in $S$ are substructures of structured object in $X$, we can assume that $G_S=G_X$.} \mysecondremove{Let $G_X$ be the ground set relevant for objects in $X$.} We \mysecondinsert{then} define $\smallvectors{G_X} \subset \R^d$ as the direct random index of $G_X$ \myinsert{\cite{sahlgren05}}.  \finalcomment{Section 3.1: The reference to "direct random index" requires a citation.}{R1.4.1}
%%The set $\smallvectors{G_X}$ is a set of nearly orthogonal versors, where a versor is a vector of norm one. 
%Vectors $\svec{g} \in \smallvectors{G_X}$ are built drawing elements $\svec{g}_i$ from a normal distribution $\mathcal{N}(0,1)$ and normalizing the final results so that they are unit vectors, i.e. $||\svec{g}|| = 1$. These conditions are sufficient to guarantee that, at least, \myremove{each vector in}$\smallvectors{G_X}$ \mysecondinsert{is a set of nearly orthonormal vectors $NOV(\epsilon,\theta)$ as the Johnson-Lindenstruass Lemma holds for vectors in $\smallvectors{G_X}$}\mysecondremove{is statistically nearly-orthogonal with respect to the others}\myinsert{(see \cite{JLLsimple_demonstration})}. 


\subsection{Distributed Subpath Tree Kernels\comment{REVISED}{}}

%In this section, we show that the Distributed Tree framework can be applied to tree kernels other than the classic TK by \cite{Collins2002}. 

The second convolution kernel we consider is the Subpath Tree Kernel (STK) for unordered trees \cite{Kimura:2011:SKR:2017863.2017871}. 
%This section follows the same organization of the previous one.


%\subsubsection{Distributed Tree Fragments for the Subpath Tree Kernel}

\begin{figure}[h]
\begin{center}
\begin{eqnarray*}
&&P(\begin{array}{c}\parsetree{(.A. (.B. .W1.) (.C. (.D. .W2.)(.E. .W3.)))}\end{array})=\{
\begin{array}{c}\parsetree{(.A. . B .)}\end{array},
\begin{array}{c}\parsetree{(.A. (. B . .W1.))}\end{array},
\begin{array}{c}\parsetree{(. B . .W1.)}\end{array},
\begin{array}{c}\parsetree{(.A. (. C . (.D. .W2.)))}\end{array},
\begin{array}{c}\parsetree{(. C . (.D. .W2.))}\end{array},
\begin{array}{c}\parsetree{(.A. (. C . .D.))}\end{array},
\begin{array}{c}\parsetree{(.A. . C .)}\end{array},\\&&
\begin{array}{c}\parsetree{(. C . .D.)}\end{array},
\begin{array}{c}\parsetree{(. D . .W2.)}\end{array},
\begin{array}{c}\parsetree{(.A. (. C . (.E. .W3.)))}\end{array},
\begin{array}{c}\parsetree{(. C . (.E. .W3.))}\end{array},
\begin{array}{c}\parsetree{(.A. (. C . .E.))}\end{array},
\begin{array}{c}\parsetree{(. C . .E.)}\end{array},
\begin{array}{c}\parsetree{(. E . .W3.)}\end{array},
A, B, W1, C, D, E, W2, W1
\}
\end{eqnarray*}
\end{center}
\caption{Tree Fragments for the subpath tree kernel}
\label{stk:feature_space}
\end{figure}



STK uses very simple tree fragments as features: subpaths that are chains of nodes. 
A subpath of a tree is formally defined as a substring of a path from the root to one of the leaves of the tree.
Given a context-free grammar $G=(N,\Sigma,P,S)$, any sequence of non-terminal symbols $N$, possibly closed by one terminal symbol in $\Sigma$, is a valid tree fragment.
Function $S_{ST}(t)$ can be defined accordingly. Given a tree $t$, $S_{ST}(t)$ contains any sequence of symbols $p=a_0 ... a_n$ where $a_i$ is a direct descendant of $a_{i-1}$ for any $i>0$. The distributed tree fragments are then: 
$\df(p) = \Mo_{i=0}^n \basicdf(a_i)$.


Figure \ref{stk:feature_space} proposes an example of the above function. 
In \cite{Kimura:2011:SKR:2017863.2017871}, the weight $\omega_p$ for the subpath feature $p$ for a given tree $t$ is:
\begin{displaymath}
\omega_p = \num{p}{t}\sqrt{\lambda^{\nnodes{p}}}
\end{displaymath}
where $\nnodes{p}$ is the length of the subpath $p$ and $\num{p}{t}$ is the number of times a subpath $p$ appears in the tree $t$. 


%\subsubsection{Recursively computing Distributed Trees for the Subpath Tree Kernel}


%As in the case of the classic TK, we can define a Distributed Tree representation $\svec{T}$ for tree $T$ such that the kernel function can be approximated by the explicit dot product, i.e. $STK(T_1,T_2) \approx \svec{T_1}\cdot\svec{T_2}$.
%In this case, each standard versor $\vec{p_i}$ of the implicit feature space $\mathbb{R}^m$ corresponds to a possible subpath $p_i$. Thus, the Distributed Tree is:
%\begin{equation}
% \svec{T} = \sum_{p \in P(T)} \lambda^{|p|}\svec{p}
%\end{equation}
%where $P(T)$ is the set of subpaths of $T$ and $\svec{p}$ is the Distributed Tree Fragment vector for subpath $p$. Subpaths can be seen as trees where each node has at most one child, thus their DTF representation is the same. 


The distributed subpath tree (DST), that is the distributed convolution structure for the subpath tree kernels, follows:
\begin{eqnarray*}
&&\input{kernels/dst_1}\\
&&\input{kernels/dst_2}
\end{eqnarray*}
DST has the same  \myinsert{time} computational complexity of the DT.

%\subsubsection{Validity of the recursive computation}

%A theorem like Theorem \ref{overall_theorem} must be proved:
%
%\begin{theorem}\label{subpath_theorem}
%Given the ideal vector composition function $\mo$, the following equivalence holds:
%\begin{equation}
%\svec{T} = \sum_{n \in N(T)} s(n) = \sum_{p\in P(T)} \sqrt{\lambda^{|p|}}\svec{p}
%\label{the_equivalence}
%\end{equation}
%\end{theorem}


%%%% NON SERVE PERCHE' PROVATO PER DCK
%
%To prove Theorem \ref{subpath_theorem}, we introduce a definition and two simple lemmas, whose proof is trivial. In the following, we will denote by $(n|p)$ the concatenation of a node $n$ with a path $p$.
%
%\begin{definition}
%Let $n$ be a node of a tree $T$. We define $P(n) = \{p | p \mbox{ is a subpath of } T \mbox{ starting with } n\}$
%\end{definition}
%
%\begin{lemma}\label{subpath_lemma1}
%Let $n$ be a tree node and $C(n)$ the (possibly empty) set of its children. Then $P(n) = n \cup \displaystyle\bigcup_{c\in C(n)}{\displaystyle\bigcup_{p'\in P(c)}{(n|p')}}$.
%\end{lemma}
%
%\begin{lemma}\label{subpath_lemma2}
%Let $p= (n|p')$ be the path given by the concatenation of node $n$ and path $p'$. Then $\svec{p} = \svec{n} \mo \svec{p'}$.
%\end{lemma}
%
%
%Now we can show that function $s(n)$ computes exactly the sum of the DTFs for all the possible subpaths starting with $n$.
%\begin{theorem}
%\label{subpath_small_theorem}
%Let $n$ be a node of tree $T$. Then $s(n)=\displaystyle\sum_{p\in P(n)} \sqrt{\lambda^{|p|}}\svec{p}$.
%\end{theorem}
%\begin{proof}
%The theorem is proved by structural induction.
%
%\textbf{Basis.} Let $n$ be a terminal node. Then we have $P(n)=n$. Thus, by its definition, $s(n)=\sqrt{\lambda}\svec{n}=\displaystyle\sum_{p\in P(n)} \sqrt{\lambda^{|p|}}\svec{p}$.
%
%\textbf{Step.} Let $n$ be a node with children set $C(n)$. The inductive hypothesis is then $\forall c\in C(n). s(c)=\displaystyle\sum_{p\in P(c)} \sqrt{\lambda^{|p|}}\svec{p}$. Applying the inductive hypothesis, the definition of $s(n)$ and property \ref{ideal_operation}.\ref{p_dist}, we have
% 
%\begin{eqnarray*}
%s(n) &=&
%\sqrt{\lambda}\left(\svec{n} + \svec{n}\mo\sum_{c\in C(n)} s(c)\right) \\
%&=& \sqrt{\lambda}\left(\svec{n} + \svec{n}\mo\sum_{c\in C(n)} \sum_{p\in P(c)} \sqrt{\lambda^{|p|}}\svec{p}\right) \\
%&=& \sqrt{\lambda}\svec{n} + \sum_{c\in C(n)} \sum_{p\in P(c)} \sqrt{\lambda^{|p|+1}}\svec{n}\mo\svec{p} 
%\end{eqnarray*}
%Thus, by means of Lemmas \ref{subpath_lemma1} and \ref{subpath_lemma2}, we can conclude that $s(n) = \displaystyle\sum_{p\in P(n)} \sqrt{\lambda^{|p|}}\svec{p}$. 
%\end{proof}

\subsection{Distributed Route Tree Kernels\comment{REVISED}{}}

Large tree structures with many symbols may produce feature spaces of tree fragments that are very sparse. This may affect the final performance of the classification function as discussed in \cite{NIPS2005_762}. Route kernels for trees \cite{Aiolli2009} are introduced to address this issue. Instead of encoding a path between two nodes in the tree using the node labels, route kernels use the relative position of the edges in the production originated in a node. As shown in \cite{Aiolli2009}, this reduces the sparsity and has a positive effect on the final performance of the classifiers.
These kernels are the third kernel family we consider.


%The aim of this section is to introduce the distributed version of the Route Tree Kernels. The section is organized as the previous ones.



\begin{figure}
\begin{center}
\includegraphics[width=30mm]{Figures/Tree_for_RouteTreeKernel.jpg}
\caption{Routes in Trees: an example}
\label{fig:tree_for_RouteTreeKernel}
\end{center}
\end{figure}

%\subsubsection{Route Kernels for Trees}

Route kernels for trees deal with positional $\rho$-ary trees, i.e. trees where a unique positional index $P_n[e] \in \{1,\cdots,\rho\}$ is assigned to each edge $(n,e)$ leaving from node $n$. 
Figure \ref{fig:tree_for_RouteTreeKernel} shows the walk-through example with positional indexes as edge labels. 
Route kernels introduce the notion of \emph{route} $\pi(n_i,n_j)$ between nodes $n_i$ and $n_j$ as the sequence of indexes of the edges that constitute the shortest path between the two nodes. The definition follows:
\begin{displaymath}
\pi(n_1,n_k) = P_{n_1}[n_2]P_{n_2}[n_3]\ldots P_{n_{k-1}}[n_k] 
\end{displaymath}
 In the general case, a route may contain both positive and negative indexes, for edges that are traversed away from or towards the root, respectively.
For example, the route from the node $B$ to the node $D$ is $\pi(B,D)= [-1,2,1]$ as the edge $(A,B)$ is traversed towards the root of the tree.  

%In this setting, a generalized route kernel takes the form of:
%\begin{equation}
% K(T_1,T_2) = \displaystyle\sum_{n_i,n_j\in T_1}\sum_{n_l,n_m\in T_2} k_\pi((n_i,n_j)(n_l,n_m))k_\xi((n_i,n_j)(n_l,n_m))
%\end{equation}
%where $k_\pi$ is a local kernel defined on the routes and $k_\xi$ is some other local kernel used to add expressiveness to the kernel.

\cite{Aiolli2009} also define an instantiation of the generalized route kernel, for which an efficient implementation is proposed. In the following we will refer to this kernel as RTK. This kernel restricts the set of feasible routes to those between a node and any of its descendants. The empty route $\pi(n,n)$ is included, with $|\pi(n,n)|=0$. A decay factor $\lambda$ is introduced to counterweight the influence of larger routes.
%
%, leading to the following formulation for $k_\pi$:
%\begin{equation}
% k_\pi((n_i,n_j)(n_l,n_m)) = \delta(\pi(n_i,n_j),\pi(n_l,n_m))\lambda^{|\pi(n_i,n_j)|}
%\end{equation}
%where $\delta$ is the usual Kronecker comparison function.
%Finally, $k_\xi$ is defined as $\delta(l(n_j),l(n_m))$, i.e. 1 if $n_j$ and $n_m$ have the same label, 0 otherwise. A variant is also proposed, where the whole productions at $n_j$ and $n_m$ are compared instead. 





%\subsubsection{Distributed Tree Fragments for the Route Tree Kernel}

\begin{figure}
\begin{center}
\includegraphics[width=120mm]{Figures/RouteTreeKernel.jpg}
\caption{Tree Fragments for the Route Tree Kernel}
\label{fig:rtk_examples}
\end{center}
\end{figure}


The features considered by RTK are the routes between a node $n_i$ and any of its descendants $n_j$, together with the label of node $n_j$. An example is given in Figure \ref{fig:rtk_examples}, that reports the tree we are using in this new form. The last node of the route has the label whereas all the other nodes do not. 
The weight $\omega_\pi$ of route $\pi=\pi(n_i,n_j)$ is:
\begin{displaymath}
\omega_{\pi} = \num{\pi(n_i,n_j)}{t}\sqrt{\lambda^{|\pi(n_i,n_j)|}}
\end{displaymath}
\comment{[R2,C8]}{Section 5.3.2: 
Unlike what is stated in definition 3,  nearly orthogonal base vectors  seem to be defined over edges 
instead of nodes. Can you provide the proper definitions?}

The transposition of these \emph{routes} to distributed routes is straight-forward. Indexes $P_n[e]$ are treated as new node labels. Thus, the distributed tree fragment associated with the route $\pi(n_1,n_k)$ ended by the node $n_k$  is:
\begin{displaymath}
\df(\pi(n_1,n_k)) = \displaystyle\Mo_{i=1}^{k-1} \basicdf(P_{n_i}[n_{i+1}]) \mo \basicdf(n_k)
\end{displaymath}

%(or its whole production)

%The Distributed Tree is then:
%\begin{equation}
% \svec{T} = \sum_{\pi\in \Pi(T)} \sqrt{\lambda^{|\pi|}}\svec{\pi}
%\end{equation}
%where $\Pi(T)$ is the set of valid routes of $T$.



%\subsubsection{Recursively computing Distributed Trees for the Route Tree Kernels}




The convolution definition of distributed route (DR) follows:
%An efficient way to compute $\svec{T}$ is needed also in the case of the route features for a tree $T$. The formulation in Eq. \ref{distributed_vectors} is still valid, as long as we define recursive function $s(n)$ as follows:
\begin{eqnarray*}
&&\input{kernels/drt_1}\\
&&\input{kernels/drt_2}
\end{eqnarray*}
This formulation is similar to the distributed subpath trees we described earlier and the computational complexity is the same.


%A theorem like Theorem \ref{overall_theorem} can be demonstrated to show that Eq. \ref{distributed_vectors} with the above definition of $s(n)$ computes exactly the Eq. \ref{initial_distributed_vectors}, where tree fragments are the routes of the tree.
%
%\begin{theorem}\label{route_theorem}
%Given the ideal vector composition function $\mo$, the following equivalence holds:
%\begin{equation}
%\svec{T} = \sum_{n \in N(T)} s(n) = \sum_{\pi\in \Pi(T)} \sqrt{\lambda^{|\pi|}}\svec{\pi}
%\end{equation}
%\end{theorem}

%To prove Theorem \ref{route_theorem}, we introduce a definition and a simple lemma, whose proof is trivial.
%
%\begin{definition}
%Let $n$ be a node of a tree $T$. We define $\Pi(n) = \{\pi(n,m) | m \mbox{ is a descendant of } n\}$.
%\end{definition}
%
%
%\begin{lemma}\label{route_lemma}
%Let $n$ be a tree node, $m$ a child of $n$, and $l$ a descendant of $m$. Then $\svec{\pi_{n,l}} = \svec{P_n[(n,m)]} \mo \svec{\pi_{m,l}}$.
%\end{lemma}
%
%
%Now we can show that function $s(n)$ computes exactly the sum of the DTFs for all the possible routes starting in $n$.
%\begin{theorem}
%\label{subpath_small_theorem}
%Let $n$ be a node of tree $T$. Then $s(n)=\displaystyle\sum_{\pi\in \Pi(n)} \sqrt{\lambda^{|\pi|}}\svec{\pi}$.
%\end{theorem}
%\begin{proof}
%The theorem is proved by structural induction.
%
%\textbf{Basis.} Let $n$ be a terminal node. Then we have $\Pi(n)=\pi_{n,n}$. Thus, by its definition, $s(n)=\svec{n}=\svec{\pi_{n,n}}=\displaystyle\sum_{\pi\in \Pi(n)} \sqrt{\lambda^{|\pi|}}\svec{\pi}$.
%
%\textbf{Step.} Let $n$ be a node with children set $C(n)$. The inductive hypothesis is then $\forall c\in C(n). s(c)=\displaystyle\sum_{\pi\in \Pi(c)} \sqrt{\lambda^{|\pi|}}\svec{\pi}$. Applying the inductive hypothesis, the definition of $s(n)$ and property \ref{ideal_operation}.\ref{p_dist}, we have
% 
%\begin{eqnarray*}
%s(n) &=&
%\svec{n}+\sqrt{\lambda}\sum_{c\in C(n)} \svec{P_n[(n,c)]} \mo s(c) \\
%&=& \svec{\pi_{n,n}} + \sqrt{\lambda}\sum_{c\in C(n)} \svec{P_n[(n,c)]} \mo \displaystyle\sum_{\pi\in \Pi(c)} \sqrt{\lambda^{|\pi|}}\svec{\pi}\\
%&=& \svec{\pi_{n,n}} + \sum_{c\in C(n)} \displaystyle\sum_{\pi\in \Pi(c)} \sqrt{\lambda^{|\pi|+1}} \svec{P_n[(n,c)]} \mo \svec{\pi}
%\end{eqnarray*}
%Thus, by means of Lemma \ref{route_lemma} and the definition of tree node descendants, we can conclude that $s(n) = \displaystyle\sum_{\pi\in \Pi(n)} \sqrt{\lambda^{|\pi|}}\svec{\pi}$. 
%\end{proof}



\subsubsection{Artificial trees}
\label{sec:artgram}

As in \cite{Rieck:2010:ATK:1756006.1756022}, in many of the following experiments, we made use of artificial trees along with linguistic parse trees. The artificial trees are generated from a set of $n$ node labels, divided into terminal and non-terminal labels. A maximum out-degree $d$ for the tree nodes is chosen. The trees are generated recursively by building tree nodes whose labels and numbers of children are picked at random, according to a uniform distribution, until all tree branches end in a terminal node.

The \emph{Artificial Corpus} of trees used in the following experiments is a set of 1000 trees generated randomly according to the described procedure. The label set contains 6 terminal and 6 non-terminal labels. The maximum branching factor is 3, and the trees are composed of 30 nodes on average.

\subsubsection{Direct comparison}


\comment{[R2,C3]}{Section 6.3.1 (direct comparison):  While sections 3.2 and  6.3.2 both point to the fact (structural 
limitation) that when the structure becomes complex, distributed tree kernels become poor 
approximation, this section points to the opposite conclusion. For both the SDTK and the RDTK, the 
spearman correlation with their non-distributed versions remain high even when lambda goes to 1 
(complex structure). Is there something I do not understand, an experimental flaw, or some confidence 
intervals that make some of the conclusions meaningless. Please clarify!}
These experiments test the ability of Distributed Convolution Kernels to emulate the corresponding convolution kernels. We compared the similarities derived by the traditional convolution kernels with those derived by the distributed versions of the kernels. For each corpus (see Section \ref{sec:treecorp}), we extracted a set of 1,000 pairs and we considered the Spearman's correlations of DCKs' values with respect to CKs' values. Each Table reports the correlations for the three sets with different values of $\lambda$ and with different dimensions (d=1024, 2048, 4096, and 8192). We want to analyze the ability of the distributed convolution kernels to approximate the original kernels with respect to different dimensions of the reduced space.
We report the experiments where the distributed convolution kernels are computed using the shuffled circular convolution $\shufcconv$. 

\paragraph{Distributed Parse Tree Kernel}

%\begin{table}
%\begin{center}
%\begin{tabular}{lr|c|c|c|c|c}
%& & Dim. 512	&Dim. 1024	&Dim. 2048	&Dim. 4096	&Dim. 8192\\
%\hline
%Artificial&$\lambda$=0.2	&0.792	&0.869	&0.931	&0.96	&0.978\\
%Corpus &$\lambda$=0.4	&0.669	&0.782	&0.867	&0.925	&0.956\\
%&$\lambda$=0.6	&0.34	&0.454	&0.59	&0.701	&0.812\\
%&$\lambda$=0.8	&0.06	&0.058	&0.075	&0.141	&0.306\\
%&$\lambda$=1.0	&0.018	&0.017	&-0.043	&-0.019	&0.112\\
%\hline
%\hline
%QC&$\lambda$=0.20	&0.943	&0.961	&0.981	&0.99	&0.994\\
%Corpus &$\lambda$=0.40	&0.894	&0.925	&0.961	&0.978	&0.989	\\
%&$\lambda$=0.60	&0.571	&0.621	&0.73	&0.804	&0.88\\
%&$\lambda$=0.80	&0.165	&0.148	&0.246	&0.299	&0.377\\
%&$\lambda$=1.00	&0.037	&0.014	&0.06	&0.108	&0.107\\
%\hline
%\hline
%RTE&$\lambda$=0.20	&0.969	&0.983	&0.991	&0.996	&0.998\\
%Corpus&$\lambda$=0.40	&0.849	&0.888	&0.919	&0.943	&0.961\\
%&$\lambda$=0.60	&0.152	&0.207	&0.245	&0.299	&0.343\\
%&$\lambda$=0.80	&0.002	&0.027	&0.021	&0.041	&0.026\\
%&$\lambda$=1.00	&0.018	&0.023	&0.018	&0.003	&0\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Spearman's correlation of DTK values with respect to TK values, on trees taken from the three data sets and with the shuffled circular convolution $\shufcconv$}
%\label{test_corr_table_tk}
%\end{table}
%

\begin{table}
\begin{center}
\begin{tabular}{lr|c|c|c|c}
& &Dim. 1024	&Dim. 2048	&Dim. 4096	&Dim. 8192\\
\hline
Artificial&$\lambda$=0.2	&0.869	&0.931	&0.96	&0.978\\
Corpus &$\lambda$=0.4		&0.782	&0.867	&0.925	&0.956\\
&$\lambda$=0.6				&0.454	&0.59	&0.701	&0.812\\
&$\lambda$=0.8				&0.058	&0.075	&0.141	&0.306\\
&$\lambda$=1.0				&0.017	&-0.043	&-0.019	&0.112\\
\hline
\hline
QC&$\lambda$=0.20		&0.961	&0.981	&0.99	&0.994\\
Corpus &$\lambda$=0.40	&0.925	&0.961	&0.978	&0.989	\\
&$\lambda$=0.60			&0.621	&0.73	&0.804	&0.88\\
&$\lambda$=0.80			&0.148	&0.246	&0.299	&0.377\\
&$\lambda$=1.00			&0.014	&0.06	&0.108	&0.107\\
\hline
\hline
RTE&$\lambda$=0.20		&0.983	&0.991	&0.996	&0.998\\
Corpus&$\lambda$=0.40	&0.888	&0.919	&0.943	&0.961\\
&$\lambda$=0.60			&0.207	&0.245	&0.299	&0.343\\
&$\lambda$=0.80			&0.027	&0.021	&0.041	&0.026\\
&$\lambda$=1.00			&0.023	&0.018	&0.003	&0\\
\hline
\end{tabular}
\end{center}
\caption{Spearman's correlation of DTK values with respect to TK values, on trees taken from the three data sets and with the shuffled circular convolution $\shufcconv$}
\label{test_corr_table_tk}
\end{table}


The Spearman's correlation results in Table \ref{test_corr_table_tk} show how the DTK approximates the TK for different values of $\lambda$ and with different dimensions $d$ of the reduced space. 

For values of $\lambda$ lower than $1$, the correlation steadily increases along with the dimension of the space (except for the RTE corpus in the case of $\lambda=0.8$). This confirms that larger (but reasonably small) spaces can better approximate the huge space of the tree fragments. 

For both artificial and linguistic corpora, the DTK approximates well the TK for values of $\lambda$ smaller or equal to $0.4$. This is strictly related to the $J$ factor introduced in Theorem~\ref{The_theorem_of_the_paper} (cf. Sec.~\ref{sec:the_core_section}). As $\nnodes{t}$ can be high in these spaces, $J$ become higher as $\lambda$ increases. Thus, the distortion becomes important for $\lambda>0.4$. 

%the fact that the comparison between two large trees generates a large distortion value of the similarity (see equation \ref{eq:distorsion_of_the_dot_product} in Section \ref{sec:properties_of_the_vector_space}). This distortion is  reduced when $\lambda$ is small as the $\omega$s associated with the distributed tree fragments $\tau$ are smaller. 

DTK approximates well TK for values of $\lambda \leq 0.4$. TK is generally used in linguistic tasks with these values as it performs best (see also Section \ref{sec:task_based_exps}). Then, DTK can replace TK in these tasks.


\paragraph{Distributed Subpath Tree Kernel}

\begin{table}
\begin{center}
\begin{tabular}{lr|c|c|c|c}
&&Dim. 1024	&Dim. 2048	&Dim. 4096	&Dim. 8192\\
\hline
Artificial &$\lambda$=0.20	&0.958	&0.979	&0.992	&0.995\\
Corpus&$\lambda$=0.40	&0.945	&0.968	&0.988	&0.993\\
&$\lambda$=0.60	&0.919	&0.952	&0.981	&0.989\\
&$\lambda$=0.80	&0.876	&0.926	&0.968	&0.983\\
&$\lambda$=1.00	&0.804	&0.878	&0.944	&0.971\\
\hline
\hline
QC &$\lambda$=0.20	&0.994	&0.997	&0.999	&0.999\\
Corpus&$\lambda$=0.40	&0.991	&0.997	&0.998	&0.999\\
&$\lambda$=0.60	&0.986	&0.994	&0.997	&0.998\\
&$\lambda$=0.80	&0.975	&0.99	&0.994	&0.996\\
&$\lambda$=1.00	&0.953	&0.98	&0.989	&0.993\\
\hline
\hline
RTE&$\lambda$=0.20	&0.995	&0.998	&0.999	&0.999\\
Corpus&$\lambda$=0.40	&0.995	&0.998	&0.999	&0.999\\
&$\lambda$=0.60	&0.994	&0.998	&0.999	&0.999\\
&$\lambda$=0.80	&0.992	&0.996	&0.998	&0.998\\
&$\lambda$=1.00	&0.986	&0.993	&0.996	&0.997\\
\hline
\end{tabular}
\end{center}
\caption{Spearman's correlation of SDTK values with respect to STK values, taken on trees taken from the three data sets and with the shuffled circular convolution $\shufcconv$}
\label{test_corr_table_stk}
\end{table}


The Spearman's correlation results for the Subpath Tree Kernel are reported in Table \ref{test_corr_table_stk}. In this case, the correlation is extremely promising even for high values of $\lambda$. This is most likely because the feature space of STK is smaller than that of TK, for trees of the same size. Thus, the effect of the $J$ factor is smoothed. It should be noted however that STK is commonly used over trees larger than the one considered for this experiment. Interestingly, though, the best performance results are achieved on the linguistic corpora.

\paragraph{Distributed Route Tree Kernel}

\begin{table}
\begin{center}
\begin{tabular}{lr|c|c|c|c}
& &Dim. 1024	&Dim. 2048	&Dim. 4096	&Dim. 8192\\
\hline
Artificial&$\lambda$=0.20	&0.962	&0.982	&0.994	&0.995\\
Corpus&$\lambda$=0.40	&0.953	&0.974	&0.991	&0.992\\
&$\lambda$=0.60	&0.932	&0.962	&0.985	&0.988\\
&$\lambda$=0.80	&0.897	&0.945	&0.976	&0.983\\
&$\lambda$=1.00	&0.846	&0.919	&0.964	&0.975\\
\hline
\hline
QC&$\lambda$=0.20	&0.992	&0.998	&0.999	&1\\
Corpus&$\lambda$=0.40	&0.99	&0.997	&0.999	&0.999\\
&$\lambda$=0.60	&0.988	&0.996	&0.999	&0.999\\
&$\lambda$=0.80	&0.986	&0.994	&0.998	&0.999\\
&$\lambda$=1.00	&0.98	&0.99	&0.996	&0.998\\
\hline
\hline
RTE&$\lambda$=0.20	&0.991	&0.997	&0.998	&0.999\\
Corpus&$\lambda$=0.40	&0.987	&0.995	&0.998	&0.999\\
&$\lambda$=0.60	&0.981	&0.991	&0.996	&0.998\\
&$\lambda$=0.80	&0.97	&0.984	&0.992	&0.996\\
&$\lambda$=1.00	&0.948	&0.968	&0.985	&0.992\\
\hline
\end{tabular}
\end{center}
\caption{Spearman's correlation of RDTK values with respect to RTK values, taken on trees taken from the three data sets and with the shuffled circular convolution $\shufcconv$}
\label{test_corr_table_rtk}
\end{table}

The Spearman's correlation results for the Route Tree Kernel are reported in Table \ref{test_corr_table_rtk}. The results are analogous to those obtained for STK, since their feature space is quite similar.

\paragraph{Distributed Partial Tree Kernel}

\begin{table}
\begin{center}
\begin{tabular}{lr|c|c|c|c}
& &Dim. 1024	&Dim. 2048	&Dim. 4096	&Dim. 8192\\
\hline
Artificial&$\lambda$=0.20	&0.914	&0.924	&0.932	&0.936\\
Corpus&$\lambda$=0.40	&0.934	&0.96	&0.984	&0.991\\
&$\lambda$=0.60	&0.835	&0.898	&0.951	&0.975\\
&$\lambda$=0.80	&0.405	&0.47	&0.627	&0.76\\
&$\lambda$=1.00	&0.113	&-0.004	&0.06	&0.12\\
\hline
\hline
QC&$\lambda$=0.20	&0.994	&0.997	&0.998	&0.998\\
Corpus&$\lambda$=0.40	&0.99	&0.995	&0.997	&0.998\\
&$\lambda$=0.60	&0.93	&0.959	&0.975	&0.988\\
&$\lambda$=0.80	&0.349	&0.381	&0.481	&0.542\\
&$\lambda$=1.00	&0.115	&0.093	&0.127	&0.147\\
\hline
\hline
RTE&$\lambda$=0.20	&0.996	&0.998	&0.999	&0.999\\
Corpus&$\lambda$=0.40	&0.991	&0.994	&0.998	&0.998\\
&$\lambda$=0.60	&0.546	&0.57	&0.64	&0.661\\
&$\lambda$=0.80	&0.035	&0.065	&0.096	&0.063\\
&$\lambda$=1.00	&-0.004	&0.032	&0.043	&-0.007\\
\hline
\end{tabular}
\end{center}
\caption{Spearman's correlation of PDTK values with respect to PTK values, taken on trees taken from the three data sets and with the shuffled circular convolution $\shufcconv$}
\label{test_corr_table_ptk}
\end{table}

The Spearman's correlation results for the Partial Tree Kernel are reported in Table \ref{test_corr_table_ptk}. Even though the PTK feature space is more complex than the one for the TK, the results are slightly better. This is due to the additional decay factor introduced by the PTK, that affects many features.

\paragraph{Distributed Sequence Kernel}

\begin{table}
\begin{center}
\begin{tabular}{lr|c|c|c|c}
& &Dim. 1024	&Dim. 2048	&Dim. 4096	&Dim. 8192\\
\hline
Artificial&$\lambda$=0.20	&0.993	&0.994	&0.994	&0.994\\
Corpus&$\lambda$=0.40	&0.994	&0.996	&0.997	&0.998\\
&$\lambda$=0.60	&0.989	&0.994	&0.996	&0.998\\
&$\lambda$=0.80	&0.975	&0.986	&0.991	&0.995\\
&$\lambda$=1.00	&0.951	&0.974	&0.985	&0.99\\
\hline
\hline
QC&$\lambda$=0.20	&0.935	&0.945	&0.947	&0.945\\
Corpus&$\lambda$=0.40	&0.921	&0.94	&0.944	&0.942\\
&$\lambda$=0.60	&0.879	&0.927	&0.941	&0.945\\
&$\lambda$=0.80	&0.703	&0.828	&0.886	&0.922\\
&$\lambda$=1.00	&0.431	&0.574	&0.673	&0.746\\
\hline
\hline
RTE&$\lambda$=0.20	&0.98	&0.988	&0.993	&0.995\\
Corpus&$\lambda$=0.40	&0.979	&0.988	&0.994	&0.996\\
&$\lambda$=0.60	&0.97	&0.984	&0.992	&0.996\\
&$\lambda$=0.80	&0.924	&0.957	&0.979	&0.987\\
&$\lambda$=1.00	&0.681	&0.745	&0.832	&0.881\\
\hline
\end{tabular}
\end{center}
\caption{Spearman's correlation of DSK values with respect to SK values, taken on trees taken from the three data sets and with the shuffled circular convolution $\shufcconv$}
\label{test_corr_table_sk}
\end{table}

The Spearman's correlation results for the Sequence Kernel are reported in Table \ref{test_corr_table_sk}. \myinsert{For this experiment and any subsequent one involving Sequence Kernels, a value of $p=3$ was used.} The correlation is extremely promising, although not as promising as the one achieved with SDTK and RDTK. This reflects, again, the nature of the feature space underlying the SK and its effect on the $J$ factor.
\finalcomment{lines 1-4: what was the value of p used for the string kernels?
  Here and in section 8.2.2. - done}{R4.3}

\subsubsection{Task-based Experiments}
\label{sec:task_based_exps}
\comment{[R4,C2][IssueG]}{In the experiments conducted on a question classification task and a
textual entailment task, the authors used d = 8192 which seems quite
high. The authors do not report the size of the trees for these tasks,
but it seems extremely likely than n << d and not unlikely than $n^2 <
d$ (for natural language tree) making the claimed complexity benefits
not so appealing and the issue raised above even more troublesome. The
authors should definitely report on the distribution of the sizes of
the tree, at least average, mean, minimum and maximum size.
}
\comment{[R2,C1]}{Section 6.3.2 (task based Experiments): While the authors have added a lot of graphs and some critical 
information (for instance, there was no mention of the Charniak parser in the ICML version), 
explanations and comparisons with other approaches are still missing. For instance, for question 
classification, the authors only give the URL for the data in a footnote with no citation (URL do not last 
forever), do not say if they use the coarse or fine class set (coarse I assume) and  do not say anything 
about preprocessing except a mention in passing of Charniak parser. When reporting a performance 
decay for distributed Subpath and Route kernels, they only say that the range of accuracies is narrower. 
Comparing this with the performance of the original study (Li and Roth) would help a lot (they obtain 
84\% accuracy on the coarse labels, so the result here is still quite good). I suggest a much more detailed 
description of the experiments, while the size of the graphs could be reduced. Note also that reporting 
all the QC results in the same 81-88\% range would make results much more comparable to the eye (the 
graph would have to stop at lambda=0.5 for the PTK, but all it is not interesting beyond that).}
In this section, we report on the experiments aimed at comparing the performance of the distributed kernels with respect to the corresponding original kernels on actual linguistic tasks. The tasks considered are Question Classification (QC) and Recognizing Textual Entailment (RTE). For these experiments, we considered two versions of the Distributed Kernels, using two \mysecondinsert{binary}\mysecondremove{composition} functions. We denote by $DTK_{\shufprod}$ and $DTK_{\shufcconv}$ the DTK using shuffled $\gamma$-product and shuffled circular convolution respectively. An analogous notation is used for SDTK and RDTK. All Distributed Kernels rely on vectors of dimension 8192.

\paragraph{Performance on the Question Classification task}

\begin{figure}[ht!]
\centering
\begin{tabular}{cc}
\multicolumn{2}{c}{\subfloat[$TK$, $DTK_\shufprod$ and $DTK_\shufcconv$]{
\input{Figures/qc_tk_small} 
\label{fig:qc_tk}
}} \\
\subfloat[$STK$, $SDTK_\shufprod$ and $SDTK_\shufcconv$]{
\input{Figures/qc_stk_small} 
\label{fig:qc_stk}
} &
\subfloat[$RTK$, $RDTK_\shufprod$ and $RDTK_\shufcconv$]{
\input{Figures/qc_rtk_small} 
\label{fig:qc_rtk}
} \\
\subfloat[$PTK$, $PDTK_\shufprod$ and $PDTK_\shufcconv$]{
\input{Figures/qc_ptk_small} 
\label{fig:qc_ptk}
} &
\subfloat[$SK$, $DSK_\shufprod$ and $DSK_\shufcconv$]{
\input{Figures/qc_sk_small} 
\label{fig:qc_sk}
} \\
\end{tabular}
\caption[]{Performance on the Question Classification task ($d=8192$) for several values of $\lambda$.}
\label{fig:qc_all}
\end{figure}

This experiment compared the performance of DCKs with respect to CKs on the actual task of Question Classification. 
%As benchmark data, we used a standard question classification training and test set \footnote{The QC set is available at \url{http://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC/}}, where the test set are the 500 TREC 2001 test questions. To measure the task performance, we used a question multi-classifier by combining $n$ binary SVMs according to the ONE-vs-ALL scheme, where the final output class is the one associated with the most probable prediction. 
We repeated the evaluation using several values for $\lambda$. Although the aim here is to compare DCKs and CKs, we give some reference on what is the performance of different models on this task. In the original study, \cite{Li:2002:LQC:1072228.1072378} obtain an accuracy on the coarse-grain classification of 91\% using a range of features including bag-of-words. \cite{Moschitti2007a} instead obtain an accuracy of 90\% using tree kernels on the parse trees and, only adding bag-of-words, the accuracy reaches 91.8\%. In our experiments, we use only plain DCKs and CKs without any explicit boosting of the bag-of-word features. 

% \begin{figure}[h!]
% \centering
% \input{Figures/qc_tk} 
% \caption{Performance on Question Classification task of $TK$, $DTK_\shufprod$ and $DTK_\shufcconv$ ($d=8192$) for several values of $\lambda$. }
% \label{fig:qc_tk}
% \end{figure}

The results for the DTKs are shown in Fig. \ref{fig:qc_tk}. For $\lambda=0.4$, results of the TK are in line with those obtained in \cite{Moschitti2007a}. DTKs lead to worse performance with respect to $TK$, but the gap is narrower for small values of $\lambda \leq 0.4$. These $\lambda$ values produce better performance for the task. Moreover, it can be noted that $DTK_\shufcconv$ behaves better than $DTK_\shufprod$ for smaller values of lambda, while the opposite is true for larger values of $\lambda$. An explanation of this phenomenon may be given in light of the results of the experiments in Sec. \ref{sec:approximation_properties}. Since the norm of large vector compositions, using $\shufprod$, tends to drop greatly, their final weight is smaller than expected. In other words, $DTK_\shufprod$ adds an implicit decay factor to the explicit one, i.e. $\lambda$. Thus, adopting larger values of $\lambda$ affects $DTK_\shufprod$ less heavily than it does for $DTK_\shufcconv$ and $TK$ itself.

% \begin{figure}[h!]
% \centering
% \input{Figures/qc_stk} 
% \caption{Performance on Question Classification task of $STK$, $SDTK_\shufprod$ and $SDTK_\shufcconv$ ($d=8192$) for several values of $\lambda$. }
% \label{fig:qc_stk}
% \end{figure}
% 
% \begin{figure}[h!]
% \centering
% \input{Figures/qc_rtk} 
% \caption{Performance on Question Classification task of $RTK$, $RDTK_\shufprod$ and $RDTK_\shufcconv$ ($d=8192$) for several values of $\lambda$. }
% \label{fig:qc_rtk}
% \end{figure}

The results for the SDTKs, RDTKs, PDTKs and DSKs are shown in Figures \ref{fig:qc_stk}, \ref{fig:qc_rtk}, \ref{fig:qc_ptk} and \ref{fig:qc_sk} respectively. All of these kernels behave similarly to the DTKs. Their performance is very similar for small values of $\lambda$, while the gap increases for higher values of $\lambda$. 
%STK and RTK seem to constantly gain accuracy when $\lambda$ increases. 
Kernels using $\shufcconv$ achieve better performance for lower values of $\lambda$, while they show a performance decay for higher values. It should be noted though, that the range of accuracies obtained by these kernels is much narrower than the one obtained by the Tree Kernel.

% \begin{figure}[h!]
% \centering
% \input{Figures/qc_ptk} 
% \caption{Performance on Question Classification task of $PTK$, $DPTK_\shufprod$ and $DPTK_\shufcconv$ ($d=8192$) for several values of $\lambda$. }
% \label{fig:qc_ptk}
% \end{figure}
% 
% \begin{figure}[h!]
% \centering
% \input{Figures/qc_sk} 
% \caption{Performance on Question Classification task of $SK$, $DSK_\shufprod$ and $DSK_\shufcconv$ ($d=8192$) for several values of $\lambda$. }
% \label{fig:qc_sk}
% \end{figure}

\begin{figure}[ht!]
\centering
\begin{tabular}{cc}
\multicolumn{2}{c}{\subfloat[$TK$, $DTK_\shufprod$ and $DTK_\shufcconv$]{
\input{Figures/qc_d_tk_small} 
\label{fig:qc_d_tk}
}} \\
\subfloat[$STK$, $SDTK_\shufprod$ and $SDTK_\shufcconv$]{
\input{Figures/qc_d_stk_small} 
\label{fig:qc_d_stk}
} &
\subfloat[$RTK$, $RDTK_\shufprod$ and $RDTK_\shufcconv$]{
\input{Figures/qc_d_rtk_small} 
\label{fig:qc_d_rtk}
} \\
\subfloat[$PTK$, $PDTK_\shufprod$ and $PDTK_\shufcconv$]{
\input{Figures/qc_d_ptk_small} 
\label{fig:qc_d_ptk}
} &
\subfloat[$SK$, $DSK_\shufprod$ and $DSK_\shufcconv$]{
\input{Figures/qc_d_sk_small} 
\label{fig:qc_d_sk}
} \\
\end{tabular}
\caption[]{Performance on Question Classification task ($\lambda=0.4$) for several vector sizes.}
\label{fig:qc_d_all}
\end{figure}

The experiment was also repeated using vectors of different size for Distributed Convolution Structures. Figure \ref{fig:qc_d_tk} reports the accuracy obtained on the Question Classification task by using vectors of increasing size, for a fixed value of $\lambda=0.4$. The accuracy gains are substantial both for $DTK_\shufprod$ and $DTK_\shufcconv$. Figures \ref{fig:qc_d_stk}, \ref{fig:qc_d_rtk}, \ref{fig:qc_d_ptk} and \ref{fig:qc_d_sk} report the same results for the other convolution kernels considered. These results clearly show that increasing the vector size leads to much better approximations of the original convolution kernels.


% \begin{figure}[h!]
% \centering
% \input{Figures/qc_d_tk} 
% \caption{Performance on Question Classification task of $TK$, $DTK_\shufprod$ and $DTK_\shufcconv$ ($\lambda=0.4$) for several vector sizes. }
% \label{fig:qc_d_tk}
% \end{figure}

% \begin{figure}[h!]
% \centering
% \input{Figures/qc_d_stk} 
% \caption{Performance on Question Classification task of $STK$, $SDTK_\shufprod$ and $SDTK_\shufcconv$ ($\lambda=0.4$) for several vector sizes. }
% \label{fig:qc_d_stk}
% \end{figure}
% 
% \begin{figure}[h!]
% \centering
% \input{Figures/qc_d_rtk} 
% \caption{Performance on Question Classification task of $RTK$, $RDTK_\shufprod$ and $RDTK_\shufcconv$ ($\lambda=0.4$) for several vector sizes. }
% \label{fig:qc_d_rtk}
% \end{figure}
% 
% \begin{figure}[h!]
% \centering
% \input{Figures/qc_d_ptk} 
% \caption{Performance on Question Classification task of $PTK$, $PDTK_\shufprod$ and $PDTK_\shufcconv$ ($\lambda=0.4$) for several vector sizes. }
% \label{fig:qc_d_ptk}
% \end{figure}
% 
% \begin{figure}[h!]
% \centering
% \input{Figures/qc_d_sk} 
% \caption{Performance on Question Classification task of $SK$, $DSK_\shufprod$ and $DSK_\shufcconv$ ($\lambda=0.4$) for several vector sizes. }
% \label{fig:qc_d_sk}
% \end{figure}

\paragraph{Performance on the Textual Entailment Recognition task}

\begin{figure}[ht!]
\centering
\begin{tabular}{cc}
\multicolumn{2}{c}{\subfloat[$TK$, $DTK_\shufprod$ and $DTK_\shufcconv$]{
\input{Figures/rte_tk_small} 
\label{fig:rte_tk}
}} \\
\subfloat[$STK$, $SDTK_\shufprod$ and $SDTK_\shufcconv$]{
\input{Figures/rte_stk_small} 
\label{fig:rte_stk}
} &
\subfloat[$RTK$, $RDTK_\shufprod$ and $RDTK_\shufcconv$]{
\input{Figures/rte_rtk_small} 
\label{fig:rte_rtk}
} \\
\subfloat[$PTK$, $PDTK_\shufprod$ and $PDTK_\shufcconv$]{
\input{Figures/rte_ptk_small} 
\label{fig:rte_ptk}
} &
\subfloat[$SK$, $DSK_\shufprod$ and $DSK_\shufcconv$]{
\input{Figures/rte_sk_small} 
\label{fig:rte_sk}
} \\
\end{tabular}
\caption[]{Performance on the Textual Entailment Recognition task ($d=8192$) for several values of $\lambda$. Each point is the average accuracy on the 4 data sets.}
\label{fig:rte_all}
\end{figure}

For this experiment, we considered the recognizing textual entailment sets ranging from the first challenge to the fifth \cite{RTE1}, except for the fourth, which has no training set. 
%These sets are referred to as RTE1-5. The dev/test distribution for RTE1-3, and RTE5 is respectively 567/800, 800/800, 800/800, and 600/600 T-H pairs. 
We used these sets for the traditional task of pair-based entailment recognition, where a pair of text-hypothesis $p=(t,h)$ is assigned a positive or negative entailment class. For our comparative analysis, we use the syntax-based approach described in \cite{Zanzotto2009c} with two kernel function schemes: (1) $PK_{S}(p_1,p_2) = K_{S}(t_1,t_2) + K_{S}(h_1,h_2)$; and, (2)  $PK_{S+Lex}(p_1,p_2) = Lex(t_1,h_1)Lex(t_2,h_2) + K_{S}(t_1,t_2) + K_{S}(h_1,h_2)$. 
$Lex$ is the lexical similarity between $T$ and $H$ computed using WordNet-based metrics as in \cite{corley-mihalcea:2005:SemEq}. This feature is used in combination with the basic kernels and it gives an important boost to their performance.
$K_{S}$ is realized with the several considered CK. %$TK$, $DTK_{\shufprod}$, and $DTK_{\shufcconv}$. In the plots, the different $PK_{S}$ kernels are referred to as $PTK$, $DPTK_{\shufprod}$, and $DPTK_{\shufcconv}$ whereas the different $PK_{S+Lex}$ kernels are referred to as $PTK+Lex$, $DPTK_{\shufprod}+Lex$, and $DPTK_{\shufcconv}+Lex$. Analogous notations are used for Subpath and Route Tree Kernels.
For the computation of the similarity feature \emph{Lex} \cite{corley-mihalcea:2005:SemEq}, we exploited the Jiang\&Conrath distance \cite{JNC} computed using the \texttt{wn::similarity} package \cite{Pedersen:wn::similarity}. As for the case of the QC task, we considered several values of $\lambda$. 
In this case, comparing DCK-based approaches to CK-based ones with what proposed in \cite{Zanzotto2009c} gives already an idea on how distributed approaches are behaving with respect to the state-of-the-art. \cite{Zanzotto2009c}'s approach is significantly above the average of the approaches in the considered challenges ranking in the first five positions.

% \begin{figure}[h]
% \centering
% \input{Figures/rte_tk} 
% \caption{Performance on Recognizing Textual Entailment task of $TK$, $DTK_\shufprod$ and $DTK_\shufcconv$ ($d=8192$) for several values of $\lambda$. Each point is the average accuracy on the 4 data sets.}
% \label{fig:rte_tk}
% \end{figure}

Accuracy results for DTKs are reported in Fig. \ref{fig:rte_tk}. The results lead to conclusions similar to the ones drawn from the QC experiments. For $\lambda < 0.4$, $DTK_\shufprod$ and $DTK_\shufcconv$ is similar to $TK$. Differences are not statistically significant except for  $\lambda=0.4$ where $DTK_\shufprod$ behaves better than $TK$ (with $p<0.1$). Statistical significance is computed using the two-sample Student t-test. $DTK_\shufprod+Lex$ and $DTK_\shufcconv+Lex$ are statistically similar to $TK+Lex$ for any value of $\lambda$.
\finalcomment{$lambda < 0.4$ - done}{R3.12}

% \begin{figure}[h]
% \centering
% \input{Figures/rte_stk} 
% \caption{Performance on Recognizing Textual Entailment task of $STK$, $SDTK_\shufprod$ and $SDTK_\shufcconv$ ($d=8192$) for several values of $\lambda$. Each point is the average accuracy on the 4 data sets.}
% \label{fig:rte_stk}
% \end{figure}

% \begin{figure}[h]
% \centering
% \input{Figures/rte_rtk} 
% \caption{Performance on Recognizing Textual Entailment task of $RTK$, $RDTK_\shufprod$ and $RDTK_\shufcconv$ ($d=8192$) for several values of $\lambda$. Each point is the average accuracy on the 4 data sets.}
% \label{fig:rte_rtk}
% \end{figure}

Accuracy results for SDTKs, RDTKs, PDTKs and DSKs are reported in Figures \ref{fig:rte_stk}, \ref{fig:rte_rtk}, \ref{fig:rte_ptk} and \ref{fig:rte_sk} respectively. The behavior is very similar to that of DTKs. In particular, performance for SDTKs, RDTKs and DSKs is even slightly higher than the original Subpath, Route and Sequence Kernels, and it seems to be less heavily affected by the values of parameter $\lambda$. 

% \begin{figure}[h]
% \centering
% \input{Figures/rte_ptk} 
% \caption{Performance on Recognizing Textual Entailment task of $PTK$, $PDTK_\shufprod$ and $PDTK_\shufcconv$ ($d=8192$) for several values of $\lambda$. Each point is the average accuracy on the 4 data sets.}
% \label{fig:rte_ptk}
% \end{figure}

% \begin{figure}[h]
% \centering
% \input{Figures/rte_sk} 
% \caption{Performance on Recognizing Textual Entailment task of $SK$, $DSK_\shufprod$ and $DSK_\shufcconv$ ($d=8192$) for several values of $\lambda$. Each point is the average accuracy on the 4 data sets.}
% \label{fig:rte_sk}
% \end{figure}



%\begin{figure}
%% Preamble: \pgfplotsset{width=7cm,compat=1.12}
%\pgfplotsset{footnotesize}
%\begin{center}% note that \centering uses less vspace...
%\ref{prova2}\\
%\begin{tabular}{ccc}
%\emph{QC}&\emph{entailment}&\emph{paraphrasing}\\
%\begin{tikzpicture}
%\begin{semilogxaxis}[
%legend columns=-1,
%legend entries={$DPT$,$DS$,$DT$},
%legend to name=prova2,
%ymin = 0,
%ymax = 1.01,
%xlabel={space dimension $d$},
%ylabel={probability at $\varepsilon=0.01$}]
%\addplot table  [x=dim,y=eps01] {data/MainTheoremProb/QCgrctDPartialTree_lambda04_mu04_epsilon01.dat};
%\addplot table  [x=dim,y=eps01] {data/MainTheoremProb/QCDString_lambda04_epsilon01.dat};
%\addplot table [x=dim,y=eps01]  {data/MainTheoremProb/QCDSubsetTree_lambda04_epsilon01.dat};
%\end{semilogxaxis}
%\end{tikzpicture}
%&
%\begin{tikzpicture}
%\begin{semilogxaxis}[
%xlabel={space dimension $d$},
%ymin = 0,
%ymax = 1.01]
%\addplot table  [x=dim,y=eps01] {data/MainTheoremProb/entailmentPartialTree_lambda04_mu04_epsilon01.dat};
%\addplot table  [x=dim,y=eps01] {data/MainTheoremProb/entailmentDString_lambda04_epsilon01.dat};
%\addplot table [x=dim,y=eps01]  {data/MainTheoremProb/entailmentDSubsetTree_lambda04_epsilon01.dat};
%\end{semilogxaxis}
%\end{tikzpicture}
%&
%\begin{tikzpicture}
%\begin{semilogxaxis}[
%xlabel={space dimension $d$},
%ymin = 0,
%ymax = 1.01]
%\addplot table  [x=dim,y=eps01] {data/MainTheoremProb/paraPartialTree_lambda04_mu04_epsilon01.dat};
%\addplot table  [x=dim,y=eps01] {data/MainTheoremProb/paraDString_lambda04_epsilon01.dat};
%\addplot table [x=dim,y=eps01]  {data/MainTheoremProb/paraDSubsetTree_lambda04_epsilon01.dat};
%\end{semilogxaxis}
%\end{tikzpicture}
%\\
%\begin{tikzpicture}
%\begin{semilogxaxis}[
%legend columns=-1,
%legend entries={$DPT$,$DS$,$DT$},
%legend to name=prova2,
%ymin = 0,
%ymax = 1.01,
%xlabel={space dimension $d$},
%ylabel={probability  at $\varepsilon=0.005$}]
%\addplot table  [x=dim,y=eps005] {data/MainTheoremProb/QCgrctDPartialTree_lambda04_mu04_epsilon01.dat};
%\addplot table  [x=dim,y=eps005] {data/MainTheoremProb/QCDString_lambda04_epsilon01.dat};
%\addplot table [x=dim,y=eps005]  {data/MainTheoremProb/QCDSubsetTree_lambda04_epsilon01.dat};
%\end{semilogxaxis}
%\end{tikzpicture}
%&
%\begin{tikzpicture}
%\begin{semilogxaxis}[
%xlabel={space dimension $d$},
%ymin = 0,
%ymax = 1.01]
%\addplot table  [x=dim,y=eps005] {data/MainTheoremProb/entailmentPartialTree_lambda04_mu04_epsilon01.dat};
%\addplot table  [x=dim,y=eps005] {data/MainTheoremProb/entailmentDString_lambda04_epsilon01.dat};
%\addplot table [x=dim,y=eps005]  {data/MainTheoremProb/entailmentDSubsetTree_lambda04_epsilon01.dat};
%\end{semilogxaxis}
%\end{tikzpicture}
%&
%\begin{tikzpicture}
%\begin{semilogxaxis}[
%xlabel={space dimension $d$},
%ymin = 0,
%ymax = 1.01]
%\addplot table  [x=dim,y=eps005] {data/MainTheoremProb/paraPartialTree_lambda04_mu04_epsilon01.dat};
%\addplot table  [x=dim,y=eps005] {data/MainTheoremProb/paraDString_lambda04_epsilon01.dat};
%\addplot table [x=dim,y=eps005]  {data/MainTheoremProb/paraDSubsetTree_lambda04_epsilon01.dat};
%\end{semilogxaxis}
%\end{tikzpicture}
%\end{tabular}
%\end{center}
%\caption{Main theorem probability estimation QC-set}
%\label{fig:main_theorem_probability_estimation}
%\end{figure}
%

